{"posts":[{"title":"「 Kata Containers 」架构与组件概述","text":"based on 2.1.1 概述Kata Containers 是一个开源项目，它采用轻量化虚拟机作为容器的隔离来构建一个安全容器运行时，而其虚拟化技术作为容器的二层保护为负载提供了更好的隔离性，这使得 Kata Containers 兼具传统容器的形态和虚拟机的安全性。 早在 2015 年，来自英特尔开源技术中心的工程师就开始探索采用 英特尔® 虚拟技术（英特尔® Virtualization Technology，英特尔® VT）来提高容器的安全隔离性，并以此发起了英特尔® Clear Containers 开源项目，与此同时，来自 Hyper.sh（一家中国的高科技初创公司）的工程师也发起了 runV10 开源项目，这两个项目采用的技术和目的都非常相似，都是为了将容器置于一个安全“沙箱“，以便进一步促进该技术发展和成熟。随后在 2017 年，英特尔和 Hyper.sh 团队将这两个开源项目在社区合并成了一个新的项目 Kata Containers。 传统虚拟机（VMs）可提供硬件隔离，而容器可快速响应，且占用空间相对较小，Kata Containers 将这两者的优势完美结合了起来。 每个容器或 Pod 都在自己单独的虚拟机中启动， 并不再能够访问主机内核，杜绝了恶意代码侵入其它相临容器的可能。由于 Kata Containers 同时具备硬件隔离，也使得互不信任的租户，甚至于生产应用或前生产应用都能够在同一集群内安全运行，从而使得在裸机上运行容器即服务（Containers as a Service, CaaS）成为可能。 AssetsKata Containers 创建一个 VM，在其中运行一个或多个容器。需要通过启动 hypervisor 创建虚拟机来实现这一点。hypervisor 需要两个 assets 来完成这项任务：一个 Linux 内核和一个用于引导 VM 的小型根文件系统镜像。 kernelguest 内核传递到 hypervisor 用于引导虚拟机。 Kata Containers 中提供了一个对虚机启动时间和内存占用做了高度优化的默认内核，仅提供了容器工作负载所需的必要服务。该内核是基于最新的上游 Linux 内核做的定制化。 imagehypervisor 使用一个镜像文件，该文件提供了一个最小的根文件系统，供 guest 内核用来启动 VM 和托管 Kata 容器。 Kata Containers 支持基于 initrd 和 rootfs 的最小 guest 镜像（但是，并非所有的 hypervisor 均支持）。默认包同时提供 image 和 initrd，两者都是使用 osbuilder 工具创建的。 rootfs默认打包的 rootfs 映像，也称 mini O/S，是一个高度优化的容器引导系统。 使用此镜像启动 Kata 容器的背后流程为： 运行时将启动 hypervisor hypervisor 将使用 guest 内核启动 rootfs 镜像 内核将在 VM 根环境中以 PID 1（systemd）启动 init 守护进程 在 rootfs 上下文中运行的 systemd 将在 VM 的根上下文中启动 kata-agent kata-agent 将创建一个新的容器环境，将其根文件系统设置为用户请求的文件系统（例如 Ubuntu、busybox 等） kata-agent 将在新容器内执行容器启动命令 下表总结了默认的 rootfs，显示了创建的环境、在这些环境中运行的服务（适用于所有平台）以及每个服务使用的根文件系统： Process Environment systemd service? rootfs User accessible Notes systemd VM root n/a VM guest image debug console The init daemon, running as PID 1 Agent VM root yes VM guest image debug console Runs as a systemd service chronyd VM root yes VM guest image debug console Used to synchronise the time with the host container workload (sh(1) in the example) VM container no User specified (Ubuntu in the example) exec command Managed by the agent 1234567891011$ ps -ef&gt;&gt;&gt;UID PID PPID C STIME TTY TIME CMDroot 1 0 0 11:53 ? 00:00:00 /sbin/initroot 2 0 0 11:53 ? 00:00:00 [kthreadd]&lt;skip...&gt;root 61 1 0 11:53 ? 00:00:00 /usr/bin/kata-agentroot 71 61 0 11:53 ? 00:00:00 /pauseroot 73 61 0 11:53 ? 00:00:00 tail -f /dev/nullroot 75 61 0 11:55 pts/0 00:00:00 [bash]root 77 75 0 11:55 pts/0 00:00:00 ps -ef 12345$ ./usr/bin/kata-agent&gt;&gt;&gt;{&quot;msg&quot;:&quot;announce&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;ts&quot;:&quot;2021-07-14T14:56:42.558066805+08:00&quot;,&quot;source&quot;:&quot;agent&quot;,&quot;pid&quot;:&quot;88325&quot;,&quot;subsystem&quot;:&quot;root&quot;,&quot;name&quot;:&quot;kata-agent&quot;,&quot;version&quot;:&quot;0.1.0&quot;,&quot;api-version&quot;:&quot;0.0.1&quot;,&quot;agent-version&quot;:&quot;2.1.0&quot;,&quot;config&quot;:&quot;AgentConfig { debug_console: false, dev_mode: false, log_level: Info, hotplug_timeout: 3s, debug_console_vport: 0, log_vport: 0, container_pipe_size: 0, server_addr: \\&quot;vsock://-1:1024\\&quot;, unified_cgroup_hierarchy: false }&quot;,&quot;agent-type&quot;:&quot;rust&quot;,&quot;agent-commit&quot;:&quot;2.1.0-645e950b8e0e238886adbff695a793126afb584f&quot;}{&quot;msg&quot;:&quot;starting uevents handler&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;ts&quot;:&quot;2021-07-14T14:56:42.558356885+08:00&quot;,&quot;name&quot;:&quot;kata-agent&quot;,&quot;source&quot;:&quot;agent&quot;,&quot;subsystem&quot;:&quot;uevent&quot;,&quot;pid&quot;:&quot;88325&quot;,&quot;version&quot;:&quot;0.1.0&quot;}{&quot;msg&quot;:&quot;ttRPC server started&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;ts&quot;:&quot;2021-07-14T14:56:42.558522099+08:00&quot;,&quot;name&quot;:&quot;kata-agent&quot;,&quot;source&quot;:&quot;agent&quot;,&quot;version&quot;:&quot;0.1.0&quot;,&quot;subsystem&quot;:&quot;rpc&quot;,&quot;pid&quot;:&quot;88325&quot;,&quot;address&quot;:&quot;vsock://-1:1024&quot;} initrdinitrd 镜像是一个压缩的 cpio(1) 归档文件，它是从加载到内存中的 rootfs 创建的，并用作 Linux 启动过程的一部分。在启动过程中，内核将其解压到一个特殊的 tmpfs 挂载实例中，该实例成为初始根文件系统。 使用此镜像启动 Kata 容器的背后流程为： 运行时将启动 hypervisor hypervisor 将使用 guest 内核启动 initrd 镜像 内核将在 VM 根环境中以 PID 1（kata-agent）启动 init 守护进程 kata-agent 将创建一个新的容器环境，将其根文件系统设置为用户请求的文件系统（例如 Ubuntu、busybox 等） kata-agent 将在新容器内执行容器启动命令 下表总结了默认的 initrd，显示了创建的环境、在这些环境中运行的服务（适用于所有平台）以及每个服务使用的根文件系统： Process Environment rootfs User accessible Notes Agent VM root VM guest image debug console Runs as the init daemon (PID 1) container workload VM container User specified (Ubuntu in this example) exec command Managed by the agent 12345678910$ ps -ef&gt;&gt;&gt;UID PID PPID C STIME TTY TIME CMDroot 1 0 0 06:23 hvc0 00:00:02 /initroot 2 0 0 06:23 ? 00:00:00 [kthreadd]&lt;skip...&gt;root 41 1 0 06:23 hvc0 00:00:00 /pauseroot 43 1 0 06:23 hvc0 00:00:00 tail -f /dev/nullroot 45 1 0 06:24 pts/0 00:00:00 [bash]root 58 45 0 06:27 pts/0 00:00:00 ps -ef 12345$ ./sbin/init&gt;&gt;&gt;{&quot;msg&quot;:&quot;announce&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;ts&quot;:&quot;2021-07-14T14:58:37.454291069+08:00&quot;,&quot;source&quot;:&quot;agent&quot;,&quot;pid&quot;:&quot;66236&quot;,&quot;name&quot;:&quot;kata-agent&quot;,&quot;subsystem&quot;:&quot;root&quot;,&quot;version&quot;:&quot;0.1.0&quot;,&quot;api-version&quot;:&quot;0.0.1&quot;,&quot;agent-type&quot;:&quot;rust&quot;,&quot;agent-commit&quot;:&quot;2.1.0-645e950b8e0e238886adbff695a793126afb584f&quot;,&quot;agent-version&quot;:&quot;2.1.0&quot;,&quot;config&quot;:&quot;AgentConfig { debug_console: false, dev_mode: false, log_level: Info, hotplug_timeout: 3s, debug_console_vport: 0, log_vport: 0, container_pipe_size: 0, server_addr: \\&quot;vsock://-1:1024\\&quot;, unified_cgroup_hierarchy: false }&quot;}{&quot;msg&quot;:&quot;starting uevents handler&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;ts&quot;:&quot;2021-07-14T14:58:37.455243334+08:00&quot;,&quot;version&quot;:&quot;0.1.0&quot;,&quot;subsystem&quot;:&quot;uevent&quot;,&quot;name&quot;:&quot;kata-agent&quot;,&quot;pid&quot;:&quot;66236&quot;,&quot;source&quot;:&quot;agent&quot;}{&quot;msg&quot;:&quot;ttRPC server started&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;ts&quot;:&quot;2021-07-14T14:58:37.455325746+08:00&quot;,&quot;version&quot;:&quot;0.1.0&quot;,&quot;pid&quot;:&quot;66236&quot;,&quot;subsystem&quot;:&quot;rpc&quot;,&quot;source&quot;:&quot;agent&quot;,&quot;name&quot;:&quot;kata-agent&quot;,&quot;address&quot;:&quot;vsock://-1:1024&quot;} 总结 Image type Default distro Init daemon Reason Notes image Clear Linux (for x86_64 systems) systemd Minimal and highly optimized systemd offers flexibility initrd Alpine Linux Kata agent (as no systemd support) Security hardened and tiny C library osbuilderosbuilder 本身是 Kata Containers 项目中的一个模块，主要负责构建 guest OS 的引导镜像。 Kata Containers 支持两种引导镜像：rootfs 和 initrd。无论哪种方式，默认都会将 kata-agent 编译到镜像中，在对 kata-agent 有定制化需求的场景下，可以手动编译后添加到镜像中。 VirtualizationKata 容器是在传统 namespace 隔离之上创建的以硬件虚拟化为基础的第二层隔离。 Kata 启动一个轻量级虚拟机，并使用 guest 中特供的内核来承载容器工作负载。 接口映射Kata 容器的典型部署场景是借助 CRI 实现在 Kubernetes 中进行。在每个节点上，Kubelet 将与 CRI 实现者（如 Containerd 或 CRI-O 等）交互，CRI 实现者将与 Kata Containers（基于 OCI 规范的底层运行时）交互。 hypervisor（VMM）Kata Containers 本身支持多种 hypervisor 工具，如 QEMU、cloud-hypervisor、firecracker、ACRN 和 Dragonball（Kata 3.0 引入）。 Hypervisor Written in Architectures Type Configuration file ACRN C x86_64 Type 1 (bare metal) configuration-acrn.toml Cloud Hypervisor rust aarch64, x86_64 Type 2 (KVM) configuration-clh.toml Firecracker rust aarch64, x86_64 Type 2 (KVM) configuration-fc.toml QEMU C all Type 2 (KVM) configuration-qemu.toml Dragonball rust aarch64, x86_64 Type 2 (KVM) configuration-dragonball.toml 异同点参考 Hypervisor Summary Features Limitations Container Creation speed Memory density Use cases Comment ACRN Safety critical and real-time workloads excellent excellent Embedded and IOT systems For advanced users Cloud Hypervisor Low latency, small memory footprint, small attack surface Minimal excellent excellent High performance modern cloud workloads Firecracker Very slimline Extremely minimal Doesn’t support all device types excellent excellent Serverless / FaaS QEMU Lots of features Lots good good Good option for most users Dragonball Built-in VMM, low CPU and memory overhead Minimal excellent excellent Optimized for most container workloads out-of-the-box Kata Containers experience QEMU/KVMKata Containers with QEMU 与 Kubernetes 完全兼容（此外，Kata 社区对 QEMU 作了定制化的 patch 补丁） 取决于不同的 host 架构，Kata Containers 支持各种机器类型（machine），例如 x86 系统上的 q35、ARM 系统上的 virt 和 IBM Power 系统上的 pseries。 使用到的设备和特性有： virtio VSOCK or virtio serial virtio block or virtio SCSI virtio net virtio fs or virtio 9p (recommend: virtio fs) VFIO hotplug machine accelerators Kata 容器中使用加速器（accelerators）和热插拔来管理资源限制、缩短启动时间并减少内存占用。 加速器 加速器是特定于体系结构的，可用于提高性能并启用机器类型的特定功能。 Kata 容器中支持以下机器加速器： NVDIMM 此机器加速器特定于 x86，并且仅支持 q35 机器类型。 nvdimm 用于将根文件系统作为持久内存设备提供给 VM 设备热插拔 Kata Containers VM 为了更快的启动时间和减少内存占用，往往是以最少的资源启动。在容器启动过程中，设备会热插拔到 VM 中。例如，当指定了额外 CPU 时，便是通过热添加的方式追加资源。 Kata Containers 支持热添加以下设备： Virtio block Virtio SCSI VFIO CPU Firecracker/KVMFirecracker 是基于 rust-VMM 的衍生项目，支持的设备类型有限，但能提供更轻的体量和攻击面，专注于 FaaS 场景。因此，带有 Firecracker VMM 的 Kata 容器支持 CRI API 的一个子集。 Firecracker 不支持文件系统共享，仅支持基于块存储驱动程序。 Firecracker 不支持设备热插拔，也不支持 VFIO。因此，带有 Firecracker VMM 的 Kata Containers 不支持在启动后更新容器资源，也不支持设备透传。 支持的设备类型： virtio VSOCK virtio block virtio net Cloud Hypervisor/KVMCloud Hypervisor 同样是基于 rust-VMM 的衍生项目，旨在为运行现代云工作负载提供更小的占用空间和更小的攻击面。具有 Cloud Hypervisor 的 Kata Containers 提供与 Kubernetes 的几乎完全兼容性，与 QEMU 能力相当。从 Kata Containers 1.12 和 2.0.0 版本开始，Cloud Hypervisor 配置支持 CPU 和内存大小调整、设备热插拔（磁盘和 VFIO）、通过 virtio-fs 共享文件系统、基于块的卷、从 VM 镜像启动由 pmem 设备支持，并为每个 VMM 线程（例如所有 virtio 设备工作线程）提供细粒度的 seccomp 过滤器。 支持的设备类型与特性： virtio VSOCK or virtio serial virtio block virtio net virtio fs virtio pmem VFIO hotplug seccomp filters HTTP OpenAPI 总结 Solution release introduced brief summary Cloud Hypervisor 1.10 upstream Cloud Hypervisor with rich feature support, e.g. hotplug, VFIO and FS sharing Firecracker 1.5 upstream Firecracker, rust-VMM based, no VFIO, no FS sharing, no memory/CPU hotplug QEMU 1.0 upstream QEMU, with support for hotplug and filesystem sharing StorageKata Containers 与现有标准运行时兼容。从存储的角度来看，这意味着容器工作负载可能使用的存储量没有限制。由于 cgroups 无法设置存储分配限制，如果希望限制容器使用的存储量，请考虑使用现有设施，例如 quota(1) 限制或 device mapper 限制。 virtio SCSIvirtio-scsi 用于将工作负载镜像（例如 busybox:latest）共享到 VM 内的容器环境中。现阶段，Kata Containers 支持 virtio SCSI 和 virtio BLK，后者由于较多限制已不做推荐。 virtio FSvirtio-fs（VIRTIO）覆盖文件系统挂载点来共享工作负载镜像。kata-agent 使用此挂载点作为容器进程的根文件系统。 对于 virtio-fs，运行时为每个创建的 VM 启动一个 virtiofsd 守护进程（在主机上下文中运行）。 Kata Containers 使用轻量级虚拟机和硬件虚拟化技术来提供更强隔离，以构建安全的容器运行时。但也正是因为使用了虚拟机，容器的根文件系统无法像 runC 那样直接使用主机上构建好的目录，而需要有一种方法把 host 上的目录共享给 guest。 在此之前，有两种方法能够透传 host 目录或者数据给 guest，一种是基于 file 的方案，一个是基于 block 的方案。而这两种方案各有利弊，这里分别以 9pfs 和 devicemapper 为例来说明： 9pfs devicemapper 优势 使用 host 的 overlayfs，充分利用 host page cache 性能较好，POSIX 语义兼容性较好 痛点 基于网络协议，未对虚拟化场景做优化，性能较差；POSIX 语义兼容性不好 无法利用 host page cache，需要维护 lvm volume 针对以上两个方案的痛点和优势，virtio-fs 在某种程度上做了很好的互补，在 Kata Containers 中，支持两种文件共享方式：virtio-fs 和 virtio-9p，在 Kata Containers 2.x 之后，virtio-fs 作为默认且推荐的方案选择。 virtio-fs 本身采用类似于 CS 的架构，选择 FUSE 作为文件系统，而非网络文件系统协议。server 端是位于 host 上的 virtiofsd，用于向 guest 提供 fuse 服务；client 端是把 guest kernel 抽象成一个 fuse client，用于挂载 host 上导出的目录。两者之间通过 vhost_user 建立连接。 最大的特点是利用了 VM 和 VMM 同时部署在一个 host 上的，数据的共享访问都是通过共享内存的方式，避免了 VM 和 VMM 之间的网络通讯，共享内存访问比基于网络文件系统协议访问要更轻量级也有更好的本地文件系统语义和一致性。在面对多 guest 要 mmap 同一个文件的时候，virtio-fs 会将该文件 mmap 到 QEMU 的进程空间里，其余的 guest 通过 DAX 直接访问。 12345678910111213# qemu 进程参数节选$ ps -ef | grep qemu&gt;&gt;&gt;/usr/bin/qemu-system-x86_64-name sandbox-f13846d4f1d58e82b2d3f461c3f2296c57992d415e32d7b41f689cf1126ee8d9-uuid 50041ac8-a9ed-4a60-9db1-44a55b2343d8-machine pc,accel=kvm,kernel_irqchip-cpu host,pmu=off -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-117410659,guest-cid=117410659 -chardev socket,id=char-25f51af992a053e1,path=/run/vc/vm/f13846d4f1d58e82b2d3f461c3f2296c57992d415e32d7b41f689cf1126ee8d9/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-25f51af992a053e1,tag=kataShared -kernel /usr/share/kata-containers/vmlinux-5.10.25-85-initrd /usr/share/kata-containers/kata-containers-initrd-2021-07-14-11:02:27.932339999+0800-645e950 1234567891011121314151617# 宿主机共享目录$ ll /run/kata-containers/shared/sandboxes/f13846d4f1d58e82b2d3f461c3f2296c57992d415e32d7b41f689cf1126ee8d9/shared&gt;&gt;&gt;total 16drwxr-xr-x 3 root root 60 Jul 19 14:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e drwxr-xr-x 1 root root 40 Jul 19 14:57 rootfs-rw-rw-rw- 1 root root 0 Jul 19 14:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-1b95530f54b2fab0-termination-logdrwxrwxrwt 3 root root 140 Jul 19 14:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-395f94e69275ce07-serviceaccount lrwxrwxrwx 1 root root 13 Jul 19 14:57 ca.crt -&gt; ..data/ca.crt lrwxrwxrwx 1 root root 16 Jul 19 14:57 namespace -&gt; ..data/namespace lrwxrwxrwx 1 root root 12 Jul 19 14:57 token -&gt; ..data/token-rw-r--r-- 1 root root 212 Jul 19 14:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-45f076005d889842-hosts-rw-r--r-- 1 root root 103 Jul 19 14:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-70624933bdd41bd7-resolv.conf-rw-r--r-- 1 root root 15 Jul 19 14:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-c4568deafa816abf-hostnamedrwxr-xr-x 3 root root 60 Jul 19 14:57 f13846d4f1d58e82b2d3f461c3f2296c57992d415e32d7b41f689cf1126ee8d9 drwxr-xr-x 1 root root 40 Jul 19 14:57 rootfs-rw-r--r-- 1 root root 103 Jul 19 14:57 f13846d4f1d58e82b2d3f461c3f2296c57992d415e32d7b41f689cf1126ee8d9-172f4c5d001a82b4-resolv.conf 123456# 虚拟机 mount 点$ mount | grep kataShared&gt;&gt;&gt;kataShared on /run/kata-containers/shared/containers type virtiofs (rw,relatime)kataShared on /run/kata-containers/f13846d4f1d58e82b2d3f461c3f2296c57992d415e32d7b41f689cf1126ee8d9/rootfs type virtiofs (rw,relatime)kataShared on /run/kata-containers/6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e/rootfs type virtiofs (rw,relatime) 123456789101112# 虚拟机共享目录$ ls -l /run/kata-containers/shared/containers&gt;&gt;&gt;total 16drwxr-xr-x 3 root root 60 Jul 19 06:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-rw-rw-rw- 1 root root 0 Jul 19 06:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-1b95530f54b2fab0-termination-logdrwxrwxrwt 3 root root 140 Jul 19 06:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-395f94e69275ce07-serviceaccount-rw-r--r-- 1 root root 212 Jul 19 06:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-45f076005d889842-hosts-rw-r--r-- 1 root root 103 Jul 19 06:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-70624933bdd41bd7-resolv.conf-rw-r--r-- 1 root root 15 Jul 19 06:57 6cc73ba11330cfbdb54bf40c77613d5f832aad01413d566ff8dabbf4e29d748e-c4568deafa816abf-hostnamedrwxr-xr-x 3 root root 60 Jul 19 06:57 f13846d4f1d58e82b2d3f461c3f2296c57992d415e32d7b41f689cf1126ee8d9-rw-r--r-- 1 root root 103 Jul 19 06:57 f13846d4f1d58e82b2d3f461c3f2296c57992d415e32d7b41f689cf1126ee8d9-172f4c5d001a82b4-resolv.conf Devicemapperdevicemapper snapshotter 是一个特例。snapshotter 使用专用的块设备而不是格式化的文件系统，并且在块级别而不是文件级别运行。用于容器根文件系统直接使用底层块设备而不是覆盖文件系统。块设备映射到覆盖层的顶部读写层。与使用 virtio-fs 共享容器文件系统相比，这种方法提供了更好的 I/O 性能。 Kata Containers 具有热插拔添加和热插拔移除块设备的能力。这使得在 VM 启动后启动的容器可以使用块设备。 用户可以通过在容器内调用 mount(8) 来检查容器是否使用 devicemapper 块设备作为其 rootfs。如果使用 devicemapper 块设备，根文件系统（/）将从 /dev/vda 挂载。用户可以通过运行时配置禁止直接挂载底层块设备。 VSOCKs虚拟机中的进程可以通过以下两种方式与主机中的进程进行通信： 使用串口，虚拟机中的进程可以在串口设备读/写数据，主机中的进程可以从在 Unix socket 读/写数据。但是，串行链接一次限制对一个进程的读/写访问 更新、更简单的方法是 VSOCK，它可以接受来自多个客户端的连接 在 Kata Containers 2.x 中实现默认采用 VSOCK 的方式（依赖 4.8 以上版本内核和 vhost_vsock 内核模块） 12345678910111213141516171819.----------------------.| .------------------. || | .-----. .-----. | || | |cont1| |cont2| | || | `-----' `-----' | || | | | | || | .---------. | || | | agent | | || | `---------' | || | | | | || | POD .-------. | || `-----| vsock |----' || `-------' || | | || .------. .------. || | shim | | shim | || `------' `------' || Host |`----------------------' 优势 高密度 Pod 在 shimv1 使用 kata-proxy 建立 VM 和主机之间的连接，每一个 Pod 的内存大小大概是 4.5 MB 左右，在高密度 Pod 的集群中，内存的消耗过大。 可靠性 kata-proxy 负责虚拟机和主机进程之间的连接，如果 kata-proxy 异常，所有连接都会中断，尽管容器仍在运行。由于通过 VSOCK 的通信是直接的，与容器失去通信的唯一场景是 VM 本身或 containerd-shim-kata-v2 异常停止，但是在这种情况下，容器也会被自动删除。 NetworkingKata Containers 受限于 hypervisor 的功能，没有直接采用 Docker 默认的 Bridge 网络方案，而是采用的 macvtap 或者 tcfilter（使用 tc rules 将 veth 的 ingress 和 egress 队列分别对接 tap 的 egress 和 ingress 队列实现 veth 和 tap 的直连）方案。Kata Containers 本身是支持 CNI 管理网络的，网络方面相比容器，虽有额外开销但兼容性不差。 Docker 默认采用的容器网络方案是基于 network namespace + bridge + veth pairs 的，即在 host 上创建一个 network namespace，在 docker0 网桥上连接 veth pairs 的一端，再去 network namespace 中连上另一端，打通容器和 host 之间的网络。这种方案得益于 namespace 技术，而许多 hypervisor 比如 QEMU 不能处理 veth interfaces。所以 Kata Containers 为 VM 创建了 TAP interfaces 来打通 VM 和 host 之间的网络。传统的 Container Engine 比如 Docker，会为容器创建 network namespace 和 veth pair，然后 Kata 会将 veth pair 的一端连上 TAP，即 macvtap 方案。 Kata Containers 网络由 network namespaces、tap 和 tc 打通，创建 sandbox 之前首先创建网络命名空间，里面有 veth-pair 和 tap 两种网络接口，eth0 属于 veth-pair 类型接口，一端接入 CNI 创建的网络命名空间，一端接入宿主机；tap0_kata 属于 tap 类型接口，一端接入 cni 创建的网络命名空间，一端接入 QEMU 创建的 hypervisor，并且在 CNI 创建的网络命名空间使用 tc 策略打通 eth0 网络接口和 tap0_kata 网络接口，相当于把 eth0 和 tap0_kata 两个网络接口连成一条线。 sandbox 环境中只有 eth0 网络接口，这个接口是 QEMU 和 tap 模拟出的接口，mac、ip、掩码都和宿主机中 CNI 创建的网络命名空间中 eth0 的配置一样。 容器运行在 sandbox 环境中，容器采用共享宿主机网络命名空间方式创建容器，所以在容器中看到的网络配置和 sandbox 一样。 网络流量走向：流量进入宿主机后首先由物理网络通过网桥或者路由接入到网络命名空间，网络命名空间中在使用 tc 策略牵引流量到 tap 网络接口，然后再通过 tap 网络接口把流量送入虚拟化环境中，最后虚拟化环境中的容器共享宿主机网络命名空间后就可以在容器中拿到网络流量。 12345678910111213141516171819[root@node1 kata]# ip netns exec cni-d27eff58-b9c9-a258-3a1e-a34528d9796f ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.04: eth0@if29: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1430 qdisc noqueue state UP group default qlen 1000 link/ether fe:68:1c:e3:47:da brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.166.150/32 brd 10.244.166.150 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::fc68:1cff:fee3:47da/64 scope link valid_lft forever preferred_lft forever5: tap0_kata: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1430 qdisc mq state UNKNOWN group default qlen 1000 link/ether 76:c7:1b:ab:30:64 brd ff:ff:ff:ff:ff:ff inet6 fe80::74c7:1bff:feab:3064/64 scope link valid_lft forever preferred_lft forever 1234567891011121314151617181920212223242526272829[root@node1 kata]# ip netns exec cni-d27eff58-b9c9-a258-3a1e-a34528d9796f tc -s qdisc show dev eth0qdisc noqueue 0: root refcnt 2 Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0) backlog 0b 0p requeues 0qdisc ingress ffff: parent ffff:fff1 ---------------- Sent 480 bytes 5 pkt (dropped 0, overlimits 0 requeues 0) backlog 0b 0p requeues 0 [root@node1 kata]# ip netns exec cni-d27eff58-b9c9-a258-3a1e-a34528d9796f tc -s filter show dev eth0 ingressfilter protocol all pref 49152 u32filter protocol all pref 49152 u32 fh 800: ht divisor 1filter protocol all pref 49152 u32 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw (rule hit 5 success 5) match 00000000/00000000 at 0 (success 5 ) action order 1: mirred (Egress Redirect to device tap0_kata) stolen index 1 ref 1 bind 1 installed 439 sec used 437 sec Action statistics: Sent 480 bytes 5 pkt (dropped 0, overlimits 0 requeues 0) backlog 0b 0p requeues 0 [root@node1 kata]# ip netns exec cni-d27eff58-b9c9-a258-3a1e-a34528d9796f tc -s filter show dev tap0_kata ingressfilter protocol all pref 49152 u32filter protocol all pref 49152 u32 fh 800: ht divisor 1filter protocol all pref 49152 u32 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw (rule hit 12 success 12) match 00000000/00000000 at 0 (success 12 ) action order 1: mirred (Egress Redirect to device eth0) stolen index 2 ref 1 bind 1 installed 451 sec used 165 sec Action statistics: Sent 768 bytes 12 pkt (dropped 0, overlimits 0 requeues 0) backlog 0b 0p requeues 0 Kata Containerskata-runtime (v1)kata-runtime 实现 OCI 运行时标准，负责处理 OCI 标准命令，并启动 kata-shim 实例。 kata-agent (v1 &amp; v2)kata-agent 是运行在 Kata 创建的 VM 中的管理程序，使用 libcontainer 管理容器和容器中的进程服务。具体来说，kata-agent 借助 QEMU、VIRTIO serial 或 VSOCK interface 的形式在 host 上暴露一个 socket 文件，并在 VM 内运行一个 gRPC server 和 Kata 其他组件交互，runtime（kata-runtime &amp; containerd-shim-kata-v2）会通过 gRPC 来与 kata-agent 通信，来管理 VM 中的容器。 kata-proxy (v1)可选进程，在支持 VSOCK 的环境可以不需要。kata-proxy 给多个 kata-shim 和 kata-runtime 提供对 kata-agent 访问入口，负责路由 I/O 流和信号。kata-proxy 连接到 kata-agent 的 socket 上。一般情况下，kata-runtime 会通过 kata-proxy 来与 VM 内的 kata-agent 通信，管理 VM 内容器进程。 kata-shim (v1)kata-shim 的出现主要是考虑了 VM 内有多个容器的情况。在此之前，每个容器进程的回收由外层的一个 Reaper 负责。而 Kata Containers 方案中，容器运行在一个 VM 内，runtime 是无法监控、控制和回收这些 VM 内的容器，最多就是看到 QEMU 等进程，所以就设计了 kata-shim，用来监控容器进程，处理容器的所有 I/O 流，以及转发所有的要发送出去的信号。kata-runtime 会为每个容器创建一个对应的 kata-shim，每个 Pod sandbox（infra）也会有一个 kata-shim。 containerd-shim-kata-v2 (v1 &amp; v2)在 Kata Containers v1.5 版本之后，整合了原本的 kata-runtime、kata-shim、kata-proxy 以及 reaper 的功能。 在原方案（v1）中，每个 Pod 需要 2N + 1 个 shim（N 代表容器，每个容器需要一个 containerd-shim 和 kata-shim，而每一个 Pod sandbox 也需要一个 kata-shim）。而 containerd-shim-kata-v2 实现了 Containerd Runtime V2 (Shim API， 用于 runtime 和 Containerd 集成)，K8s 只需要为每个 Pod、包括其内部的多个容器创建一个 shimv2 就够了。除此之外，无论 kata-agent 的 gRPC server 是否使用 VSOCK 暴露到 host 上，都不再需要单独的 kata-proxy。 整体架构 蓝色区域代表的是 Kubernetes CRI 的组件；红色区域代表的是 Kata Containers 的组件；黄色区域代表的是 Kata Containers 的 VM shimV1 中 CRI 的流程只会通过 kata-proxy （非 Vsock 环境）和 VM 通信管理容器进程等 runc cmdline 就是实现了 OCI 标准的命令行工具 在 Kata 1.5 之后版本中 kata-runtime 得以保留，但是仅用作命令行工具判断 Kata Containers 的运行环境等，真正的 runtime 为 containerd-shim-kata-v2 Kata Containers 1.x Component Type Description agent core Management process running inside the virtual machine / POD that sets up the container environment. documentation documentation Documentation common to all components (such as design and install documentation). KSM throttler optional core Daemon that monitors containers and deduplicates memory to maximize container density on the host. osbuilder infrastructure Tool to create “mini O/S” rootfs and initrd images for the hypervisor. packaging infrastructure Scripts and metadata for producing packaged binaries (components, hypervisors, kernel and rootfs). proxy core Multiplexes communications between the shims, agent and runtime. runtime core Main component run by a container manager and providing a containerd shimv2 runtime implementation. shim core Handles standard I/O and signals on behalf of the container process. Kata Containers 2.x Component Type Description agent-ctl utility Tool that provides low-level access for testing the agent. agent core Management process running inside the virtual machine / POD that sets up the container environment. documentation documentation Documentation common to all components (such as design and install documentation). osbuilder infrastructure Tool to create “mini O/S” rootfs and initrd images for the hypervisor. packaging infrastructure Scripts and metadata for producing packaged binaries (components, hypervisors, kernel and rootfs). runtime core Main component run by a container manager and providing a containerd shimv2 runtime implementation. trace-forwarder utility Agent tracing helper. 与 Kubernetes 集成架构 流程示例以容器创建流程为例，初步理解下 Kata Containers 是如何运作 用户通过类似于 sudo ctr run --runtime &quot;io.containerd.kata.v2&quot; --rm -t &quot;quay.io/libpod/ubuntu:latest&quot; foo sh 命令请求 Container Manager 创建容器 Container Manager 守护进程启动 Kata 运行时的单个实例，即 containerd-shim-kata-v2 Kata 运行时加载配置文件 Container Manager 调用一组 shimv2 的 API Kata 运行时启动配置好的 hypervisor hypervisor 使用 guest 资源配置创建并启动（引导）VM hypervisor DAX 将 guest 镜像共享到 VM 中成为 VM rootfs（安装在 /dev/pmem* 设备上），即 VM 根环境 hypervisor 使用 virtio FS 将 OCI bundle 安装到 VM 的 rootfs 内的容器特定目录中（这个容器特定目录将成为容器 rootfs，称为容器环境） Kata agent 作为 VM 启动的一部分 运行时调用 Kata agent 的 CreateSandbox API 来请求 agent 创建容器 Kata agent 在包含容器 rootfs 的特定目录中创建容器环境（容器环境在容器 rootfs 目录中托管工作负载）（agent 创建的容器环境相当于 runc OCI 运行时创建的容器环境；Linux cgroups 和命名空间由 guest 内核在 VM 内创建，用于将工作负载与创建容器的 VM 环境隔离开来） Kata agent 在容器环境中生成工作负载 Container Manager 将容器的控制权返回给运行 ctr 命令的用户","link":"/2021/04/06/2021-04-06%20Kata%20Containers%20%E6%9E%B6%E6%9E%84%E4%B8%8E%E7%BB%84%E4%BB%B6%E6%A6%82%E8%BF%B0/"},{"title":"「 Kata Containers 」源码编译","text":"based on 2.4.3 Requirement这里采用 ubuntu:18.04 容器化编译，各依赖版本参考版本说明 1234567891011$ docker run --privileged -dit -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /dev:/dev --name kata-build ubuntu:18.04$ docker exec -it kata-build bash# 可选的 TARGET_ARCH 有 amd64 和 arm64$ export TARGET_ARCH=amd64$ export GOPATH=/root/go$ export GOPROXY=https://proxy.golang.com.cn,direct$ export https_proxy=http://10.52.17.42:7890$ mkdir -p $GOPATH/bin$ mkdir -p /etc/docker Dependence 软件包 1$ apt-get update &amp;&amp; apt-get -y install git wget make curl gcc xz-utils sudo flex bison bc python3 ninja-build pkg-config libglib2.0-dev librbd-dev libseccomp-dev libpixman-1-dev apt-utils libcap-ng-dev cpio libpmem-dev libelf-dev Golang 1.16.10 - 1.17.3 1$ wget https://go.dev/dl/go1.16.10.linux-$TARGET_ARCH.tar.gz &amp;&amp; tar -C /usr/local -zxvf go1.16.10.linux-$TARGET_ARCH.tar.gz &amp;&amp; cp /usr/local/go/bin/go /usr/bin/go Rust （1.58.1，仅在手动编译 kata-agent 组件时需要） 1234567$ curl https://sh.rustup.rs -sSf | sh$ source $HOME/.cargo/env$ rustup override set 1.58.1$ export ARCH=$(uname -m)$ if [ &quot;$ARCH&quot; = &quot;ppc64le&quot; -o &quot;$ARCH&quot; = &quot;s390x&quot; ]; then export LIBC=gnu; else export LIBC=musl; fi$ [ ${ARCH} == &quot;ppc64le&quot; ] &amp;&amp; export ARCH=powerpc64le$ rustup target add ${ARCH}-unknown-linux-${LIBC} yq 3.4.1 1$ wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_$TARGET_ARCH &amp;&amp; chmod +x yq_linux_$TARGET_ARCH &amp;&amp; mv yq_linux_$TARGET_ARCH $GOPATH/bin/yq &amp;&amp; cp $GOPATH/bin/yq /usr/bin/ docker 1234567$ curl -sSL https://get.docker.com/ | sh$ cat &gt; /etc/docker/daemon.json &lt;&lt; EOF{ &quot;storage-driver&quot;: &quot;vfs&quot;}EOF$ service docker start Source Code kata-containers 2.4.3 123$ GO111MODULE=off go get -d -u github.com/kata-containers/kata-containers$ cd $GOPATH/src/github.com/kata-containers/kata-containers$ git checkout 2.4.3 tests 2.4.3（仅在编译 UEFI ROM 时需要） 123$ GO111MODULE=off go get -d github.com/kata-containers/tests$ cd $GOPATH/src/github.com/kata-containers/tests$ git checkout 2.4.3 qemu（x86 下为 v6.2.0，arm64 下为 v6.1.0，仅在编译 QEMU 时需要） 123$ GO111MODULE=off go get -d github.com/qemu/qemu$ cd ${GOPATH}/src/github.com/qemu/qemu$ git checkout v6.2.0 Kata Containers12$ cd $GOPATH/src/github.com/kata-containers/kata-containers/src/runtime$ make &amp;&amp; sudo -E PATH=$PATH make install 编译结果 /usr/local/bin/containerd-shim-kata-v2 /usr/local/bin/kata-collect-data.sh /usr/local/bin/kata-monitor /usr/local/bin/kata-runtime /usr/share/defaults/kata-containers/configuration.toml Image1234567891011121314$ cd $GOPATH/src/github.com/kata-containers/kata-containers/tools/osbuilder# 根据社区 release 中所推荐对应架构所使用的 image 发行版，分别设置 rootfs 和 initrd 镜像，这里以 x86 架构为例$ ./rootfs-builder/rootfs.sh -lalpinecentosclearlinuxdebianubuntu# x86 下推荐 clearlinux，arm64 下推荐 ubuntu$ export rootfsdistro=clearlinux# x86 和 arm64 下均推荐 alpine$ export initrddistro=alpine 编译 Kata agent（可选） 123456$ cd $GOPATH/src/github.com/kata-containers/kata-containers/src/agent &amp;&amp; make# 默认情况下，Kata agent 是使用 seccomp 功能构建的。如果要构建没有 seccomp 功能的 Kata agent，则需要使用 SECCOMP=no 运行 make$ make -C $GOPATH/src/github.com/kata-containers/kata-containers/src/agent SECCOMP=no# 如果在配置文件中启用了 seccomp 但构建了没有 seccomp 功能的 Kata Agent，则 runtime 会保守地退出并显示一条错误消息 rootfs创建镜像文件系统 1234$ export ROOTFS_DIR=${GOPATH}/src/github.com/kata-containers/kata-containers/tools/osbuilder/rootfs-builder/rootfs$ sudo rm -rf ${ROOTFS_DIR}$ cd $GOPATH/src/github.com/kata-containers/kata-containers/tools/osbuilder/rootfs-builder$ script -fec 'sudo -E GOPATH=$GOPATH USE_DOCKER=true ./rootfs.sh ${rootfsdistro}' 添加 Kata agent 仅在 Kata agent 定制化后添加 123$ sudo install -o root -g root -m 0550 -t ${ROOTFS_DIR}/usr/bin ../../../src/agent/target/x86_64-unknown-linux-musl/release/kata-agent$ sudo install -o root -g root -m 0440 ../../../src/agent/kata-agent.service ${ROOTFS_DIR}/usr/lib/systemd/system/$ sudo install -o root -g root -m 0440 ../../../src/agent/kata-containers.target ${ROOTFS_DIR}/usr/lib/systemd/system/ 构建镜像 12$ cd $GOPATH/src/github.com/kata-containers/kata-containers/tools/osbuilder/image-builder$ script -fec 'sudo -E USE_DOCKER=true ./image_builder.sh ${ROOTFS_DIR}' 安装镜像 12345$ commit=$(git log --format=%h -1 HEAD)$ date=$(date +%Y-%m-%d-%T.%N%z)$ image=&quot;kata-containers-${date}-${commit}&quot;$ sudo install -o root -g root -m 0640 -D kata-containers.img &quot;/usr/share/kata-containers/${image}&quot;$ (cd /usr/share/kata-containers &amp;&amp; sudo ln -sf &quot;$image&quot; kata-containers.img) 编译结果 /usr/share/kata-containers/kata-containers-&lt;date&gt; /usr/share/kata-containers/kata-containers.img initrd创建镜像文件系统 1234$ export ROOTFS_DIR=&quot;${GOPATH}/src/github.com/kata-containers/kata-containers/tools/osbuilder/rootfs-builder/rootfs&quot;$ sudo rm -rf ${ROOTFS_DIR}$ cd $GOPATH/src/github.com/kata-containers/kata-containers/tools/osbuilder/rootfs-builder$ script -fec 'sudo -E GOPATH=$GOPATH AGENT_INIT=yes USE_DOCKER=true ./rootfs.sh ${initrddistro}' 添加 Kata agent 仅在 Kata agent 定制化后添加 1$ sudo install -o root -g root -m 0550 -T ../../../src/agent/target/${ARCH}-unknown-linux-${LIBC}/release/kata-agent ${ROOTFS_DIR}/sbin/init 构建镜像 12$ cd $GOPATH/src/github.com/kata-containers/kata-containers/tools/osbuilder/initrd-builder$ script -fec 'sudo -E AGENT_INIT=yes USE_DOCKER=true ./initrd_builder.sh ${ROOTFS_DIR}' 安装镜像 12345$ commit=$(git log --format=%h -1 HEAD)$ date=$(date +%Y-%m-%d-%T.%N%z)$ image=&quot;kata-containers-initrd-${date}-${commit}&quot;$ sudo install -o root -g root -m 0640 -D kata-containers-initrd.img &quot;/usr/share/kata-containers/${image}&quot;$ (cd /usr/share/kata-containers &amp;&amp; sudo ln -sf &quot;$image&quot; kata-containers-initrd.img) 编译结果 /usr/share/kata-containers/kata-containers-initrd-&lt;date&gt; /usr/share/kata-containers/kata-containers-initrd.img HypervisorQEMU1234567891011121314$ qemu_directory=${GOPATH}/src/github.com/qemu/qemu$ packaging_dir=&quot;${GOPATH}/src/github.com/kata-containers/kata-containers/tools/packaging&quot;$ cd $qemu_directory# 根据架构的 QEMU，应用对应版本的 patch$ $packaging_dir/scripts/apply_patches.sh $packaging_dir/qemu/patches/6.2.x/# 本地 commit 去除 dirty$ git config --global user.email kata@kata.com$ git config --global user.name kata$ git commit -am &quot;update&quot;$ $packaging_dir/scripts/configure-hypervisor.sh kata-qemu &gt; kata.cfg$ eval ./configure &quot;$(cat kata.cfg)&quot;$ make -j $(nproc) $ sudo -E make install 编译结果 /usr/bin/qemu-system-&lt;arch&gt; /usr/libexec/kata-qemu/virtiofsd /usr/share/kata-qemu/qemu/* Kernel1$ cd $GOPATH/src/github.com/kata-containers/kata-containers/tools/packaging/kernel x86 操作 123456# x86 环境下删除 arm-experimental 中的 patch 文件，避免误 patch$ rm -rf patches/5.15.x/arm-experimental/$ ./build-kernel.sh setup$ ./build-kernel.sh build$ ./build-kernel.sh install arm64 操作 12345# 重复 patch 导致流程异常，注释即可$ sed -i &quot;377s/^/#/&quot; build-kernel.sh$ ./build-kernel.sh -a aarch64 -E -d setup$ ./build-kernel.sh -a aarch64 -E -d build$ ./build-kernel.sh -a aarch64 -E -d install 编译结果 /usr/share/kata-containers/config-5.15.26 /usr/share/kata-containers/vmlinux.container /usr/share/kata-containers/vmlinux-5.15.26-90 /usr/share/kata-containers/vmlinuz.container /usr/share/kata-containers/vmlinuz-5.15.26-90 UEFI ROMUEFI ROM 仅在 arm64 环境下需要，用于设备热插拔 12$ cd $GOPATH/src/github.com/kata-containers/tests$ .ci/aarch64/install_rom_aarch64.sh 编译结果 /usr/share/kata-containers/kata-flash0.img /usr/share/kata-containers/kata-flash1.img","link":"/2021/04/22/2021-04-22%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"},{"title":"「 Kata Containers 」快速开始","text":"based on 3.0.0 安装Kata Containers 社区提供了 x86 架构制品，arm64 架构制品需要手动编译。这里以 x86 架构的社区制品安装为例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172$ wget https://github.com/kata-containers/kata-containers/releases/download/3.0.0/kata-static-3.0.0-x86_64.tar.xz$ tar -xvf kata-static-3.0.0-x86_64.tar.xz$ tree opt/kata/opt/kata/├── bin # 可执行的二进制文件与脚本│ ├── cloud-hypervisor│ ├── containerd-shim-kata-v2│ ├── firecracker│ ├── jailer│ ├── kata-collect-data.sh│ ├── kata-monitor│ ├── kata-runtime│ └── qemu-system-x86_64├── libexec # 可执行的二进制文件│ └── virtiofsd├── runtime-rs # Rust 实现下的 shimv2│ └── bin│ └── containerd-shim-kata-v2└── share ├── bash-completion # 命令行补全脚本 │ └── completions │ └── kata-runtime ├── defaults # 不同 hypervisor 实现下的静态配置文件 │ └── kata-containers │ ├── configuration-acrn.toml │ ├── configuration-clh.toml │ ├── configuration-dragonball.toml │ ├── configuration-fc.toml │ ├── configuration-qemu.toml │ └── configuration.toml -&gt; configuration-qemu.toml ├── kata-containers # 内核与 guest 镜像 │ ├── config-5.19.2 │ ├── kata-alpine-3.15.initrd │ ├── kata-clearlinux-latest.image │ ├── kata-containers.img -&gt; kata-clearlinux-latest.image │ ├── kata-containers-initrd.img -&gt; kata-alpine-3.15.initrd │ ├── vmlinux-5.19.2-96 │ ├── vmlinux.container -&gt; vmlinux-5.19.2-96 │ ├── vmlinuz-5.19.2-96 │ └── vmlinuz.container -&gt; vmlinuz-5.19.2-96 └── kata-qemu # QEMU 依赖 └── qemu ├── bios-256k.bin ├── bios.bin ├── bios-microvm.bin ├── edk2-aarch64-code.fd ├── edk2-arm-code.fd ├── edk2-arm-vars.fd ├── edk2-i386-code.fd ├── edk2-i386-secure-code.fd ├── edk2-i386-vars.fd ├── edk2-licenses.txt ├── edk2-x86_64-code.fd ├── edk2-x86_64-secure-code.fd ├── efi-virtio.rom ├── firmware │ ├── 50-edk2-i386-secure.json │ ├── 50-edk2-x86_64-secure.json │ ├── 60-edk2-aarch64.json │ ├── 60-edk2-arm.json │ ├── 60-edk2-i386.json │ └── 60-edk2-x86_64.json ├── hppa-firmware.img ├── kvmvapic.bin ├── linuxboot.bin ├── linuxboot_dma.bin ├── multiboot_dma.bin ├── pvh.bin ├── qboot.rom ├── qemu-nsis.bmp ├── s390-ccw.img └── s390-netboot.img 配置参数Kata Containers 中配置的优先级为：动态配置项 &gt; 静态配置项 &gt; 默认值 动态配置项是通过 OCI spec 中的 annotations 传递，主流的 Container Engine 均实现支持将容器 annotations 透传至 Kata 运行时 各个动态与静态配置项支持与否视 hypervisor 具体实现的能力有所区别 hypervisor动态配置项的前缀为 io.katacontainers.hypervisor.&lt;静态配置项&gt; QEMU 静态配置项 动态配置 含义 path Y hypervisor 可执行文件的路径 kernel Y VM 内核路径 image Y VM rootfs 镜像路径，与 initrd 有且仅有一个 initrd Y VM rootfs 镜像路径，与 image 有且仅有一个 machine_type Y QEMU 机器类型，例如 amd64 架构下为 q35、arm64 架构下为 virt confidential_guest N 是否启用机密容器特性。机密容器需要 host 支持 tdxProtection（Intel Trust Domain Extensions）、sevProtection（AMD Secure Encrypted Virtualization）、pefProtection（IBM POWER 9 Protected Execution Facility）以及 seProtection（IBM Secure Execution (IBM Z &amp; LinuxONE)）。不支持 CPU 和内存的热插拔以及 NVDIMM 设备，不支持 arm64 架构 rootless Y 是否以非 root 权限的随机用户启动 QEMU VMM，默认为 false enable_annotations N 允许 hypervisor 动态配置的配置项 valid_hypervisor_paths N 以 glob(3) 规则校验 path 参数是否为合法的路径集合 kernel_params Y VM kernel 的额外附加参数，默认为空 firmware Y 固件路径，默认为空 firmware_volume Y 固件卷路径，默认为空 machine_accelerators Y 机器加速器参数，默认为空 seccompsandbox N seccomp 参数。QEMU seccomp sandbox 是 QEMU VM 中的一种安全特性，通过限制 QEMU 进程的系统调用，以提高 VM 的安全性。它使用了 Linux 内核提供的 seccomp 机制，将 QEMU 进程限制在一组安全的系统调用中，从而降低 VM 遭受攻击的风险。推荐设置 /proc/sys/net/core/bpf_jit_enable 文件内容为 1，以降低该特性带来的性能下降 cpu_features Y CPU 特性参数，默认为空 default_vcpus Y VM 默认的 CPU 数量，默认为 1，最大为 host CPU 数量 default_maxvcpus Y VM 最大的 CPU 数量，默认为 host CPU 数量，具体能否使用到 host CPU 数量，还需要视 hypervisor 限制而定。过大的 CPU 数量会影响到 VM 的性能以及内存占比 default_bridges N VM 默认的 PCI 桥数量，默认为 1，最大为 5。目前，仅支持 PCI bridge，每个 PCI bridge 最多支持 30 个设备的热插拔，每个 VM 最多支持 5 个 PCI bridge（这可能是 QEMU 或内核中的一个 bug） default_memory Y VM 默认的内存总量，默认为 1，最大为 host 内存总量 memory_slots Y VM 默认的内存插槽数量，默认为 10，即内存热添加次数上限为 10 default_maxmemory Y VM 最大的内存总量，默认为 host 内存总量 memory_offset Y VM 内存偏移量，用于描述 NVDIMM 设备的内存空间，当 block_device_driver 为 nvdimm 时，需要设置此参数，最终会追加到 default_maxmemory 中 enable_virtio_mem Y 是否启用 virtio-mem 设备，默认为 false。virtio-mem 设备可以提高 VM 的内存性能。它通过在 host 和 VM 之间共享内存，使 VM 可以直接访问 host 内存，而无需通过复制或传输数据。这种直接访问可显著降低内存访问延迟和 CPU 使用率，并提高 VM 的性能和吞吐量。推荐设置 /proc/sys/vm/overcommit_memory 文件内容为 1 disable_block_device_use Y 禁止块设备用于容器的 rootfs。例如 devicemapper 之类的存储驱动程序中，容器的 rootfs 由块设备支持，出于性能原因，块设备默认直接传递给 hypervisor。 禁用传递时，会用 virtio-fs 传递 rootfs shared_fs Y host 和 VM 之间共享文件系统类型，默认为 virtio-fs，此外支持 virtio-9p 和 virtio-fs-nydus virtio_fs_daemon Y vhost-user-fs 可执行文件的路径 valid_virtio_fs_daemon_paths N 以 glob(3) 规则校验 virtio_fs_daemon 参数是否为合法的路径集合 virtio_fs_cache_size Y DAX 缓存大小，默认为 0 MiB。virtio_fs 支持 DAX（Direct Access）模式，这意味着 VM 可以直接访问 host 的文件系统缓存，从而提高了读取和写入数据的速度 virtio_fs_extra_args Y vhost-user-fs 的额外附加参数 virtio_fs_cache Y virtio-fs 文件系统在 VM 和 host 之间共享文件时的缓存模式，默认是 auto，此外支持 none 和 always。none 表示 VM 中不缓存文件系统的元数据、数据和路径名查找，所有这些信息都需要从 host 中获取。在这种模式下，任何对文件的修改都会立即被推送到 host；alway 则截然相反，表示 VM 中的文件系统元数据、数据和路径名查找都会被缓存，并且永不过期；而 auto 表示 VM 中的元数据和路径名查找缓存会在一定时间后过期（默认为 1 秒），而数据则会在文件打开时缓存（即 close-to-open 一致性）。在这种模式下，VM 会根据需要从 host 中获取文件信息，而不是每次都从 host 获取 block_device_driver Y hypervisor 用于管理容器 rootfs 的块存储驱动程序，默认为 virtio-scsi，此外支持 virtio-blk 和 nvdimm。virtio-scsi 是一种基于SCSI 协议的存储虚拟化技术；virtio-blk 则是一种用于块设备的存储虚拟化技术，在使用 virtio-scsi 和 virtio-blk 时，host 上的块设备可以被 VM 视为本地的块设备，从而可以在 VM 中进行读写操作；nvdimm 是一种用于非易失性内存（NVM）的存储技术。它允许将内存作为块设备使用，并提供了与传统块设备相似的可靠性和数据完整性保护 block_device_aio Y QEMU 使用的块设备异步 I/O 机制，默认为 io_uring，此外支持 threads 和 native。threads 表示 QEMU 使用基于 pthread 的磁盘 I/O 机制，这种机制是在用户空间实现的，可以在多个线程之间共享 CPU 时间，但是性能比较一般；native 表示 QEMU 使用本地的 Linux I/O 机制。这种机制是在内核空间实现的，可以获得更好的性能，但是需要特权；io_uring 表示 QEMU 使用 Linux io_uring API 来实现异步 I/O，这种机制提供了 Linux 中最快的 I/O 操作，可以在 QEMU 5.0 及以上版本中使用，但需要 Linux 内核版本大于 5.1，io_uring 机制可以减少 CPU 的上下文切换次数，提高 I/O 操作的效率 block_device_cache_set Y 是否将缓存相关选项设置给块设备，默认为 false。该参数影响到 block_device_cache_direct 和 block_device_cache_noflush 是否生效 block_device_cache_direct Y 是否启用 O_DIRECT 选项，默认为 false。O_DIRECT 是一种 Linux 系统提供的选项，可以绕过 host 页缓存，直接访问块设备，从而提高存储 I/O 的性能。受 block_device_cache_set 参数设置影响 block_device_cache_noflush Y 是否忽略块设备的缓存刷盘请求，默认为 false。受 block_device_cache_set 参数设置影响 enable_iothreads Y 是否启用独立的 I/O 线程，默认为 false。启用时，块设备的 I/O 操作将在一个单独的 I/O 线程中处理，而非 QEMU 的主线程中进行处理，可以减少了主线程的阻塞时间，提高 VM 的 I/O 性能 enable_mem_prealloc Y 是否启用 VM 内存预分配，默认为 false。启用 VM 内存的预分配可以使内存分配更加稳定和可预测，从而提高 VM 的性能。但是，预分配内存也会占用更多的系统资源，降低容器密度 enable_hugepages Y 是否启用 VM 大页内存，默认为 false。Huge Pages 的特点是将内存分配成固定大小的页（通常为 2MB 或 1GB），从而降低了页表的大小和操作系统内核的开销，使用 Huge Pages 分配 VM 内存可以提升性能。在启用大页内存时，内存预分配（enable_mem_prealloc）会被强制设置启用 enable_vhost_user_store Y 是否启用 vhost-user 存储设备，默认为 false。启用 vhost-user 存储设备可以将 host 上的块设备虚拟化为一种可以在 VM 中使用的设备，通过 vhost-user 协议在 host 和 VM 之间传输数据，从而提高 VM 的存储性能。在启用 vhost-user 存储设备时，Linux 中的一些保留块类型（Major Range 240-254）将被选择用于表示 vhost-user 设备 vhost_user_store_path Y vhost-user 设备的目录，默认为 /var/run/kata-containers/vhost-user。在该目录下，”block” 子目录用于存储块设备，”block/sockets” 子目录用于存储 vhost-user sockets，”block/devices” 子目录用于存储模拟的块设备节点 enable_iommu Y 是否启用 vIOMMU 设备，默认为 false。vIOMMU 用于将 VM 的 I/O 操作隔离在一个独立的内存地址空间中，以提高 VM 的安全性和性能。此外，vIOMMU 还可以提供更好的 I/O 性能，因为它可以减少 VM 和 host 之间的数据传输次数 enable_iommu_platform Y 是否启用 IOMMU_PLATFORM 设备，默认为 false。IOMMU_PLATFORM 用于设备 DMA（Direct Memory Access）操作隔离在一个独立的内存地址空间中，以提高系统的安全性和性能。此外，IOMMU_PLATFORM 还可以提供更好的 DMA 性能，因为它可以减少系统和设备之间的数据传输次数。 valid_vhost_user_store_paths N 以 glob(3) 规则校验 vhost_user_store_path 参数是否为合法的路径集合 file_mem_backend Y 基于文件的内存支持的路径，默认为空。基于文件的 VM 内存支持是一种将 VM 内存保存在文件中的技术，而不是保存在 host 的物理内存中。此外，使用基于文件的 VM 内存还可以减少 VM 和 host 之间的数据传输，从而提高 VM 的性能。在使用 virtio-fs 时，该选项会自动启用，并使用 “/dev/shm” 作为后端文件 valid_file_mem_backends N 以 glob(3) 规则校验 file_mem_backend 参数是否为合法的路径集合 pflashes N 向 VM 中添加的镜像文件路径，默认为空。镜像文件通常用于模拟系统中的 BIOS 或 UEFI 固件等。例如，arm64 架构下的内存热插拔则需要提供一对 pflash enable_debug N 是否启用 hypervisor 和内核的 debug 参数，默认为 false disable_nesting_checks N 是否禁止嵌套虚拟化环境检查，默认为 false。禁用嵌套检查可以从运行时的行为与在裸机上相同 msize_9p Y virtio-9p 共享文件系统中描述 9p 数据包有效载荷的字节数量，默认为 8192 disable_image_nvdimm Y 是否禁止使用 NVDIMM 设备挂载 VM 镜像，默认为 false。在未禁用且支持 NVDIMM 设备时，VM 镜像会借助 NVDIMM 设备热添加，否则，使用 virtio-block 设备 hotplug_vfio_on_root_bus Y 是否允许 VFIO 设备在 root 总线上热插拔，默认为 true。VFIO 是一种用于虚拟化环境中的设备直通技术，它允许将物理设备直接分配给 VM，从而提高 VM 的性能和可靠性。然而，在桥接设备上进行 VFIO 设备的热插拔存在一些限制，特别是对于具有大型 PCI 条的设备。因此，通过将该选项设置为 true，可以在 root 总线上启用 VFIO 设备的热插拔，从而解决这些限制问题 pcie_root_port Y pcie_root_port 设备数量，默认为 0。在热插拔 PCIe 设备之前需要添加 pcie_root_port 设备，主要针对使用一些大型 PCI 条设备（如 Nvidia GPU）的情况。仅在启用 hotplug_vfio_on_root_bus 且 machine_type 为 q35 时生效 disable_vhost_net Y 是否禁用 vhost-net 作为 virtio-net 的后端，默认为 false。使用 vhost-net 时意味着在提高网络 I/O 性能的同时，会牺牲一定的安全性（因为 vhost-net 运行在 ring0 模式下，具有最高的权限和特权） entropy_source Y 熵源路径，默认为 /dev/urandom，用于生成随机数的来源。/dev/random 是一个阻塞的熵源，如果 host 的熵池用尽，VM 的启动时间会增加，可能会导致启动超时。相比之下，/dev/urandom 是一个非阻塞的熵源，可以适用于大多数场景 valid_entropy_sources N 以 glob(3) 规则校验 entropy_source 参数是否为合法的路径集合 guest_hook_path Y VM 中 hook 脚本路径，默认为空。hook 必须按照其 hook 类型存储在 guest_hook_path 的子目录中，例如 “guest_hook_path/{prestart,poststart,poststop}”。Kata agent 将扫描这些目录查找可执行文件，按字母顺序将其添加到容器的生命周期中，并在 VM 运行时命名空间中执行 rx_rate_limiter_max_rate Y 网络 I/O inbound 带宽限制，默认为 0，即不作限制。在 QEMU 中，借助 HTB(Hierarchy Token Bucket) 限制管理 tx_rate_limiter_max_rate Y 网络 I/O outbound 带宽限制，默认为 0，即不作限制。在 QEMU 中，借助 HTB(Hierarchy Token Bucket) 限制管理 guest_memory_dump_path N VM 内存转储文件路径，默认为空。在出现 GUEST_PANICKED 事件时，VM 的内存将被转储到 host 文件系统下的指定目录中（如果该目录不存在，会自动创建）。被转储的文件（也称为 vmcore 文件）可以使用 crash 或 gdb 等工具进行处理。注意，转储 VM 内存可能需要很长时间，具体取决于 VM 内存的大小，并且会占用大量磁盘空间 guest_memory_dump_paging N 是否启用 VM 内存分页，默认为 false。在 VM 内存转储时，将使用分页机制来处理虚拟地址和物理地址之间的映射关系。如果禁用该选项，则将使用物理地址而不是虚拟地址来进行转储。比如，如果希望使用 gdb 工具而不是 crash 工具，或者需要在 ELF vmcore 中使用VM 的虚拟地址，那么则需要启用内存分页功能 enable_guest_swap Y 是否启用 VM 中的交换空间，默认为 false。启用时，会将一个 raw 格式的设备添加到 VM 中作为 SWAP 设备。如果 annotations[“io.katacontainers.container.resource.swappiness”] 大于 0，则根据 annotations[“io.katacontainers.container.resource.swap_in_bytes”] 计算 SWAP 设备大小：默认为 swap_in_bytes - memory_limit_in_bytes；如果 swap_in_bytes 未设置，则为 memory_limit_in_bytes，如果均未设置，则为 default_memory use_legacy_serial Y 是否使用传统的串行接口作为 VM 控制台设备，默认为 false disable_selinux N 是否禁用在 hypervisor 上应用 SELinux，默认为 false factory不支持动态配置项 静态配置项 含义 enable_template 是否启用 VM 模板，默认为 false。 启用后，从模板克隆创建新的 VM。 它们将通过只读映射共享相同的内核、initramfs 和 Kata agent 内存。 如果在同一 host 上运行许多 Kata 容器，VM 模板有助于加快容器的创建并节省大量内存。仅支持镜像类型为 initrd template_path VM 模板保存的路径，默认为 /run/vc/vm/template vm_cache_number VMCache 的数量，默认为 0，表示禁用 VMCache。VMCache 是一种在使用之前将 VM 创建为缓存的功能，有助于加快容器的创建。 该功能由服务器和通过 Unix socket 进行通信的客户端组成，服务器将创建一些 VM 并缓存起来。如果启用了 VMCache 功能，kata-runtime 在创建新的 sandbox 时会向 VMCache 服务器请求 VM vm_cache_endpoint VMCache 服务器的 socket 地址，默认为 /var/run/kata-containers/cache.sock runtime动态配置项的前缀为 io.katacontainers.config.runtime.&lt;静态配置项&gt; 静态配置项 动态配置 含义 enable_debug N 是否启用 containerd-shim-kata-v2 的 debug 参数，默认为 false internetworking_model Y VM 与容器网络的连通方式，默认为 tcfilter，此外支持 tcfilter、macvtap 和 none。无论哪种方式，tap 设备都是创建的，区别在于 tap 设备和容器网络是如何打通的 disable_guest_seccomp Y 是否在 VM 中启用 seccomp 特性，默认为 false。启用时，seccomp 配置文件会由 Kata agent 传递到 VM 中并应用，用于提供额外的安全层 enable_tracing N 是否启用 opentracing 的 traces 和 spans，默认为 false jaeger_endpoint N Jaeger 服务地址，默认为 http://localhost:14268/api/traces jaeger_user N Jaeger 服务账号，默认为空 jaeger_password N Jaeger 服务密码，默认为空 disable_new_netns Y 是否禁止为 shim 和 hypervisor 进程创建网络命名空间，默认为 false。适用于 internetworking_model 为 none，此时 tap 设备将位于 host 网络命名空间中，并可以直接连接到 bridge（如 OVS） sandbox_cgroup_only Y 是否仅启用 sandboxCgroup，默认为 false。启用时，cgroups 仅有一个 sandboxCgroup，用于限制所有的 Kata 进程；禁用时，cgroups 分为 sandboxCgroup 和 overheadCgroup，除 vCPU 线程外的其他 Kata 进程和线程都将在 overheadCgroup 下运行 static_sandbox_resource_mgmt N 是否启用静态资源管理，默认为 false。启用时，Kata Containers 将在 VM 启动之前尝试确定适当的资源大小，而非动态更新 VM 中的内存和 CPU 数量，用作不支持 CPU 和内存热插拔的硬件架构或 hypervisor 解决方案 sandbox_bind_mounts N VM 中待挂载 host 的文件路径，默认为空。启用时，host 的该路径文件会被以只读的形式挂载到 VM 的 /run/kata-containers/shared/containers/sandbox-mounts 路径中，不会暴露给容器工作负载，仅为潜在的 VM 服务提供 vfio_mode Y VFIO 的模式，默认为 guest-kernel，可选的有 vfio 和 guest-kernel。vfio 与 runC 的行为相近，在容器中，VFIO 设备将显示为 VFIO 字符设备，位于 /dev/vfio 下，确切的名称可能与 host 不同（需要匹配 VM 的 IOMMU 组号，而不是 host 的）；guest-kernel 是 Kata 特有的行为，VFIO 设备由 VM 内核中的驱动程序管理，意味着它将显示为一个或多个设备节点或网络接口，具体取决于设备的特性。这种模式要求容器内的工作负载具有显式支持 VM 内设备的代码或逻辑 disable_guest_empty_dir N 是否禁用在 VM 文件系统创建 emptyDir 挂载点，默认为 false。禁用时，Kata Containers 将不会在 VM 文件系统上创建 Kubernetes emptyDir 挂载点，而是在 host 上创建 emptyDir 挂载点，并通过 virtio-fs 共享，虽然更慢一些，但允许从 host 共享文件到 VM 中 experimental Y 体验特性，默认为空。暂未有支持的体验特性 enable_pprof Y 是否启用 pprof，默认为 false。启用后，可以通过 kata-monitor 运行 pprof 工具来分析 shim 进程 annotation 参数扩展Kata Containers 可以通过 annotation 的方式定制化每一个 Kata 容器的底层运行时参数： 上层容器运行时将 annotation 透传至底层运行时（例如 Containerd 1.4.x 以上的版本支持 annotation 透传；CRI-O 默认透传所有 annotation，无需额外配置。具体参考 Container Manager 集成） Kata Containers 配置中开启识别特定的 annotation（[hypervisor].enable_annotations） 此外，Kata Containers 支持 OCI 和容器级别的配置，例如： OCI 配置 配置项 含义 io.katacontainers.config_path Kata Containers 配置文件路径 io.katacontainers.pkg.oci.bundle_path OCI bundle 路径 io.katacontainers.pkg.oci.container_type OCI 容器类型，可选的有 pod_container 和 pod_sandbox 容器配置 配置项 含义 io.katacontainers.container.resource.swappiness 即 Resources.Memory.Swappiness，用于配置容器内存管理器在何时将内存页面写入 SWAP 空间的一个相对度量。该参数的值介于 0 和 100 之间，表示内存页面的使用频率 io.katacontainers.container.resource.swap_in_bytes 即 Resources.Memory.Swap，用于配置容器可以使用的 SWAP 空间的大小 例如，通过 annotation 启动一个忽略底层默认大小，具有 5 CPUs 的 VM： 123456789101112apiVersion: v1kind: Podmetadata: name: kata annotations: io.katacontainers.config.hypervisor.default_vcpus: &quot;5&quot;spec: runtimeClassName: kata containers: - name: kata image: busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;] 与 Container Manager 集成DockerTODO：Docker 23.0.0 版本，新增了运行时 shim 的支持，也就支持了 Kata Containers Containerd123# 生成 Containerd 默认的配置文件$ sudo mkdir -p /etc/containerd$ containerd config default | sudo tee /etc/containerd/config.toml 可以看到，Containerd 的默认底层运行时为 runC，新增以下内容支持 Kata Containers： 12345678[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata] runtime_type = &quot;io.containerd.kata.v2&quot; privileged_without_host_devices = true pod_annotations = [&quot;io.katacontainers.*&quot;] container_annotations = [&quot;io.katacontainers.*&quot;] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata.options] ConfigPath = &quot;/opt/kata/share/defaults/kata-containers/configuration.toml&quot; CRI-OTODO 至此，可以单独通过 Container Manager 各自的命令行运行 Kata Containers，以 Containerd 为例： 123$ sudo ctr image pull docker.io/library/ubuntu:latest$ sudo ctr run --runtime io.containerd.run.kata.v2 -t --rm docker.io/library/ubuntu:latest hello sh -c &quot;free -h&quot;$ sudo ctr run --runtime io.containerd.run.kata.v2 -t --memory-limit 536870912 --rm docker.io/library/ubuntu:latest hello sh -c &quot;free -h&quot; 与 Kubernetes 集成Kubernetes 中对于运行时的集成是通过 RuntimeClass 资源对象，例如 123456789101112kind: RuntimeClassapiVersion: node.k8s.io/v1metadata: name: kata-containershandler: kataoverhead: podFixed: memory: &quot;140Mi&quot; cpu: &quot;250m&quot;scheduling: nodeSelector: runtime: kata handler需要和 CRI 中注册的 handler（HANDLER_NAME） 保持一致。 Containerd 1[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.${HANDLER_NAME}] CRI-O 1[crio.runtime.runtimes.${HANDLER_NAME}] scheduling通过为 RuntimeClass 指定 scheduling 字段， 可以通过设置约束，确保运行该 RuntimeClass 的 Pod 被调度到支持该 RuntimeClass 的节点上。 如果未设置 scheduling，则假定所有节点均支持此 RuntimeClass 。 为了确保 Pod 会被调度到支持指定运行时的节点上，每个节点需要设置一个通用的 label 用于被 runtimeclass.scheduling.nodeSelector 挑选。在 admission 阶段，RuntimeClass 的 nodeSelector 将会与 Pod 的 nodeSelector 合并，取二者的交集。如果有冲突，Pod 将会被拒绝。 如果节点需要阻止某些需要特定 RuntimeClass 的 Pod，可以在 tolerations 中指定。 与 nodeSelector 一样，tolerations 也在 admission 阶段与 Pod 的 tolerations 合并，取二者的并集。 overhead在节点上运行 Pod 时，Pod 本身占用大量系统资源。这些资源是运行 Pod 内容器所需资源的附加资源。Overhead 是一个特性，用于计算 Pod 基础设施在容器请求和限制之上消耗的资源。 在 Kubernetes 中，Pod 的开销是根据与 Pod 的 RuntimeClass 相关联的开销在准入控制时设置的。 如果启用了 Pod Overhead，在调度 Pod 时，除了考虑容器资源请求的总和外，还要考虑 Pod 开销。 类似地，Kubelet 将在确定 Pod cgroups 的大小和执行 Pod 驱逐排序时也会考虑 Pod 开销。 VM factoryVMCacheVMCache 是一项新功能，可在使用前将 VM 创建为缓存。它有助于加快新容器的创建。 该功能由借助 Unix socket 通信的一个 gRPC Server 和一些 Client 组成。 VMCache server 将事先创建并缓存一些 VM，它将 VM 转换为 gRPC 格式并在收到 client 请求时返回；grpccache factory 是 VMCache 客户端，它将请求到的 gRPC 格式的 VM 并将其转换回 VM。如果启用了 VMCache 功能，Kata 运行时在创建新的 sandbox 时会向 grpccache 请求获取 VM。 与 VM tmplating 的区别 VM tmplating 和 VMCache 都有助于加快新容器的创建。 当启用 VM tmplating 时，通过从预先创建的模板 VM 克隆来创建新的 VM，它们将以只读模式共享相同的 initramfs、内核和 agent 内存。因此，如果在同一主机上运行许多 Kata 容器，它会节省大量内存。 而 VMCache 不容易受到共享内存 CVE 的影响，因为每个 VM 不共享内存。 如何启用 VM Cache 配置文件中修改以下配置项： [factory].vm_cache_number 指定 VM 缓存的个数 [factory].vm_cache_endpoint 指定 socket 地址（自动创建），默认为 /var/run/kata-containers/cache.sock 通过以下命令创建一个 VM 模板供以后使用，通过 CTRL+C 退出： 1$ kata-runtime factory init 区别于 VM templating，VMCache 创建的 VM 是处于运行状态，而非保存在 [factory].template_path 目录下 123456789101112$ kata-runtime factory statusVM cache server pid = 38308VM pid = 38334 Cpu = 1 Memory = 2048MiBVM pid = 38331 Cpu = 1 Memory = 2048MiBVM pid = 38332 Cpu = 1 Memory = 2048MiBvm factory not enabled$ ls -la /run/vc/vma78a9744-5984-4e54-bda9-9b6280bf9a3f41648333-a4a3-48ee-b80b-f7c19e3081b157d2dd69-0e73-4779-b23f-68ee8e4f66detemplate 12$ kata-runtime factory destroyvm factory destroyed 已知限制 无法与 VM templating 共存 仅支持 QEMU 作为 hypervisor [hypervisor].shared_fs 为 virtio-9p（社区有支持 virtio-fs 的提案 https://github.com/kata-containers/kata-containers/pull/4522，但截至 Kata 3.0.0 暂未合入） 经验证，截至 Kata 3.0.0，VMCache 并不能开箱即用，在 VMCache 流程中部分变量缺少赋值，导致代码报错 VM templatingVM templating 是 Kata Containers 的一项功能，可以借助克隆技术创建新的 VM。启用后，新的 VM 将通过从预先创建的模板进行克隆来创建，它们将以只读模式共享相同的 initramfs、内核和 agent 内存。类似于内核的 fork 进程操作，这里 fork 的是 VM。 与 VMCache 的区别 VMCache 和 VM templating 都有助于加快新容器的创建。 启用 VMCache 后，VMCache 服务器会创建新的 VM。所以它不容易受到共享内存 CVE 的攻击，因为每个 VM 都不共享内存。 如果在同一主机上运行许多 Kata 容器，VM templating 可以节省大量内存。 优势 如果在同一主机上运行许多 Kata 容器，VM templating 有助于加快新容器的创建并节省大量内存。如果正在运行高密度工作负载，或者非常关心容器启动速度，VM templating 可能非常有用。 在一个示例中，创建了 100 个 Kata 容器，每个容器都拥有 128MB 的 VM 内存，并且在启用 VM templating 特性时最终总共节省了 9GB 的内存，这大约是 VM 内存总量的 72%。 在另一个示例中，创建了 10 个 Kata 容器，并计算了每个容器的平均启动速度。结果表明，VM templating 将 Kata 容器的创建速度提高了 38.68%。 不足 VM templating 的一个缺点是它无法避免跨 VM 侧通道攻击，例如最初针对 Linux KSM 功能的 CVE-2015-2877。得出的结论是，“相互不信任的租户之间用于内存保护的共享直到写入的方法本质上是可检测的信息泄露，并且可以归类为潜在的被误解的行为而不是漏洞。”如果对此敏感，不要使用 VM templating 或 KSM。 如何启用 VM templating 配置文件中修改以下配置项： hypervisor 为 qemu，且版本为 v4.1.0 以上 [factory].enable_template 设为 true VM 镜像为 initrd 类型，即为 [hypervisor].initrd [hypervisor].shared_fs 为 virtio-9p 通过以下命令创建一个 VM 模板： 12$ kata-runtime factory initvm factory initialized 创建的模板默认保存在 /run/vc/vm/template，可以通过 [factory].template_path 指定： 12$ ls /run/vc/vm/templatememory state 模板通过以下命令销毁： 12$ kata-runtime factory destroyvm factory destroyed 如果不想手动调用 kata-runtime factory init，在启用 VM templating 后，默认创建的第一个 Kata 容器将自动创建一个 VM 模板。 kata-runtimekata-runtime 是一个命令行工具，支持以下功能： check (kata-check)检测当前环境是否可以运行 Kata Containers 以及版本是否正确。 12345678910111213141516$ kata-runtime check --verboseINFO[0000] IOMMUPlatform is disabled by default. WARN[0000] Not running network checks as super user arch=amd64 name=kata-runtime pid=29825 source=runtimeINFO[0000] CPU property found arch=amd64 description=&quot;Intel Architecture CPU&quot; name=GenuineIntel pid=29825 source=runtime type=attributeINFO[0000] CPU property found arch=amd64 description=&quot;Virtualization support&quot; name=vmx pid=29825 source=runtime type=flagINFO[0000] CPU property found arch=amd64 description=&quot;64Bit CPU&quot; name=lm pid=29825 source=runtime type=flagINFO[0000] CPU property found arch=amd64 description=SSE4.1 name=sse4_1 pid=29825 source=runtime type=flagINFO[0000] kernel property found arch=amd64 description=&quot;Intel KVM&quot; name=kvm_intel pid=29825 source=runtime type=moduleINFO[0000] kernel property found arch=amd64 description=&quot;Kernel-based Virtual Machine&quot; name=kvm pid=29825 source=runtime type=moduleINFO[0000] kernel property found arch=amd64 description=&quot;Host kernel accelerator for virtio&quot; name=vhost pid=29825 source=runtime type=moduleINFO[0000] kernel property found arch=amd64 description=&quot;Host kernel accelerator for virtio network&quot; name=vhost_net pid=29825 source=runtime type=moduleINFO[0000] kernel property found arch=amd64 description=&quot;Host Support for Linux VM Sockets&quot; name=vhost_vsock pid=29825 source=runtime type=moduleSystem is capable of running Kata ContainersINFO[0000] device available arch=amd64 check-type=full device=/dev/kvm name=kata-runtime pid=29825 source=runtimeINFO[0000] feature available arch=amd64 check-type=full feature=create-vm name=kata-runtime pid=29825 source=runtimeSystem can currently create Kata Containers 可选的 flags 包括： 名称 含义 –check-version-only 仅对比前使用版本和最新可用版本（需要网络支持，且非 root 用户） –include-all-releases 包含过滤预发布的版本 –no-network-checks, -n 不借助网络执行检测，该参数等价于设置 KATA_CHECK_NO_NETWORK 环境变量 –only-list-releases 仅列出较新的可用版本（需要网络支持，且非 root 用户） –strict, -s 进行严格检查 –verbose, -v 展示详细的检查项 env (kata-env)Kata Containers 配置展示，默认输出格式为 TOML。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667$ kata-runtime env [Kernel] Path = &quot;/opt/kata/share/kata-containers/vmlinux-5.19.2-96&quot; Parameters = &quot;systemd.unit=kata-containers.target systemd.mask=systemd-networkd.service systemd.mask=systemd-networkd.socket scsi_mod.scan=none agent.log=debug agent.debug_console agent.debug_console_vport=1026&quot;[Meta] Version = &quot;1.0.26&quot;[Image] Path = &quot;/opt/kata/share/kata-containers/kata-clearlinux-latest.image&quot;[Initrd] Path = &quot;&quot;[Hypervisor] MachineType = &quot;q35&quot; Version = &quot;QEMU emulator version 6.2.0 (kata-static)\\nCopyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers&quot; Path = &quot;/opt/kata/bin/qemu-system-x86_64&quot; BlockDeviceDriver = &quot;virtio-scsi&quot; EntropySource = &quot;/dev/urandom&quot; SharedFS = &quot;virtio-fs&quot; VirtioFSDaemon = &quot;/opt/kata/libexec/virtiofsd&quot; SocketPath = &quot;&quot; Msize9p = 8192 MemorySlots = 10 PCIeRootPort = 2 HotplugVFIOOnRootBus = true Debug = true[Runtime] Path = &quot;/usr/local/bin/kata-runtime&quot; Debug = true Trace = false DisableGuestSeccomp = true DisableNewNetNs = false SandboxCgroupOnly = false [Runtime.Config] Path = &quot;/etc/kata-containers/configuration.toml&quot; [Runtime.Version] OCI = &quot;1.0.2-dev&quot; [Runtime.Version.Version] Semver = &quot;3.0.0&quot; Commit = &quot;e2a8815ba46360acb8bf89a2894b0d437dc8548a-dirty&quot; Major = 3 Minor = 0 Patch = 0[Host] Kernel = &quot;4.18.0-305.43.25.ar.el7.x86_64&quot; Architecture = &quot;amd64&quot; VMContainerCapable = true SupportVSocks = true [Host.Distro] Name = &quot;CentOS Linux&quot; Version = &quot;7&quot; [Host.CPU] Vendor = &quot;GenuineIntel&quot; Model = &quot;QEMU Virtual CPU version (cpu64-rhel6)&quot; CPUs = 8 [Host.Memory] Total = 12057632 Free = 3352124 Available = 8508112[Agent] Debug = true Trace = false 可选的 flags 包括 名称 含义 –json 以 JSON 格式展示 exec借助 debug console，进入 VM 控制台，需要 [agent].debug_console_enabled 设置为 true。 12# 对于 Pod 而言是其 SandboxID$ kata-runtime exec 27ab74433f11c0b64e404a841d5e2f8296a723ebfa4e598b4d9d32871173b82c 可选的 flags 包括 名称 含义 –kata-debug-port debug console 监听的端口，默认为 1026 或者 0 metrics收集与用于运行 sandbox 的基础设施相关的指标，例如 runtime、agent、hypervisor 等。 1234567891011121314# 对于 Pod 而言是其 SandboxID$ kata-runtime metrics 27ab74433f11c0b64e404a841d5e2f8296a723ebfa4e598b4d9d32871173b82c# HELP kata_hypervisor_fds Open FDs for hypervisor.# TYPE kata_hypervisor_fds gaugekata_hypervisor_fds 122# HELP kata_hypervisor_io_stat Process IO statistics.# TYPE kata_hypervisor_io_stat gaugekata_hypervisor_io_stat{item=&quot;cancelledwritebytes&quot;} 0kata_hypervisor_io_stat{item=&quot;rchar&quot;} 5.915546e+06kata_hypervisor_io_stat{item=&quot;readbytes&quot;} 1.1665408e+07kata_hypervisor_io_stat{item=&quot;syscr&quot;} 95522kata_hypervisor_io_stat{item=&quot;syscw&quot;} 202276kata_hypervisor_io_stat{item=&quot;wchar&quot;} 3.715404e+06kata_hypervisor_io_stat{item=&quot;writebytes&quot;} 2.097152e+06 direct-volume管理 Kata Containers 的直通卷。*具体使用方式参考 Kata Containers Block Volume 直通说明。* add 1$ kata-runtime direct-volume add --volume-path /var/lib/kubelet/pods/8c3d29ad-84b8-45f0-9fcc-8e16778cb3cb/volumes/kubernetes.io~csi/pvc-a950ed68-622c-4ec4-81fa-506f16de2196/mount --mount-info \\{\\&quot;volume-type\\&quot;:\\&quot;block\\&quot;,\\&quot;device\\&quot;:\\&quot;/dev/sdm\\&quot;,\\&quot;fstype\\&quot;:\\&quot;xfs\\&quot;\\} 可选的 flags 包括 名称 含义 –volume-path 待操作的目标卷路径 –mount-info 管理卷挂载的详情信息 remove 1$ kata-runtime direct-volume delete --volume-path /var/lib/kubelet/pods/8c3d29ad-84b8-45f0-9fcc-8e16778cb3cb/volumes/kubernetes.io~csi/pvc-a950ed68-622c-4ec4-81fa-506f16de2196/mount 可选的 flags 包括 名称 含义 –volume-path 待操作的目标卷路径 stats 1$ kata-runtime direct-volume stats --volume-path /var/lib/kubelet/pods/8c3d29ad-84b8-45f0-9fcc-8e16778cb3cb/volumes/kubernetes.io~csi/pvc-a950ed68-622c-4ec4-81fa-506f16de2196/mount 可选的 flags 包括 名称 含义 –volume-path 待操作的目标卷路径 resize 截至 Kata Containers 3.0.0，社区仍未实现 VM 中 Kata agent 的逻辑 1$ kata-runtime direct-volume resize --volume-path /var/lib/kubelet/pods/8c3d29ad-84b8-45f0-9fcc-8e16778cb3cb/volumes/kubernetes.io~csi/pvc-a950ed68-622c-4ec4-81fa-506f16de2196/mount --size 1756519562 可选的 flags 包括 名称 含义 –volume-path 待操作的目标卷路径 –size 调整后的预期卷大小（单位为：Byte） factory管理 Kata Containers 的 VM factory。具体使用方式参考 VM factory 说明。 init 12$ kata-runtime factory initvm factory initialized status 12$ kata-runtime factory statusvm factory is on destroy 12$ kata-runtime factory destroyvm factory destroyed iptables管理 VM 中的 iptables 信息。 get 1$ kata-runtime iptables get --sandbox-id xxx --v6 可选的 flags 包括 名称 含义 –sandbox-id 待操作的 Sandbox ID –v6 获取 IPV6 的 iptables set 1$ kata-runtime iptables set --sandbox-id xxx --v6 ./iptables 可选的 flags 包括 名称 含义 –sandbox-id 待操作的 Sandbox ID –v6 设置 IPV6 的 iptables kata-monitorKata monitor 是一个守护进程，能够收集和暴露在同一 host 上运行的所有 Kata 容器工作负载相关的指标。 12$ kata-monitorINFO[0000] announce app=kata-monitor arch=amd64 git-commit=fcad969e5200607df3b0b31983cc64488e156e99 go-version=go1.16.10 listen-address=&quot;127.0.0.1:8090&quot; log-level=info os=linux runtime-endpoint=/run/containerd/containerd.sock version=0.3.0 可选的 flags 包括 名称 含义 –listen-address 监听 HTTP 请求的地址，默认为 127.0.0.1:8090 –log-level 服务日志级别，可选有 trace/debug/info/warn/error/fatal/panic，默认为 info –runtime-endpoint CRI 容器运行时服务的 socket 地址，默认为 /run/containerd/containerd.sock","link":"/2021/04/15/2021-04-15%20Kata%20Containers%20%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/"},{"title":"「 Velero 」操作元数据","text":"based on v1.6.3 部署 MinIO 服务以 Velero 提供的 MinIO 服务 进行 DEMO 验证，为了便于操作将 ClusterIP 改为 NodePort 部署结果123456789101112131415# kubectl get all -n velero -l component=minioNAME READY STATUS RESTARTS AGEpod/minio-54b5867494-28dvt 1/1 Running 0 80sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/minio NodePort 10.96.107.103 &lt;none&gt; 9000:30188/TCP 80sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/minio 1/1 1 1 80sNAME DESIRED CURRENT READY AGEreplicaset.apps/minio-54b5867494 1 1 1 80sNAME COMPLETIONS DURATION AGEjob.batch/minio-setup 1/1 6s 80s 账号密码通过 MinIO 服务中 MINIO_ACCESS_KEY 和 MINIO_SECRET_KEY 环境变量设置，登陆后效果如下： 其中，minio-setup 的 Job 已经创建出了一个名为 velero 的 Bucket 准备认证文件12345$ cat &gt; credentials-velero &lt;&lt;EOF[default]aws_access_key_id = minioaws_secret_access_key = minio123EOF 部署 Velero 服务命令行安装1234567$ velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.0.0 \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-volume-snapshots=false \\ --backup-location-config region=minio,s3ForcePathStyle=&quot;true&quot;,s3Url=http://minio.velero.svc:9000 参数说明 provider 指定 plugin 的 provider，默认格式为 x/y，如果省略 x 部分，则默认为 velero.io，需要与 plugin 注册的保持一致 plugins 指定 plugin 使用的镜像 bucket 为对象存储中的存储桶概念 secret-file 用于与后端存储服务认证的信息，如果存储服务不需要凭证，则将 secret-file 替换成 no-secret use-volume-snapshots 是否自动创建一个 SnapshotLocation，如果不打算通过创建卷快照，则设置为 false，默认为 true backup-location-config 为默认的 BackupStorageLocation 信息，可以在部署 Velero 后增量配置 部署结果123456789$ kubectl get all -n velero -l component=veleroNAME READY STATUS RESTARTS AGEpod/velero-64b8fddd66-fqdvt 1/1 Running 0 32sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/velero 1/1 1 1 32sNAME DESIRED CURRENT READY AGEreplicaset.apps/velero-64b8fddd66 1 1 1 32s 12345678910111213$ kubectl get crd -l component=veleroNAME CREATED ATbackups.velero.io 2022-11-07T08:54:56Zbackupstoragelocations.velero.io 2022-11-07T08:54:56Zdeletebackuprequests.velero.io 2022-11-07T08:54:56Zdownloadrequests.velero.io 2022-11-07T08:54:56Zpodvolumebackups.velero.io 2022-11-07T08:54:56Zpodvolumerestores.velero.io 2022-11-07T08:54:56Zresticrepositories.velero.io 2022-11-07T08:54:56Zrestores.velero.io 2022-11-07T08:54:56Zschedules.velero.io 2022-11-07T08:54:56Zserverstatusrequests.velero.io 2022-11-07T08:54:56Zvolumesnapshotlocations.velero.io 2022-11-07T08:54:56Z 123$ velero backup-location getNAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULTdefault aws velero Available 2022-11-07 16:55:06 +0800 CST ReadWrite true 流程验证以 Velero 提供的基础 nginx 服务进行 DEMO 验证 12345678910111213$ kubectl get all -n nginx-exampleNAME READY STATUS RESTARTS AGEpod/nginx-deployment-5bcc46cc5-fmnf4 1/1 Running 0 106spod/nginx-deployment-5bcc46cc5-njlhk 1/1 Running 0 106sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/my-nginx LoadBalancer 10.96.158.105 &lt;pending&gt; 80:30449/TCP 106sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nginx-deployment 2/2 2 2 106sNAME DESIRED CURRENT READY AGEreplicaset.apps/nginx-deployment-5bcc46cc5 2 2 2 106s 操作验证 1234567891011121314151617181920212223242526272829# 创建备份任务$ velero backup create default$ velero backup getNAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTORdefault Completed 0 0 2022-11-07 17:03:46 +0800 CST 29d default &lt;none&gt;# 模拟故障$ kubectl delete ns nginx-example# 创建恢复任务$ velero restore create default --from-backup default$ velero restore getNAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTORdefault-20221107170719 default Completed 2022-11-07 17:07:19 +0800 CST 2022-11-07 17:08:19 +0800 CST 0 119 2022-11-07 17:07:19 +0800 CST &lt;none&gt;# 恢复结果查看$ kubectl get all -n nginx-exampleNAME READY STATUS RESTARTS AGEpod/nginx-deployment-5bcc46cc5-fmnf4 1/1 Running 0 90spod/nginx-deployment-5bcc46cc5-njlhk 1/1 Running 0 90sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/my-nginx LoadBalancer 10.96.27.235 &lt;pending&gt; 80:31162/TCP 59sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nginx-deployment 2/2 2 2 73sNAME DESIRED CURRENT READY AGEreplicaset.apps/nginx-deployment-5bcc46cc5 2 2 2 89s 可以初步看到以下几个特点： Deployment 和 Pod 在恢复之后，UUID 并未变化 Service 和 Pod 的 IP 与端口在恢复之后，被重新分配 MinIO 中相关的数据 备份与恢复数据结构Backup 数据结构 name content velero-backup.json Backup 对象的 Json 格式 default-volumesnapshots.json.gz Velero 中 VolumeSnapshots 对象的 Json 格式 default-podvolumebackups.json.gz PodvolumeBackups 对象的 Json 格式 default-csi-volumesnapshots.json.gz CSI 中 VolumeSnapshots 对象的 Json 格式 default-csi-volumesnapshotcontents.json.gz CSI 中 VolumeSnapshotsContent 对象的 Json 格式 default-logs.gz 备份任务日志 default.tar.gz 备份的全部数据，包括两子内容：metadata 和 resources，metadata 文件夹中包含一个 verison 文件，内容为 1.1.0；resources 文件夹中包含各类资源全名的子文件夹，例如 alertmanagers.monitoring.coreos.com，里面包含以 namespaces 或 cluster 区分的资源对象的 Json 格式 default-resource-list.json.gz 备份的资源清单，格式为 {“资源全名”: [各资源信息，格式为 ns/name]} Restore 数据结构 name content restore-default-20221107170719-results.gz 恢复的详情信息，格式为 {“errors”:{},”warnings”:{“cluster”: [“各资源恢复异常原因”], “各 namespace”: [“各资源恢复异常原因”]}} restore-default-20221107170719-logs.gz 恢复任务日志","link":"/2021/06/28/2021-06-28%20Velero%20%E6%93%8D%E4%BD%9C%E5%85%83%E6%95%B0%E6%8D%AE/"},{"title":"「 Rust 」快速开始","text":"简介Rust 是由 Mozilla 主导开发的通用、编译型编程语言。设计准则为“安全、并发、实用”，支持函数式、并发式、过程式以及面向对象的编程风格。Rust 语言原本是 Mozilla 员工 Graydon Hoare 的私人计划，而 Mozilla 于 2009 年开始赞助这个计划，并且在 2010 年首次公开。也在同一年，其编译器源代码开始由原本的 OCaml 语言转移到用 Rust 语言，进行 bootstrapping 工作，称做 “rustc”，并于 2011 年实际完成。这个可自我编译的编译器在架构上采用了 LLVM 做为它的后端。第一个有版本号的 Rust 编译器于 2012 年 1 月发布。Rust 1.0 是第一个稳定版本，于 2015 年 5 月 15 日发布。Rust 是在完全开放的情况下进行开发，并且相当欢迎社区的反馈。在 1.0 稳定版之前，语言设计也因为透过撰写 Servo 网页浏览器排版引擎和 rustc 编译器本身，而有进一步的改善。虽然它由 Mozilla 资助，但它其实是一个共有项目，有很大部分的代码是来自于社区的贡献者。 安装Rust 官网https://www.rust-lang.org Rustup 安装Rustup 是一个针对 Rust 语言的工具链管理器（toolchain manager），其目标是让交叉编译 Rust 代码更加简单。Rustup 是一个命令行应用，能够下载并在不同版本的 Rust 工具链中进行切换 —— 如编译器 rustc和标准库，该应用所支持的平台数量不少。事实上，rustc 本身就支持大约 56 个平台，而 rustup 实际上能够为其中 14 个平台管理编译器，为 30 个平台管理标准库。 相关概念channelRust 发布在三个不同的 channel 上：stable，beta 和 nightly，其实就是三种不同的版本 stable — Rust 的稳定版本，每 6 周发布一次。 beta — Rust 的公开测试版本，将是下一个 stable 版本 nightly — 每天更新，包含一些实验性的新特性 toolchain工具链的标准命名格式： 12345&lt;channel&gt;[-&lt;date&gt;][-&lt;host&gt;]&lt;channel&gt; = stable|beta|nightly|&lt;version&gt;&lt;date&gt; = YYYY-MM-DD&lt;host&gt; = &lt;target-triple&gt; 工具链默认被安装在 RUSTUP_HOME （Unix系统：~/.rustup ，Windows系统：%USERPROFILE%/.rustup）目录下。 components工具链由若干组件构成，通过 rustup component list 命令可以查看所有可用和已经安装的组件。 Rustup 默认安装的组件有： rustc — Rust 编译器 rust-std — Rust 标准库 cargo — 包管理和构建工具 rust-docs — Rust 文档 rustfmt — 用来格式化 Rust 源代码 clippy — Rust 的代码检查工具 profile不同的 profile 包含不同的组件，安装 rustup 时有三种 profile 可选： minimal — 包含 rustc、rust-std、cargo default — 包含 rustc、rust-std、cargo、rust-docs、rustfmt、clippy complete — 包含所有组件 可以使用 rustup set profile 命令修改 profile，比如：rustup set profile minimal。 macOS &amp; linux123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | shinfo: downloading installerWelcome to Rust!This will download and install the official compiler for the Rustprogramming language, and its package manager, Cargo.Rustup metadata and toolchains will be installed into the Rustuphome directory, located at: /Users/shenxianghong/.rustupThis can be modified with the RUSTUP_HOME environment variable.The Cargo home directory located at: /Users/shenxianghong/.cargoThis can be modified with the CARGO_HOME environment variable.The cargo, rustc, rustup and other commands will be added toCargo's bin directory, located at: /Users/shenxianghong/.cargo/binThis path will then be added to your PATH environment variable bymodifying the profile files located at: /Users/shenxianghong/.profile /Users/shenxianghong/.zshenvYou can uninstall at any time with rustup self uninstall andthese changes will be reverted.Current installation options: default host triple: aarch64-apple-darwin default toolchain: stable (default) profile: default modify PATH variable: yes1) Proceed with installation (default)2) Customize installation3) Cancel installation&gt;1info: profile set to 'default'info: default host triple is aarch64-apple-darwininfo: syncing channel updates for 'stable-aarch64-apple-darwin'686.2 KiB / 686.2 KiB (100 %) 527.5 KiB/s in 1s ETA: 0sinfo: latest update on 2021-12-02, rust version 1.57.0 (f1edd0429 2021-11-29)info: downloading component 'cargo' 3.7 MiB / 3.7 MiB (100 %) 1.2 MiB/s in 1s ETA: 0sinfo: downloading component 'clippy'info: downloading component 'rust-std' 23.1 MiB / 23.1 MiB (100 %) 4.0 MiB/s in 6s ETA: 0sinfo: downloading component 'rustc' 59.4 MiB / 59.4 MiB (100 %) 4.5 MiB/s in 15s ETA: 0sinfo: downloading component 'rustfmt'info: installing component 'cargo'info: installing component 'clippy'info: installing component 'rust-std' 23.1 MiB / 23.1 MiB (100 %) 19.4 MiB/s in 1s ETA: 0sinfo: installing component 'rustc' 59.4 MiB / 59.4 MiB (100 %) 21.9 MiB/s in 2s ETA: 0sinfo: installing component 'rustfmt'info: default toolchain set to 'stable-aarch64-apple-darwin' stable-aarch64-apple-darwin installed - rustc 1.57.0 (f1edd0429 2021-11-29)Rust is installed now. Great!To get started you may need to restart your current shell.This would reload your PATH environment variable to includeCargo's bin directory ($HOME/.cargo/bin).To configure your current shell, run:source $HOME/.cargo/env 更新 Rust123456789101112131415161718192021222324252627282930$ rustup updateinfo: syncing channel updates for 'stable-aarch64-apple-darwin'info: latest update on 2021-12-02, rust version 1.57.0 (f1edd0429 2021-11-29)info: downloading component 'rust-src'info: downloading component 'cargo'info: downloading component 'clippy'info: downloading component 'rust-std' 23.1 MiB / 23.1 MiB (100 %) 11.6 MiB/s in 2s ETA: 0sinfo: downloading component 'rustc' 59.4 MiB / 59.4 MiB (100 %) 12.6 MiB/s in 4s ETA: 0sinfo: downloading component 'rustfmt'info: removing previous version of component 'rust-src'info: removing previous version of component 'cargo'info: removing previous version of component 'clippy'info: removing previous version of component 'rust-std'info: removing previous version of component 'rustc'info: removing previous version of component 'rustfmt'info: installing component 'rust-src'info: installing component 'cargo'info: installing component 'clippy'info: installing component 'rust-std' 23.1 MiB / 23.1 MiB (100 %) 18.8 MiB/s in 1s ETA: 0sinfo: installing component 'rustc' 59.4 MiB / 59.4 MiB (100 %) 21.3 MiB/s in 2s ETA: 0sinfo: installing component 'rustfmt'info: checking for self-updates stable-aarch64-apple-darwin updated - rustc 1.57.0 (f1edd0429 2021-11-29) (from rustc 1.53.0 (53cb7b09b 2021-06-17))info: cleaning up downloads &amp; tmp directories 卸载 Rust123456789101112$ rustup self uninstallThanks for hacking in Rust!This will uninstall all Rust toolchains and data, and remove$HOME/.cargo/bin from your PATH environment variable.Continue? (y/N) yinfo: removing rustup homeinfo: removing cargo homeinfo: removing rustup binariesinfo: rustup is uninstalled 安装校验12$ rustc --versionrustc 1.57.0 (f1edd0429 2021-11-29) 离线文档即 rust-docs 工具。 1$ rustup doc rustup doc 在 apple m1 架构下，文件不存在，无法打开，报错为 123error: couldn't open browser: command 'open' did not execute successfully; exit status: 1command stderr:The file /Users/shenxianghong/.rustup/toolchains/stable-aarch64-apple-darwin/share/doc/rust/html/index.html does not exist. Workaround 12$ rustup toolchain install stable-x86_64-apple-darwin$ rustup doc --toolchain=stable-x86_64-apple-darwin https://github.com/rust-lang/rustup/issues/2692 示例文件标准 程序文件后缀名：rs 文件命名规范： hello_world.rs（snake case） 编译与运行编译rustc 适合简单的 Rust 程序编译，即 rustc &lt;file&gt;。 1$ rustc main.rs 编译成功后，会生成一个二进制文件，在 Windows 上还会生成一个 .pdb 文件，里面包含调试信息。 类似于 Golang，当待编译的 Rust 程序文件中没有入口函数时，会编译报错 123456789101112$ rustc rust.rserror[E0601]: `main` function not found in crate `rust` --&gt; rust.rs:1:1 |1 | / fn test() {2 | | println!(&quot;test&quot;)3 | | } | |_^ consider adding a `main` function to `rust.rs`error: aborting due to previous errorFor more information about this error, try `rustc --explain E0601`. 运行类似于 Golang，Rust 是 ahead-of-time 编译的语言，可执行文件的运行不依赖于 Rust 环境。 Windows 1$ .\\main.exe Linux &amp; MacOS 1$ ./main 简单剖析123fn main() { println!(&quot;Hello World&quot;);} 定义函数使用 fn 关键字，main 函数的作用为每个 Rust 可执行程序最先运行的代码 Rust 的缩进是 4 个空格，而不是 tab println! 是一个 Rust macro（宏），如果是函数的话，就没有 ! 代码行以 ; 结尾，表示表达式结束，关于表达式和语句的后续会提到","link":"/2022/01/02/2022-01-02%20Rust%20%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/"},{"title":"「 Velero 」快速开始","text":"based on v1.6.3 概述Velero 可以提供备份和还原 Kubernetes 集群资源和持久卷的能力，可以在公有云或本地搭建的私有云环境安装 Velero，可以提供以下能力： 备份集群数据，并在集群故障的情况下进行还原 将集群资源迁移到其他集群 将生产集群复制到开发和测试集群 Velero 包含一个在集群上运行的服务器端和在本地运行的命令行客户端。 Velero 官方文档 安装install1$ velero install 可选的 flags 包括 名称 含义 –backup-location-config 描述 BackupStorageLocation 的配置信息 –bucket BackupStorageLocation 中 bucket 信息，参考对象存储中的概念和 no-default-backup-location 必须存在其一 –cacert 连接 BackupStorageLocation 时所需要的 TLS 证书 –crds-only 仅生成 CRD 资源类别，默认为 false主要用于已经安装 Velero 的集群升级 CRD –crds-version 默认为 v1，指定 CRD 的 resource version –default-restic-prune-frequency 对 Restic repo 执行 restic prune 的默认周期，默认为 1 周 –default-volumes-to-restic 全局参数，表示是否由 Restic 备份所有的 Pod 卷，默认为 false –dry-run 生成资源，但是不会实际创建通常会和 -o 参数一起使用，指定默认输出格式 –image Velero 和 Restic Pod 的镜像，默认和 Velero binary 版本一致 –label-columns 在 table 样式下，自定义表格栏展示 –no-default-backup-location 表示是否创建默认的 BackupStorageLocation，默认为 false和 provider &amp; bucket 必须存在其一 –no-secret 不为 BackupStorageLocation 生成认证 Secret和 –secret-file 必须存在其一 –output dry run，指定生成配置文件的格式，可选的有 table，json 和 yaml –plugins Velero plugin 的镜像 –pod-annotations Velero 和 Restic Pod 中追加的注释信息 –prefix BackupStorageLocation 中 prefix 信息，参考对象存储中的概念 –provider 指定 StorageProvider，例如 aws, gcp 等和 no-default-backup-location 必须存在其一 –restic-pod-cpu-limit/–restic-pod-cpu-request/–restic-pod-mem-limit/–restic-pod-mem-request Restic Pod 的资源限制信息 –restore-only 是否以 Restore-Only 形式运行服务，即 Backup、Schedule 和 GC controller 都会被禁用，仅启动 Restore Controller参数已经废弃，取而代之的是将 BackupStorageLocation 设为 ReadOnly –sa-annotations Velero 的 ServiceAccount 中追加的注释信息 –secret-file BackupStorageLocation 所需要的认证文件Velero 会将该文件以 Secret 形式创建，并挂载在 Velero 的 /cloud/credentials；和 –no-secret 必须存在其一 –show-labels 在 table 样式下，最后一栏展示标签信息 –snapshot-location-config 描述 VolumeSnapshotLocation 的配置信息 –use-restic 是否同时创建 Restic 服务 –use-volume-snapshots 是否自动创建一个 SnapshotLocation，默认为 true –velero-pod-cpu-limit/–velero-pod-cpu-request/–velero-pod-mem-limit/–velero-pod-mem-request Velero Pod 的资源限制信息 –wait 阻塞直至 Velero deployment 是 Ready手动退出等待可以 ctrl-c，并不会影响到安装流程 uninstall1$ velero uninstall 可选的 flags 包括 名称 含义 –force 是否强制卸载，忽略确认信息，默认为 false –wait 阻塞直至 Velero 被完全卸载手动退出等待可以 ctrl-c，并不会影响到卸载流程 CustomizeVelero Server以下参数均为 Velero 服务启动时的额外参数 部分参数可以通过 install 指定，而其余参数只能通过定制化 Velero Deployment 的启动参数 名称 含义 –log-level 日志级别，可选的有 debug、info、warn、error、fatal、panic 和 trace，默认为 Info在安装 velero 服务时的全局参数 -v 代表时 velero 调用其他第三方库时透传下去的日志级别，而非 velero 本身的日志级别，第三方库采用的日志库通常为 klog 或者 glog 等，通过 -v 指定日志级别，高于该级别的日志不会输出，如 -v = 5。 Velero 采用的日志库为 logrus，有 debug 、 info 、 warn 、 error 、 fatal 、 panic 和 trace，通过该参数指定，低于该级别的日志不会输出。 –log-format 日志格式，可选的有 text 和 json，默认为 text –plugin-dir Velero Plugins 的存放位置，默认为 /plugins –metrics-address 暴露至 Prometheus 的端口，默认为 8085 –backup-sync-period 备份同步的时间间隔，默认为 1 分钟，设置为 0 时表示禁用同步 –restic-timeout Pod 卷备份与恢复的执行超时时间，默认为 240 分钟 –restore-only 是否以 Restore-Only 形式运行服务，即 Backup、Schedule 和 GC controller 都会被禁用，仅启动 Restore Controller 参数已经废弃，取而代之的是将 BackupStorageLocation 设为 ReadOnly –disable-controllers 禁止启动的 Controller，可选的有 Backup、BackupDeletion、BackupSync、DownloadRequest、GarbageCollection、ResticRepo、Restore、Schedule 和 ServerStatusRequest，默认不禁止 –restore-resource-priorities 期望的资源恢复顺序，任何不在列表中的资源都将在优先资源之后按字母顺序恢复，默认恢复顺序为1. customresourcedefinitions2. namespaces3. storageclasses4. volumesnapshotclass.snapshot.storage.k8s.io5. volumesnapshotcontents.snapshot.storage.k8s.io6. volumesnapshots.snapshot.storage.k8s.io7. persistentvolumes8. persistentvolumeclaims9. secrets10. configmaps11. serviceaccounts12. limitranges13. pods14. replicasets.apps15. clusters.cluster.x-k8s.io16. clusterresourcesets.addons.cluster.x-k8s.io –default-backup-storage-location 默认的 BackupStorageLocation 名称，默认为 default参数已经废弃，取而代之的是将 BackupStorageLocation 的 –default 设置为 true –store-validation-frequency BackupStorageLocation 的检验时间间隔，默认为 1 分钟，设置为 0 时表示禁用校验 –default-volume-snapshot-locations SnapshotProvider 的信息，例如 provider1:location-01,provider2:location-02 –client-qps 访问 Kubernetes API 的最大 QPS，默认为 20，如果为 0，则置为 5详情查看 client-go –client-burst 访问 Kubernetes API 的最大突发请求量，默认为 30，如果为 0，则置为 10详情查看 client-go –profiler-address pprof 信息的地址，默认为 localhost:6060 –terminating-resource-timeout 恢复期间等待 PV 和 namespace 创建完成的超时时间，默认为 10 分钟 –default-backup-ttl backup 的过期时间，默认为 30 天 –default-restic-prune-frequency 对 Restic repo 执行 restic prune 的周期，默认为 1 周 –default-volumes-to-restic 是否由 Restic 备份所有的 Pod 卷，默认为 false Restic Server以下参数均为 Restic 服务启动时的额外参数 由于 Restic 不存在安装命令，因此只能通过定制化 Restic DaemonSet 的启动参数 名称 含义 –log-level 日志级别，可选的有 debug、info、warn、error、fatal、panic 和 trace，默认为 Info在安装 velero 服务时的全局参数 -v 代表时 velero 调用其他第三方库时透传下去的日志级别，而非 velero 本身的日志级别，第三方库采用的日志库通常为 klog 或者 glog 等，通过 -v 指定日志级别，高于该级别的日志不会输出，如 -v = 5。 Velero 采用的日志库为 logrus，有 debug 、 info 、 warn 、 error 、 fatal 、 panic 和 trace，通过该参数指定，低于该级别的日志不会输出。 –log-format 日志格式，可选的有 text 和 json，默认为 text 仓库StorageProviderStorageProvider 用于存放备份过程中产生的元数据信息、由 Restic 备份的卷数据信息、备份和恢复的任务日志等，对应的资源对象为 BackupStorageLocation。 createBackupStorageLocation API 1$ velero backup-location create 可选的 flags 包括 名称 含义 –access-mode 访问权限，默认为 ReadWrite，可选值有 ReadWrite 和 ReadOnly –backup-sync-period 备份同步的时间间隔，默认为 1 分钟，设置为 0 时表示禁用同步 –bucket BackupStorageLocation 中 bucket 信息，参考对象存储中的概念 –cacert 连接 BackupStorageLocation 时所需要的 TLS 证书 –config 描述 BackupStorageLocation 配置信息 –credential 连接 BackupStorageLocation 所需要的认证信息格式为 key-value，key 为 K8s secret 的名称，value 为 secret 中的 key，仅支持一对 –default 是否为默认的 BackupStorageLocation –label-columns 在 table 样式下，自定义表格栏展示 –labels 设置创建出来的 BackupStorageLocation 对象的标签信息 –output dry run，指定生成配置文件的格式，可选的有 table，json 和 yaml –prefix BackupStorageLocation 中 prefix 信息，参考对象存储中的概念 –provider 指定 StorageProvider，例如 aws, gcp 等 –show-labels 在 table 样式下，最后一栏展示标签信息 –validation-frequency BackupStorageLocation 的检验时间间隔，默认为 1 分钟，设置为 0 时表示禁用校验 delete1$ velero backup-location delete 可选的 flags 包括 名称 含义 –all 删除所有 BackupStorageLocation 对象 –confirm 确认删除交互 –selector 删除满足标签选择的所有 BackupStorageLocation 对象 name，–all 和 –selector 仅能指定一个 get1$ velero backup-location get 可选的 flags 包括 名称 含义 –default 仅展示默认的 BackupStorageLocation –label-columns 在 table 样式下，自定义表格栏展示 –output 格式化输出的样式，可选的有 table，json 和 yaml，默认为 table –selector 可以通过标签选择器展示符合要求的 BackupStorageLocation 对象 –show-labels 在 table 样式下，最后一栏展示标签信息 set1$ velero backup-location set 可选的 flags 包括 名称 含义 –cacert 连接 BackupStorageLocation 时所需要的 TLS 证书 –credential 连接 BackupStorageLocation 所需要的认证信息格式为 key-value，key 为 K8s secret 的名称，value 为 secret 中的 key，仅支持一对 –default 设置为默认的 BackupStorageLocation默认的仅能有一个，其余的会被设置为 false SnapshotProviderSnapshotProvider 用于存放备份过程中的卷快照数据，数据源自于 SnapshotProvider Plugin。 createVolumeSnapshotLocation API 1$ velero snapshot-location create 可选的 flags 包括 名称 含义 –config 描述 VolumeSnapshotLocation 的配置信息 –label-columns 在 table 样式下，用于自定义表格栏信息 –labels 设置创建出来的 VolumeSnapshotLocation 对象的标签信息 –output 格式化输出的样式，可选的有 table，json 和 yaml，默认为 table –provider 指定 SnapshotProvider，例如 aws, gcp 等 –show-labels 在 table 样式下，最后一栏展示标签信息 get1$ velero snapshot-location get 可选的 flags 包括 名称 含义 –label-columns 在 table 样式下，自定义表格栏信息展示 –output dry run，指定生成配置文件的格式，可选的有 table，json 和 yaml –selector 可以通过标签选择器获取符合要求的 VolumeSnapshotLocation 对象 –show-labels 在 table 样式下，最后一栏展示标签信息 ResticRepository目前 Velero 仅支持 Restic AWS、AZURE 和 GCP 作为卷数据存储，数据的采集和传输均由 Velero Restic 操作，目标会上传至卷数据存储，对应的资源对象为 ResticRepository。 repoget1$ velero restic repo get 可选的 flags 包括 名称 含义 –label-columns 在 table 样式下，自定义表格栏信息展示 –output dry run，指定生成配置文件的格式，可选的有 table，json 和 yaml –selector 可以通过标签选择器获取符合要求的 ResticRepository 对象 –show-labels 在 table 样式下，最后一栏展示标签信息 插件add1$ velero plugin add name 就是镜像地址 可选的 flags 包括 名称 含义 –image-pull-policy plugin 镜像的拉取策略，可选的有 Always，IfNotPresent 和 Never，默认为 IfNotPresent get1$ velero plugin get 可选的 flags 包括 名称 含义 –output 格式化输出的样式，可选的有 table，json 和 yaml，默认为 table –timeout 命令输出的超时时间，默认为 5 秒钟 remove1$ velero plugin remove 备份即时备份即时备份（on-demand）也就是单次的备份任务，对应的资源对象为 Backup。 createBackup API 1$ velero backup create &lt;name&gt; 可选的 flags 包括 名称 含义 –default-volumes-to-restic 默认为 true，即由 restic 备份所有的 Pod 卷如果不指定，则以全局的为主 –include-namespaces / –exclude-namespaces 显式包含/排除的命名空间，支持逗号分割 –include-resources / –exclude-resources 显式包含/排除的资源，支持逗号分割 –from-schedule 基于某一个定时备份创建一次即时备份指定此参数时，其他的 filter flag 均会失效，并以 Schedule 的模板为准；不指定备份名称时会以 schedule-timestamp 作为备份任务名称 –include-cluster-resources 是否备份集群级别的资源，默认为 true即使开启了此特性，如果并未备份全量 namespaces 的资源，仍然不会备份集群级别资源 –label-columns 在 table 样式下，用于自定义表格栏信息 –labels 设置创建出来的 Backup 对象的标签信息 –ordered-resources 指定备份的顺序集群级别的资源格式为 resource name，非集群级别的资源格式为 namespace/resource name，例如 pods=ns1/pod1,ns1/pod2;persistentvolumeclaims=ns1/pvc4,ns1/pvc8 –output dry run，指定生成配置文件的格式，可选的有 table，json 和 yaml –selector 可以通过标签选择器备份符合要求的资源 –show-labels 在 table 样式下，最后一栏展示标签信息 –snapshot-volumes 默认为 true，即备份时，默认会对 PV 资源调用 SnapshotProvider 打快照 –storage-location 指定 BackupStorageLocation，仅支持单个 –ttl 过期时间，默认 720 小时 –volume-snapshot-locations 指定 VolumeSnapshotLocation，支持多个 –wait 阻塞直至备份状态不再是 New 或者 InProgress手动退出等待可以 ctrl-c，并不会影响到备份任务 deletevelero backup 的删除涉及到存储在远端数据的同步删除，因此并非单纯删除 Backup 对象，而是借助于 DeleteBackupRequest 对象。 DeleteBackupRequest API 1$ velero backup delete 可选的 flags 包括 名称 含义 –all 删除所有资源 –confirm 确认删除交互 –selector 删除满足标签选择的所有资源 name，–all 和 –selector 仅能指定一个 describe1$ velero backup describe 可选的 flags 包括 名称 含义 –cacert 连接 BackupStorageLocation 时所需要的 TLS 证书 –details 更详细的信息输出 –insecure-skip-tls-verify 是否跳过 TLS 验证，默认为 false –selector 可以通过标签选择器获取符合要求的 Backup 对象 downloadDownloadRequest CR 1$ velero backup download 可选的 flags 包括 名称 含义 –cacert 连接 BackupStorageLocation 时所需要的 TLS 证书 –force 下载文件存在则覆盖 –insecure-skip-tls-verify 是否跳过 TLS 验证，默认为 false –output 文件保存的路径，默认为当前目录，名称为 &lt;backup&gt;-data.tar.gz –timeout 等待下载的超时时间，默认为 1m get1$ velero backup get 可选的 flags 包括 名称 含义 –label-columns 在 table 样式下，自定义表格栏展示 –output 格式化输出的样式，可选的有 table，json 和 yaml，默认为 table –selector 可以通过标签选择器展示符合要求的 Backup 对象 –show-labels 在 table 样式下，最后一栏展示标签信息 logs1$ velero backup logs 可选的 flags 包括 名称 含义 –cacert 连接 BackupStorageLocation 时所需要的 TLS 证书 –insecure-skip-tls-verify 是否跳过 TLS 验证，默认为 false –timeout 等待获取日志的超时时间，默认为 1 分钟 定时备份定时备份（schedule）是符合特定时间规律，由 Velero 控制面负责触发的备份任务，对应的资源对象为 Schedule。 createSchedule CR 1$ velero schedule create 可选的 flags 包括 名称 含义 –default-volumes-to-restic 是否由 Restic 备份所有的 Pod 卷，默认为 true优先级小于 velero install 中的对应参数 –include-cluster-resources 是否备份集群级别的资源，默认为 true即使开启了此特性，如果并未备份全量 namespaces 的资源，仍然不会备份集群级别资源 –include-namespaces / –exclude-namespaces 显式包含/排除的命名空间，支持逗号分割 –include-resources / –exclude-resources 显式包含/排除的资源，支持逗号分割 –label-columns 在 table 样式下，自定义表格栏展示 –labels 设置创建出来的 Backup 对象的标签信息 –ordered-resources 指定备份的顺序集群级别的资源格式为 resource name，非集群级别的资源格式为 namespace/resource name，例如 pods=ns1/pod1,ns1/pod2;persistentvolumeclaims=ns1/pvc4,ns1/pvc8 –output dry run，指定生成配置文件的格式，可选的有 table，json 和 yaml –schedule 定时规则的表达式不仅支持 cron 表达式，还支持易读的形式，例如 “0 */6 * * *” 和 @every 6h –selector 可以通过标签选择器备份符合要求的资源 –show-labels 在 table 样式下，最后一栏展示标签信息 –snapshot-volumes 备份时，是否会对 PV 资源调用 SnapshotProvider 打快照，默认为 true， –storage-location 指定 BackupStorageLocation，仅支持单个 –ttl 过期时间，默认 720 小时 –use-owner-references-in-backup 由 Schedule 创建出来的 Backup 是否带有 OwnerReferences 信息，默认为 false， –volume-snapshot-locations 卷快照的存储后端，支持多个 delete1$ velero schedule delete 可选的 flags 包括 名称 含义 –all 删除所有 Schedule 对象 –confirm 确认删除交互 –selector 删除满足标签选择器的所有 Schedule 对象 describe1$ velero schedule describe 可选的 flags 包括 名称 含义 –selector 可以通过标签选择器获取符合要求的 Schedule 对象 get1$ velero schedule get 可选的 flags 包括 名称 含义 –label-columns 在 table 样式下，自定义表格栏展示 –output 格式化输出的样式，可选的有 table，json 和 yaml，默认为 table –selector 可以通过标签选择器展示符合要求的 Schedule 对象 –show-labels 在 table 样式下，最后一栏展示标签信息 恢复createRestore API 1$ velero restore create 可选的 flags 包括 名称 含义 –allow-partially-failed 在开启 –from-schedule 时，是否允许从部分失败的 Backup 中恢复 –include-namespaces / –exclude-namespaces 显式包含/排除的命名空间，支持逗号分割 –include-resources / –exclude-resources 显式包含/排除的资源，支持逗号分割 –from-backup 指定从哪一个 Backup 中恢复 –from-schedule 指定从哪一个 Schedule 中恢复从 Schedule 最新创建的 Backup 恢复 –include-cluster-resources 默认为 true，即恢复集群级别的资源即使开启了此特性，如果并未恢复全量 namespaces 的资源，仍然不会恢复集群级别资源 –label-columns 在 table 样式下，自定义表格栏展示 –labels 设置创建出来的 Restore 对象的标签信息 –namespace-mappings 恢复时，命名空间的映射关系例如，src1:dst1,src2:dst2 –output dry run，指定生成配置文件的格式，可选的有 table，json 和 yaml –preserve-nodeports 恢复时，是否保留 Service 资源的 NodePort 信息，默认为 true， –restore-volumes 恢复时，是否从快照中恢复卷数据，默认为 true –selector 可以通过标签选择器恢复符合要求的资源 –show-labels 在 table 样式下，最后一栏展示标签信息 –wait 阻塞直至恢复状态不再是 New 或者 InProgress手动退出等待可以 ctrl-c，并不会影响到恢复任务 delete1$ velero restore delete 可选的 flags 包括 名称 含义 –all 删除所有 Restore 对象 –confirm 确认删除交互 –selector 删除满足标签选择器的所有 Restore 对象 describe1$ velero restore describe 可选的 flags 包括 名称 含义 –cacert 连接 BackupStorageLocation 时所需要的 TLS 证书 –details 更详细的信息输出 –insecure-skip-tls-verify 是否跳过 TLS 验证，默认为 false –selector 可以通过标签选择器获取符合要求的 Restore 对象 get1$ velero restore get 可选的 flags 包括 名称 含义 –label-columns 在 table 样式下，自定义表格栏展示 –output 格式化输出的样式，可选的有 table，json 和 yaml，默认为 table –selector 可以通过标签选择器展示符合要求的 Restore 对象 –show-labels 在 table 样式下，最后一栏展示标签信息 logs1$ velero restore logs 可选的 flags 包括 名称 含义 –cacert 连接 BackupStorageLocation 时所需要的 TLS 证书 –insecure-skip-tls-verify 是否跳过 TLS 验证，默认为 false –timeout 等待获取日志的超时时间，默认为 1 分钟","link":"/2021/06/07/2021-06-07%20Velero%20%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/"},{"title":"「 Velero 」操作卷数据（Restic）","text":"based on v1.6.3 与 Restic 集成安装命令行安装12345678velero install \\ --provider aws \\ --bucket velero \\ --plugins velero/velero-plugin-for-aws:v1.0.0 \\ --secret-file /root/credential \\ --use-restic \\ --default-volumes-to-restic \\ --use-volume-snapshots=false 参数说明 use-restic 表示是否启用 Restic 组件操作 Pod 中的卷数据 default-volumes-to-restic 表示是否默认备份 Pod 中所有的卷 部署文件相比于之前的部署结果： 在指定 --use-restic 之后，额外部署了 DaemonSet 类型的 Restic 服务 在指定 --default-volumes-to-restic 之后，Velero 的启动参数中会新增 feature 特性 default-volumes-to-restic 1234- args: - server - --features= - --default-volumes-to-restic=true use-restic 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576apiVersion: apps/v1kind: DaemonSetmetadata: creationTimestamp: null labels: component: velero name: restic namespace: velerospec: selector: matchLabels: name: restic template: metadata: creationTimestamp: null labels: component: velero name: restic spec: containers: - args: - restic - server - --features= command: - /velero env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: VELERO_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: VELERO_SCRATCH_DIR value: /scratch - name: GOOGLE_APPLICATION_CREDENTIALS value: /credentials/cloud - name: AWS_SHARED_CREDENTIALS_FILE value: /credentials/cloud - name: AZURE_CREDENTIALS_FILE value: /credentials/cloud - name: ALIBABA_CLOUD_CREDENTIALS_FILE value: /credentials/cloud image: velero:dev imagePullPolicy: IfNotPresent name: restic resources: limits: cpu: &quot;1&quot; memory: 1Gi requests: cpu: 500m memory: 512Mi volumeMounts: - mountPath: /host_pods mountPropagation: HostToContainer name: host-pods - mountPath: /scratch name: scratch - mountPath: /credentials name: cloud-credentials securityContext: runAsUser: 0 serviceAccountName: velero volumes: - hostPath: path: /var/lib/kubelet/pods name: host-pods - emptyDir: {} name: scratch - name: cloud-credentials secret: secretName: cloud-credentials updateStrategy: {} 部署结果123456$ kubectl get pod -n veleroNAME READY STATUS RESTARTS AGEminio-54b5867494-6plnl 1/1 Running 0 12mminio-setup-54vx8 0/1 Completed 0 12mrestic-vqdmn 1/1 Running 0 14mvelero-598755d478-7l8gd 1/1 Running 0 14m 流程验证以 local PV 作为 Pod 卷数据为例 PV 12345678910111213141516171819202122apiVersion: v1kind: PersistentVolumemetadata: name: example-pvspec: capacity: storage: 100Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: &quot;&quot; local: path: /tmp/example nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - sxh-1 PVC 1234567891011kind: PersistentVolumeClaimapiVersion: v1metadata: name: example-pvcspec: storageClassName: &quot;&quot; accessModes: - ReadWriteOnce resources: requests: storage: 50Mi Pod 1234567891011121314151617181920kind: PodapiVersion: v1metadata: name: example-podspec: containers: - name: test-pod image: busybox command: - &quot;/bin/sh&quot; args: - &quot;-c&quot; - &quot;sleep 1000000&quot; volumeMounts: - name: local-pvc mountPath: &quot;/mnt&quot; volumes: - name: local-pvc persistentVolumeClaim: claimName: example-pvc 操作验证 12345678910111213141516171819# 写入测试数据$ kubectl exec -it example-pod ls /mnthello-here# 创建备份任务$ velero backup create default# 模拟故障$ rm -rf /tmp/example/* &amp;&amp; kubectl delete pod test-pod &amp;&amp; kubectl delete pvc test-claim &amp;&amp; kubectl delete pv test-pv # 重新部署 pv，后续在 troubleshooting 会说明原因$ kubectl apply -f pv.yaml# 创建恢复任务$ velero restore create default --from-backup default# 查看测试数据$ kubectl exec -it example-pod ls /mnthello-here 流程走读Velero 在对 Pod 卷数据做备份时，可以与开源项目 Restic 集成，集成使用时，Restic 本身的优势也会得到支持，如加密传输， 压缩备份， 增量备份， 断点续传等。 CRD有关卷备份与恢复的 CRD 包含：podvolumebackups.velero.io、podvolumerestores.velero.io 和 resticrepositories.velero.io。 ResticRepositoryRestic 中 repository 的概念，表示备份用于存储的位置。 原生 Restic 中通过 restic init --repo &lt;repo&gt; 的方式初始化不同的 backend。 local 123456$ restic init --repo /srv/restic-repoenter password for new repository:enter password again:created restic repository 085b3c76b9 at /srv/restic-repoPlease note that knowledge of your password is required to access the repository.Losing your password means that your data is irrecoverably lost. sftp 123456$ restic -r sftp:user@host:/srv/restic-repo initenter password for new repository:enter password again:created restic repository f1c6108821 at sftp:user@host:/srv/restic-repoPlease note that knowledge of your password is required to access the repository.Losing your password means that your data is irrecoverably lost. Amazon S3 12345678$ export AWS_ACCESS_KEY_ID=&lt;MY_ACCESS_KEY&gt;$ export AWS_SECRET_ACCESS_KEY=&lt;MY_SECRET_ACCESS_KEY&gt;$ restic -r s3:s3.amazonaws.com/bucket_name initenter password for new repository:enter password again:created restic repository eefee03bbd at s3:s3.amazonaws.com/bucket_namePlease note that knowledge of your password is required to access the repository.Losing your password means that your data is irrecoverably lost. Minio Server 12345678$ export AWS_ACCESS_KEY_ID=&lt;YOUR-MINIO-ACCESS-KEY-ID&gt;$ export AWS_SECRET_ACCESS_KEY= &lt;YOUR-MINIO-SECRET-ACCESS-KEY&gt;$ ./restic -r s3:http://localhost:9000/restic initenter password for new repository:enter password again:created restic repository 6ad29560f5 at s3:http://localhost:9000/restic1Please note that knowledge of your password is required to accessthe repository. Losing your password means that your data is irrecoverably lost. 在 Velero 中封装了初始化 Restic Repository 的动作（具体包含 restic init、restic check 和 restic prune），但是仅支持 velero.io/aws（包含 aws 或者非 aws，但是兼容 s3 的存储，如 minio）、velero.io/aure 和 velero.io/gcp。 pkg/restic/config.go 1234567891011121314151617181920212223242526272829303132333435363738394041424344// getRepoPrefix returns the prefix of the value of the --repo flag for// restic commands, i.e. everything except the &quot;/&lt;repo-name&gt;&quot;.func getRepoPrefix(location *velerov1api.BackupStorageLocation) (string, error) { var bucket, prefix string if location.Spec.ObjectStorage != nil { layout := persistence.NewObjectStoreLayout(location.Spec.ObjectStorage.Prefix) bucket = location.Spec.ObjectStorage.Bucket prefix = layout.GetResticDir() } backendType := getBackendType(location.Spec.Provider) if repoPrefix := location.Spec.Config[&quot;resticRepoPrefix&quot;]; repoPrefix != &quot;&quot; { return repoPrefix, nil } switch backendType { case AWSBackend: var url string switch { // non-AWS, S3-compatible object store case location.Spec.Config[&quot;s3Url&quot;] != &quot;&quot;: url = location.Spec.Config[&quot;s3Url&quot;] default: region, err := getAWSBucketRegion(bucket) if err != nil { url = &quot;s3.amazonaws.com&quot; break } url = fmt.Sprintf(&quot;s3-%s.amazonaws.com&quot;, region) } return fmt.Sprintf(&quot;s3:%s/%s&quot;, strings.TrimSuffix(url, &quot;/&quot;), path.Join(bucket, prefix)), nil case AzureBackend: return fmt.Sprintf(&quot;azure:%s:/%s&quot;, bucket, prefix), nil case GCPBackend: return fmt.Sprintf(&quot;gs:%s:/%s&quot;, bucket, prefix), nil } return &quot;&quot;, errors.New(&quot;restic repository prefix (resticRepoPrefix) not specified in backup storage location's config&quot;)} 针对每一个待备份卷数据的 Pod 所在的 namespace，velero 会创建一个和 namespace 对应的 ResticRepository，如果对应 namespace 的 ResticRepository 存在，则不会重复创建，命名方式为 &lt;namespace&gt;-&lt;backupstoragelocation&gt;-&lt;id&gt;。 1234567891011121314151617$ velero restic repo getNAME STATUS LAST MAINTENANCEdefault-default-n5mz4 Ready 2022-01-06 16:05:14 +0800 CSTvelero-default-8q767 Ready 2022-01-06 15:46:31 +0800 CST$ kubectl get resticrepositories -n veleroNAME AGEdefault-default-n5mz4 2mvelero-default-8q767 2m28s$ kubectl get resticrepositories -n velero default-default-n5mz4 -o yaml&lt;skip&gt;spec: backupStorageLocation: default maintenanceFrequency: 168h0m0s resticIdentifier: s3:http://minio.velero.svc:9000/velero/restic/default volumeNamespace: default 在 BackupStorageLocation 中的存储方式如下： velero.io/aws PodVolumeBackup代表 Pod 卷备份任务，每有一个待备份的 Pod 卷，Velero 会创建一个和 backup 对应的 PodVolumeBackup，由于该 CR 是和 backup 对应且 backup 名称唯一，所以针对相同的 Pod 卷的多次备份，会创建多个 PodVolumeBackup，命名方式为 &lt;backup&gt;-&lt;id&gt;，各节点上的 restic daemonset controller 会根据 PodVolumeBackup指定 restic backup 命令。 123456789101112131415161718192021222324252627282930$ kubectl get podvolumebackups -n veleroNAME AGEdefault-l2dd6 105svelero-j2xj5 2m5svelero-llv9m 2m5svelero-nxlkq 2m13svelero-x6m72 2m7svelero-xqdhk 2m7svelero-z7rhq 2m12s$ kubectl describe podvolumebackups -n velero default-l2dd6&lt;skip&gt;Spec: Backup Storage Location: default Node: sxh-1 Pod: Kind: Pod Name: example-pod Namespace: default UID: bb17e801-b595-4e96-8ced-e27e8686be23 Repo Identifier: s3:http://minio.velero.svc:9000/velero/restic/default Tags: Backup: default Backup - UID: 2fa40af0-2dcc-43dc-9636-79e5f1c95045 Ns: default Pod: example-pod Pod - UID: bb17e801-b595-4e96-8ced-e27e8686be23 Pvc - UID: f89b5daf-cf5e-49f1-9ba8-26e62f55baf2 Volume: local-pvc Volume: local-pvc PodVolumeRestore代表 Pod volume 的恢复任务，每有一个待恢复的 Pod 卷，Velero 会创建一个和 restore 对应的 PodVolumeRestore，由于该 CR 是和 restore 对应且 restore 名称唯一，所以针对相同的 Pod 卷建多个 PodVolumeRestore，命名方式为 &lt;restore&gt;-&lt;id&gt;，各节点上的 restic daemonset controller 会根据 PodVolumeRestore执行 restic restore 命令。 12345678910111213141516$ kubectl get podvolumerestores -n veleroNAME AGEalls-20220106165015-p54kz 6m17salls-20220106165349-rl4sh 2m42s$ kubectl describe podvolumerestores alls-20220106165349-rl4sh -n veleroSpec: Backup Storage Location: default Pod: Kind: Pod Name: example-pod Namespace: default UID: d72f5c66-2f93-4e4b-b2f6-0e5ce0aa2042 Repo Identifier: s3:http://minio.velero.svc:9000/velero/restic/default Snapshot ID: abdd9af5 Volume: local-pvc 备份Velero 在开启 Restic 对 Pod volume 备份时，根据以下两种方式获取待备份卷的信息： Velero args 在开启 default-volumes-to-restic 时，默认所有备份均使用 restic 备份所有的 Pod 卷。该参数即可以在 velero install 中全局生效，也可以在 velero backup create 时针对单次备份生效。 pkg/cmd/cli/install/install.go Pod annotation 在未开启 default-volumes-to-restic 时，Velero 会根据 Pod annotation 的中声明信息，获取待备份的 Pod 卷，例如 backup.velero.io/backup-volumes=nginx-logs，也可以指定排除备份的卷，例如 backup.velero.io/backup-volumes-excludes=nginx-logs。 注意 如果两种方式均开启时，仅 backup-volumes-excludes 生效 并非所有的 in-tree volume 均会备份。例如，以下卷类型不会参与备份 hostpath，由于 hostpath 不会挂载到 /var/lib/kubelet/pods 中，因此无法被 Restic 获取 secret，会作为 K8s metadata 单独备份 configMap，会作为 K8s metadata 单独备份 projected，为运行时状态数据，不会备份 **”default-token”**，默认的 service account token，不需要备份 如上所述，Velero 为 Pod 中要备份的每个卷创建一个 PodVolumeBackup，并等待其状态返回 completed 或者 failed。 与此同时，每个节点的 restic controller 会有一个 /var/lib/kubelet/pods 的 hostPath 卷挂载用来访问 Pod 的卷数据，通过访问该 hostPath 卷，获取到待备份的 Pod 卷数据后，执行 restic backup，并根据实际情况将 odVolumeBackup 状态设置为 completed 或者 failed。 在每一个 PodVolumeBackup 完成时，Velero 会将信息添加到 &lt;backup-name&gt;-podvolumebackups.json.gz 文件中，备份完成时，汇总候上传到后端存储中，该文件中包含本次备份所有的 PodVolumeBackup 信息。 恢复当对 Pod 卷数据进行恢复时，Velero 会根据 restore 的 --from-backup 的备份获取到 PodVolumeBackup。 对于获取到 PodVolumeBackup，Velero 会确保待恢复 Pod 的 namespace有与之对应的 ResticRepository，如果不存在，则创建一个，并执行 restic init 和 restic check。 Velero 向每一个待恢复卷数据的 Pod 注入一个 init 容器，这个程序会一直等待，直到在每一个恢复的卷中找到一个位于 .velero 下的文件，该文件的名称是恢复任务的 UID，即 Pod 完成所有卷数据的恢复。 Velero 创建出来的这个待恢复的 Pod，Kubernetes 调度器将这个 Pod 调度到一个可工作的节点，确保该 Pod 处于运行状态。如果 Pod 由于某种原因（即集群资源不足）启动失败，则不会进行 Restic 恢复。 Velero 为 Pod 中要恢复的每个卷创建一个 PodVolumeRestore，并等待其状态返回 completed 或者 failed。 与此同时，每个节点的 restic controller 会有一个 /var/lib/kubelet/pods 的 hostPath 卷挂载用来访问 Pod 的卷数据，等待 Pod 运行 init 容器，通过访问该 hostPath 卷，获取到待备份的 Pod 卷数据后，执行 restic restore，成功之后，将文件写入 Pod 卷中的 .velero 子目录中，名称是恢复任务的 UID，并根据实际情况将 PodVolumeRestore 状态设置为 completed 或者 failed。 123$ ls -l /mnt/.velerototal 0-rw-r--r-- 1 root root 0 Jan 17 06:28 b1f704be-7f60-475e-833f-4471544a2f87 当 init 容器在 .velero 下获取到所有的待恢复的卷信息后，便会成功退出，Pod 继续运行其他 init 容器/主进程。 Troubleshooting静态供应的 PV 的备份与恢复默认情况下，Velero 会备份静态 PV，如 local pv、nfs pv 等，但是在恢复的时候，如果待恢复的对象中包含使用该 PV 的 Pod 时，Velero 并不会恢复 PV，而是默认由 StorageClass 动态供应创建 PV。此时， PVC 会处于 pending 状态（由于不存在 PV），Pod 也会处于 pending 状态（由于 PVC pending），restore 任务会等待直至超时。 目前来看 Velero 社区并没有将此视为 bug，建议在使用层面进行处理，方案的核心思路是如果恢复的 Pod 使用静态的 PV 时，需要确保恢复流程执行之前，存在一个可以满足 PVC 的 PV，例如以下两种方案： 手动创建 PV 也就是流程验证中的操作，通过手动创建 PV，来与 Velero 恢复的 PVC 进行绑定，完成后续恢复流程。 单独备份 PV 通过 --include-resources pv 单独备份静态 PV，在恢复之前，单独恢复静态 PV，具体流程为： 备份静态 PV 创建备份任务 恢复静态 PV 恢复备份任务 https://github.com/vmware-tanzu/velero/issues/2520","link":"/2022/01/06/2022-01-06%20Velero%20%E6%93%8D%E4%BD%9C%E5%8D%B7%E6%95%B0%E6%8D%AE%EF%BC%88Restic%EF%BC%89/"},{"title":"「 Golang 」Web Server 代码结构","text":"Open Sourcehttps://github.com/slok/kubewebhook（Go framework to create Kubernetes mutating and validating webhooks.）。是一个用于创建 Kubernetes mutating 和 validating webhook 的 Golang 框架，其中提供了用于生产环境的示例模板。 Samplehttps://github.com/shenxianghong/shenxianghong.github.io/tree/main/elegant-code/web-structure Structurehandlershandlers 中聚焦实际的业务处理逻辑 welcome.go请求到来时，组装返回的消息内容 1234567891011121314151617181920212223242526package handlersimport &quot;fmt&quot;// Welcome 抽象了一系列的业务逻辑// 例如 Hello 用于组装返回的消息内容type Welcome interface { Hello() string}// 接口实现type WelcomeHandler struct { User string}// 业务逻辑func (h WelcomeHandler) Hello() string { return fmt.Sprintf(&quot;hello %s, welcome.&quot;, h.User)}// 工厂函数func NewWelcomeHandler() Welcome { var handler WelcomeHandler handler.User = &quot;Arthur&quot; return handler} goodbye.go原理类似 welcome.go webweb 框架的基础结构 handlers.go调用业务逻辑，构建 http.Handler 类型，作为请求的 handler 函数 123456789101112131415161718package webimport &quot;net/http&quot;// 逻辑路由的工厂函数func (h handler) welcomeHandler() http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) { w.Write([]byte(h.welcome.Hello())) w.WriteHeader(http.StatusOK) })}func (h handler) goodbyeHandler() http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) { w.Write([]byte(h.goodbye.Goodbye())) w.WriteHeader(http.StatusOK) })} router.go用于注册业务逻辑部分的路由，也就是子路由 12345678910package webimport &quot;net/http&quot;func (h handler) router(router *http.ServeMux) { // 逻辑路由的工厂函数返回 http.Handler 类型,用于注册子路由 router.Handle(&quot;/welcome&quot;, h.welcomeHandler()) router.Handle(&quot;/goodbye&quot;, h.goodbyeHandler()) return} web.goweb 框架的上层对象，包括配置和根路由 handler 的生成等 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package webimport ( &quot;elegant-coding/handlers&quot; &quot;errors&quot; &quot;net/http&quot;)// 配置信息// 上层会传入业务逻辑的工厂函数来初始化这个结构体，同样的，也可以传入比如全局的 logger 等信息// 初始化下面的 handler 时会根据这个配置信息type Config struct { WelcomeHandler handlers.Welcome GoodbyeHandler handlers.Goodbye}// 用于做一些校验设置默认信息等func (c *Config) defaults() error { if c.WelcomeHandler == nil { return errors.New(&quot;welcome handler is missing&quot;) } if c.GoodbyeHandler == nil { return errors.New(&quot;goodbye handler is missing&quot;) } return nil}// 封装了 http.Handler，同时也包含了一系列的业务逻辑的接口实现type handler struct { // 业务逻辑的 interface welcome handlers.Welcome goodbye handlers.Goodbye // 原生 http handler 功能 handler http.Handler}// 调用原生的 http.Handler.ServeHTTP，启动服务func (h handler) ServeHTTP(w http.ResponseWriter, r *http.Request) { h.handler.ServeHTTP(w, r)}// 初始化上面的 handler// 返回的是一个 http.Handler 类型，是为了在主函数的时候作为根路由的 handlerfunc New(config Config) http.Handler { if err := config.defaults(); err != nil { panic(&quot;handler configuration is not valid&quot;) } mux := http.NewServeMux() h := handler{ welcome: config.WelcomeHandler, goodbye: config.GoodbyeHandler, handler: mux, } h.router(mux) return h} main.go123456789101112131415161718192021222324252627282930package mainimport ( &quot;fmt&quot; &quot;net/http&quot; &quot;elegant-coding/handlers&quot; &quot;elegant-coding/web&quot;)func main() { // 根路由的 handler 函数 handler := web.New(web.Config{ // 通过工厂函数生成 handler WelcomeHandler: handlers.NewWelcomeHandler(), GoodbyeHandler: handlers.NewGoodbyeHandler(), }) mux := http.NewServeMux() mux.Handle(&quot;/&quot;, handler) server := http.Server{ Addr: &quot;:8081&quot;, Handler: mux, } err := server.ListenAndServe() if err != nil { fmt.Println(err.Error()) return }}","link":"/2021/12/01/2021-12-01%20Golang%20Web%20Server%20%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/"},{"title":"「 Rust 」快速开始 — Cargo","text":"简介Cargo 是 Rust 的构建系统和包管理工具，可以帮助构建代码、下载依赖库、构建依赖库等。 安装 Rust 的时候会默认安装 Cargo，可以通过 cargo --version 判断是否安装 12$ cargo --versioncargo 1.57.0 (b2e52d7ca 2021-10-21) 创建项目使用 cargo new &lt;path&gt; 创建一个工程项目，工程项目名为 &lt;path&gt;。同时，也会创建一个 &lt;path&gt; 的目录，在不显示声明 vcs 的情况下，默认会同时创建 git 仓库，可以通过 cargo new &lt;path&gt; --vcs xxx 指定其他 vcs，或者通过 --vcs none 不创建 vcs。 12345678910$ cargo new hello-world Created binary (application) `hello-world` package $ tree -a hello-worldhello-world├── .git├── .gitignore├── Cargo.toml└── src └── main.rs src 目录用于存放源代码 Cargo.toml 是项目的配置文件，类似于 Golang 的 go.mod 文件 .gitignore 中包含忽略 /target 的信息 Cargo.toml12345678[package]name = &quot;hello-world&quot;version = &quot;0.1.0&quot;edition = &quot;2021&quot;# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html[dependencies] package 用于描述项目信息的 name — 项目名 version — 项目版本 authors — 开发者昵称和邮箱信息，如果系统中存在 edtion — 使用的 Rust 版本 dependecies 用于描述项目的依赖项，在 Rust 中代码的包（库）称为 crate 转换为 Cargo如果创建项目时没有使用 Cargo，也可以把项目转化为使用 Cargo，具体做法为：把源代码移到 src 目录下，创建 Cargo.toml 并填写相应的配置。 构建项目满足 Cargo 规范的项目，可以使用 cargo build 来构建项目。 cargo builddevunoptimized + debuginfo 在项目的根目录下，执行 cargo build 会将可执行文件编译在 target/debug/hello-world 目录下，第一次构建时，会在顶层目录生成 cargo.lock 文件，负责追踪项目依赖的精确版本，类似于 Golang 的 go.sum 文件。 cargo.lock 1234567# This file is automatically @generated by Cargo.# It is not intended for manual editing.version = 3[[package]]name = &quot;hello-world&quot;version = &quot;0.1.0&quot; 123$ cargo build Compiling hello-world v0.1.0 (/Users/shenxianghong/Documents/Project/Rustaceans/hello-world) Finished dev [unoptimized + debuginfo] target(s) in 2.21s releaseoptimized 默认情况，cargo build 适用于开发阶段的常规编译，在最终发布构建时，可以通过 cargo build --release 发布，这样在编译的时候会进行优化，代码运行的更快，但是编译的时间也更长，同时，会在 target/release 而不是 target/debug 生成可执行文件。 123$ cargo build --release Compiling hello-world v0.1.0 (/Users/shenxianghong/Documents/Project/Rustaceans/hello-world) Finished release [optimized] target(s) in 0.38s cargo runcargo run 本质上就是编译代码 + 执行结果，如果之前编译成功过，且源码没有改变，那么就会直接运行二进制文件。 12345678910$ cargo run Compiling hello-world v0.1.0 (/Users/shenxianghong/Documents/Project/Rustaceans/hello-world) Finished dev [unoptimized + debuginfo] target(s) in 0.66s Running `target/debug/hello-world`Hello, world!$ cargo run Finished dev [unoptimized + debuginfo] target(s) in 0.00s Running `target/debug/hello-world`Hello, world! cargo checkcargo check 用于检查代码，确保能通过编译，但是不会实质的进行编译，cargo check 运行速度比 cargo build 快得多，可以用于开发阶段反复检查，提高效率。 123$ cargo check Checking hello-world v0.1.0 (/Users/shenxianghong/Documents/Project/Rustaceans/hello-world) Finished dev [unoptimized + debuginfo] target(s) in 0.06s 当检查有错误时，会报错提示 123456789$ cargo check Checking hello-world v0.1.0 (/Users/shenxianghong/Documents/Project/Rustaceans/hello-world)error: expected one of `-&gt;`, `;`, `where`, or `{`, found `err` --&gt; src/main.rs:1:11 |1 | fn main() err { | ^^^ expected one of `-&gt;`, `;`, `where`, or `{`error: could not compile `hello-world` due to previous error 国内源加速默认情况下，cargo 获取包依赖等通过 crate.io，由于网络问题，可以将其替换成国内的 cargo 源。 ~/.cargo/config 12345678910111213141516171819[source.crates-io]registry = &quot;https://github.com/rust-lang/crates.io-index&quot;replace-with = 'sjtu'# 清华大学[source.tuna]registry = &quot;https://mirrors.tuna.tsinghua.edu.cn/git/crates.io-index.git&quot;# 中国科学技术大学[source.ustc]registry = &quot;git://mirrors.ustc.edu.cn/crates.io-index&quot;# 上海交通大学[source.sjtu]registry = &quot;https://mirrors.sjtug.sjtu.edu.cn/git/crates.io-index&quot;# rustcc 社区[source.rustcc]registry = &quot;git://crates.rustcc.cn/crates.io-index&quot;","link":"/2022/01/09/2022-01-09%20Rust%20%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B%20-%20Cargo/"},{"title":"「 Rust 」概念初识","text":"单次猜测获取用户的命令行输入 12345678use std::io;fn main() { println!(&quot;guess a number&quot;); let mut guess = String::new(); io::stdin().read_line(&amp;mut guess).expect(&quot;error reading line&quot;); println!(&quot;your number is {}&quot;, guess);} 库的引用默认情况下 Rust 会将 prelude 模块（预导入模块）的内容导入到每个程序的作用域中，如果要使用的库不位于 prelude 模块中，则需要通过 use 关键字显示的导入。在此示例中，获取用户命令行输入的库为 io 库，而 io 库位于 Rust 标准库 std 中，导入方法为 use std::io; 变量的不可变默认情况下，Rust 的变量均为不可变的（immutable）。 12345fn main() { let foo = 1; foo = 2; println!(&quot;foo is {}&quot;, foo)} 1234567891011$ cargo runerror[E0384]: cannot assign twice to immutable variable `foo` --&gt; src/main.rs:3:5 |2 | let foo = 1; | --- | | | first assignment to `foo` | help: consider making this binding mutable: `mut foo`3 | foo = 2; | ^^^^^^^ cannot assign twice to immutable variable 如果要声明一个可变的变量，那么需要在变量前加上 mut 关键字 12345fn main() { let mut foo = 1; foo = 2; println!(&quot;bar is {}&quot;, foo)} 12$ cargo runbar is 2 需要注意的是，引用默认也是不可变的，而 read_line() 方法会根据用户的输入修改传入的变量，因此，也要对入参声明可变。 关联函数String::new() 会返回字符串的一个新的实例，内部是 utf-8 编码的，中间的两个冒号表示 new() 是 String 这个类型的关联函数，关联函数表示针对这个类型本身来实现的，不是针对这个类型的某个特定示例来实现的，也就是 new() 不会作用于 guess 实例，类似于 Golang 中的结构体方法。 同理，io::stdin() 会返回一个 Stdin 类型的句柄。 ResultRust 中有很多种 Result 类型，即有通用泛型的 Result，也有针对特定类型的 Result，例如 io::Result ，Result 实际上就是一个枚举类型，包括两个值，一个是 Ok 一个是 Err ，expect() 方法用作错误判断，如果返回的值为 Err，那么会中断程序并将入参输出。 占位符区别于 Golang，println!() 中如果想输出变量，那么必须要有占位符，即 {}。 神秘数字引入第三方 rand 包，实现随机数的生成 123456789101112use std::io;use rand::Rng;fn main() { let secret_number = rand::thread_rng().gen_range(1, 101); println!(&quot;secret_number is {}&quot;, secret_number); println!(&quot;guess a number&quot;) let mut guess = String::new(); io::stdin().read_line(&amp;mut guess).expect(&quot;error reading line&quot;); println!(&quot;your number is {}&quot;, guess);} 第三方依赖包在 Cargo.toml 的 dependencies 新增 package = version 信息为项目新增第三方依赖包。 ^ 表示任何一个与指定版本 api 兼容的库均可以，并且该标识为默认。 Cargo.toml 123456789[package]name = &quot;hello-world&quot;version = &quot;0.1.0&quot;edition = &quot;2021&quot;# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html[dependencies]rand = &quot;^0.3.14&quot; 首次构建时，cargo 会更新源的 index，根据 Cargo.toml 的内容下载依赖，将下载的依赖信息写入 Cargo.lock 中，并完成源码和依赖的构建，当后续源码或者发生变化，则仅会重新构建变化部分。 12345678910$ cargo build Updating crates.io index Downloaded rand v0.3.23 Downloaded rand v0.4.6 Downloaded 2 crates (87.7 KB) in 0.77s Compiling libc v0.2.112 Compiling rand v0.4.6 Compiling rand v0.3.23 Compiling hello-world v0.1.0 (/Users/shenxianghong/Documents/Project/Rustaceans/hello-world) Finished dev [unoptimized + debuginfo] target(s) in 5.06s Cargo.lock 12345678910111213141516&lt;skip&gt;[[package]]name = &quot;libc&quot;version = &quot;0.2.112&quot;source = &quot;registry+https://github.com/rust-lang/crates.io-index&quot;checksum = &quot;1b03d17f364a3a042d5e5d46b053bbbf82c92c9430c592dd4c064dc6ee997125&quot;[[package]]name = &quot;rand&quot;version = &quot;0.3.23&quot;source = &quot;registry+https://github.com/rust-lang/crates.io-index&quot;checksum = &quot;64ac302d8f83c0c1974bf758f6b041c6c8ada916fbb44a609158ca8b064cc76c&quot;dependencies = [ &quot;libc&quot;, &quot;rand 0.4.6&quot;,] 当对依赖跨大版本版本更新时，需要手动修改 Cargo.toml ，除了可以通过重新构建的方式，还可以通过 cargo update 重新维护依赖关系。 rand 包升级至 0.7 1234567891011121314151617181920212223242526272829303132333435363738$ cargo update Updating crates.io index Removing cfg-if v1.0.0 Adding fuchsia-cprng v0.1.1 Removing getrandom v0.1.16 Removing ppv-lite86 v0.2.16 Removing rand v0.7.3 Adding rand v0.3.23 Adding rand v0.4.6 Removing rand_chacha v0.2.2 Removing rand_core v0.5.1 Adding rand_core v0.3.1 Adding rand_core v0.4.2 Removing rand_hc v0.2.0 Adding rdrand v0.4.0 Removing wasi v0.9.0+wasi-snapshot-preview1 Adding winapi v0.3.9 Adding winapi-i686-pc-windows-gnu v0.4.0 Adding winapi-x86_64-pc-windows-gnu v0.4.0shenxianghong@Corgi hello-world % cargo update Updating crates.io index Adding cfg-if v1.0.0 Removing fuchsia-cprng v0.1.1 Adding getrandom v0.1.16 Adding ppv-lite86 v0.2.16 Removing rand v0.3.23 Removing rand v0.4.6 Adding rand v0.7.3 Adding rand_chacha v0.2.2 Removing rand_core v0.3.1 Removing rand_core v0.4.2 Adding rand_core v0.5.1 Adding rand_hc v0.2.0 Removing rdrand v0.4.0 Adding wasi v0.9.0+wasi-snapshot-preview1 Removing winapi v0.3.9 Removing winapi-i686-pc-windows-gnu v0.4.0 Removing winapi-x86_64-pc-windows-gnu v0.4.0 除此之外，cargo update 还可以用于依赖包的小版本升级：当执行升级时，Cargo 会忽略 Cargo.lock，根据 Cargo.toml 的包版本信息，升级到最新的小版本，而不会突破大版本，升级之后 Cargo.lock 会更新，而 Cargo.toml 保持不变，也就是基于语义化的版本升级。 TraitTrait 可以理解成 Golang 中的接口，定义了许多方法。rand::Rng 就是一个 Trait，定义了一组随机数生成器所需要的方法。rand::thread_rng() 这个函数返回是一个 ThreadRng 类型，本质上是一个运行在本地线程空间中，通过操作系统获取随机数种子的随机数生成器，而 gen_range() 就是 Trait 的方法之一。 不导入 trait，但是使用 trait 方法，会引起报错 1234567891011121314151617$ cargo runerror[E0599]: no method named `gen_range` found for struct `ThreadRng` in the current scope --&gt; src/main.rs:6:44 |6 | let secret_number = rand::thread_rng().gen_range(1, 101); | ^^^^^^^^^ method not found in `ThreadRng` | ::: /Users/shenxianghong/.cargo/registry/src/github.com-1ecc6299db9ec823/rand-0.4.6/src/lib.rs:524:8 |524 | fn gen_range&lt;T: PartialOrd + SampleRange&gt;(&amp;mut self, low: T, high: T) -&gt; T where Self: Sized { | --------- the method is available for `ThreadRng` here | = help: items from traits can only be used if the trait is in scopehelp: the following trait is implemented but not in scope; perhaps add a `use` for it: |1 | use rand::Rng; | 比较猜测数字与神秘数字猜测数字为 string 类型，神秘数字为 int 类型，转换后进行大小比较 1234567891011121314151617181920use std::io;use std::cmp::Ordering;use rand::Rng;fn main() { let secret_number = rand::thread_rng().gen_range(1, 101); println!(&quot;secret_number is {}&quot;, secret_number); println!(&quot;guess a number&quot;); let mut guess = String::new(); io::stdin().read_line(&amp;mut guess).expect(&quot;error reading line&quot;); let guess: u32 = guess.trim().parse().expect(&quot;error parsing guess number&quot;); println!(&quot;your number is {}&quot;, guess); match guess.cmp(&amp;secret_number) { Ordering::Less =&gt; println!(&quot;Too small&quot;), Ordering::Greater =&gt; println!(&quot;Too big&quot;), Ordering::Equal =&gt; println!(&quot;You win&quot;), }} 枚举std::cmp::Ordering 是一个枚举类型，包含三个值，分别是 Ordering::Less、Ordering::Greater 和 Ordering::Equal ，枚举类型的使用也需要使用双冒号格式。 matchcmp 方法返回的是 Ordering 类型，根据不同的分支（arm）判断匹配模式，从而执行不同的逻辑，即 =&gt; 之后的逻辑，类似于 Golang 中的 switch case 用法。 ShadowRust 中允许使用同名的变量来覆盖之前的变量，区别于 Golang，不仅可以用于覆盖值，可以类型也可以不一样。一般用于在不额外声明变量的场景下，进行类型转换。 类型Rust 是强类型语言，并且具备类型推断的能力，gen_range(1, 101) 会返回 1 到 100 之间的随机整数，Rust 中涵盖此范围的类型很多，比如 i32、u32、i64 等等，如果未做进一步的声明，Rust 默认其为 i32。 可以注意到，变量 guess 被转换成了 u32 类型，而接下来还对变量 guess 和 secret_number 进行了 match 比较，因此 Rust 也会将变量 secret_number 设置为 u32 类型编译，因此，如果没有 match 比较，则 Rust 会将其默认为 i32。 多次猜测增加死循环，直至猜对退出；增加错误处理，完善健壮性 123456789101112131415161718192021222324252627use std::io;use std::cmp::Ordering;use rand::Rng;fn main() { let secret_number = rand::thread_rng().gen_range(1, 101); loop { println!(&quot;guess a number&quot;); let mut guess = String::new(); io::stdin().read_line(&amp;mut guess).expect(&quot;error reading line&quot;); let guess: u32 = match guess.trim().parse() { Ok(num) =&gt; num, Err(_) =&gt; continue }; println!(&quot;your number is {}&quot;, guess); match guess.cmp(&amp;secret_number) { Ordering::Less =&gt; println!(&quot;Too small&quot;), Ordering::Greater =&gt; println!(&quot;Too big&quot;), Ordering::Equal =&gt; { println!(&quot;You win&quot;); break }, } }} 死循环Rust 中的死循环使用 loop 关键字，退出使用 break 关键字，继续使用 continue 关键字。 错误处理Rust 中常用的错误处理方式是基于 match 模式，例如 parse() 方法返回 Result 类型，该类型包括两个枚举值。其中 Ok(num) 表示猜测数字解析成功，num 为解析之后的数字，通过 =&gt; 赋值给 guess。同理，Err(_) 表示解析失败，_ 为错误信息，下划线表示忽略。","link":"/2022/01/08/2022-01-08%20Rust%20%E6%A6%82%E5%BF%B5%E5%88%9D%E8%AF%86/"},{"title":"「 Velero 」操作卷数据（CSI）","text":"based on v1.6.3 可以将 CSI 快照支持集成到 Velero 中，使 Velero 能够使用 Kubernetes CSI 快照 API 备份和恢复 CSI 支持的卷。通过 CSI 快照 API，Velero 可以支持任何具有 CSI 快照的卷，而无需特定的 Velero 插件。 在 Velero v1.6 中，此特性为 beta 版本，目前 1.9+ 为稳定版本 前置依赖 Kubernetes 版本至少为 1.17 集群中的 CSI 具备快照能力，兼容 v1beta1 版本 API 跨集群 CSI 卷快照恢复时，CSI Driver 快照类名需要保持一致 部署 Velero，开启 CSI 特性12345678$ velero install \\ --provider aws \\ --features EnableCSI \\ --plugins velero/velero-plugin-for-aws:v1.0.0,velero/velero-plugin-for-csi:v0.1.0 \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-volume-snapshots=false \\ --backup-location-config region=minio,s3ForcePathStyle=&quot;true&quot;,s3Url=http://minio.velero.svc:9000 参数说明 --features 开启 feature 特性 流程验证StorageClass 123456789101112apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: openebs-lvmscallowVolumeExpansion: trueparameters: volgroup: &quot;lvm_im&quot; fstype: &quot;ext4&quot; maxVolumeSize: &quot;5&quot; provisioner: local.csi.openebs.io reclaimPolicy: DeletevolumeBindingMode: WaitForFirstConsumer PersistentVolumeClaim 1234567891011kind: PersistentVolumeClaimapiVersion: v1metadata: name: local-pvcspec: storageClassName: openebs-lvmsc accessModes: - ReadWriteOnce resources: requests: storage: 30Mi Pod 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: pod-local-imspec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master restartPolicy: Never containers: - name: perfrunner image: busybox:1.27 command: [&quot;sh&quot;] args: [&quot;-c&quot;, &quot;while true ;do sleep 50; done&quot;] volumeMounts: - mountPath: /datadir name: fio-vol tty: true volumes: - name: fio-vol persistentVolumeClaim: claimName: local-pvc 操作验证 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 写入测试数据$ kubectl exec -it pod-local-im ls /datadirdata lost+found# 创建备份任务$ velero backup create default --include-namespaces default# 查看快照信息，发现此时 Velero 已经调用 CSI 创建了 volumesnapshot，并生成了 volumesnapshotcontent，并且查看 volumesnapshot 的状态 readyToUse 为 True$ kubectl get volumesnapshotNAMESPACE NAME READYTOUSE SOURCEPVC SOURCESNAPSHOTCONTENT RESTORESIZE SNAPSHOTCLASS SNAPSHOTCONTENT CREATIONTIME AGEdefault velero-local-pvc-bdbw8 true local-pvc 30Mi csi-local-snapclass snapcontent-93903201-f07a-405c-92d9-2c0b6bafd6a1 117s 118s$ kubectl get volumesnapshotcontentNAME READYTOUSE RESTORESIZE DELETIONPOLICY DRIVER VOLUMESNAPSHOTCLASS VOLUMESNAPSHOT VOLUMESNAPSHOTNAMESPACE AGEsnapcontent-93903201-f07a-405c-92d9-2c0b6bafd6a1 true 31457280 Delete local.csi.openebs.io csi-local-snapclass velero-local-pvc-bdbw8 default 2m38s# 模拟故障$ kubectl delete -f pod.yaml &amp;&amp; kubectl delete -f pvc.yaml# 创建恢复任务$ velero restore create --from-backup default$ kubectl get volumesnapshotcontent NAME READYTOUSE RESTORESIZE DELETIONPOLICY DRIVER VOLUMESNAPSHOTCLASS VOLUMESNAPSHOT VOLUMESNAPSHOTNAMESPACE AGEsnapcontent-93903201-f07a-405c-92d9-2c0b6bafd6a1 true 31457280 Delete local.csi.openebs.io csi-local-snapclass velero-local-pvc-bdbw8 default 3m31svelero-velero-local-pvc-bdbw8-z9t2f true 0 Delete local.csi.openebs.io velero-local-pvc-bdbw8 default 2m3s# 可以看到，Velero 调用 CSI Driver 基于之前的 volumesnapshot 恢复出来了一个 PVC$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGElocal-pvc Bound pvc-f034e865-340a-40cf-9381-3b95b1bd2f1f 30Mi RWO openebs-lvmsc 4m16s$ kubectl get pvc -o yaml&lt;skip&gt;spec: accessModes: - ReadWriteOnce dataSource: apiGroup: snapshot.storage.k8s.io kind: VolumeSnapshot name: velero-local-pvc-bdbw8 resources: requests: storage: 30Mi storageClassName: openebs-lvmsc volumeMode: Filesystem volumeName: pvc-f034e865-340a-40cf-9381-3b95b1bd2f1f# 数据已经从快照恢复$ kubectl exec -it pod-local-im ls /datadirdata lost+found 流程走读Velero 的 CSI 支持不依赖 Velero VolumeSnapshotter 插件。相反，Velero 采用一组 BackupItemAction 插件用于在操作 PersistentVolumeClaims 之前进行一些额外的动作。 备份时，当 BackupItemAction 发现有一个 PersistentVolumeClaims 指向由 CSI Driver 创建的 PersistentVolume 时，它将获取具有相同 Driver 名称的 VolumeSnapshotClass 来创建以 PersistentVolumeClaim 为源的 CSI VolumeSnapshot 对象，VolumeSnapshot 和 PersistentVolumeClaim 位于同一命名空间中。 接着，CSI external-snapshotter watch 到 VolumeSnapshot 之后创建一个 VolumeSnapshotContent 对象，它将指向存储系统中实际的、基于磁盘的快照。 external-snapshotter 会调用 CSI Driver 的 snapshot 方法，Driver 会调用存储系统的 API 生成快照。一旦生成 ID 并且存储系统将快照标记为可用于恢复，VolumeSnapshotContent 对象将使用 status.snapshotHandle 进行更新，并且设置status.readyToUse 字段为 true。 Velero 将在备份 tarball 中包含生成的 VolumeSnapshot 和 VolumeSnapshotContent 对象，并将 JSON 文件中的所有 VolumeSnapshots 和 VolumeSnapshotContents 对象上传到对象存储系统。当 Velero 将备份同步到新集群时，VolumeSnapshotContent 对象也将同步到集群中，以便 Velero 可以适当地管理备份过期。 VolumeSnapshotClass 的 DeletionPolicy 设置为 Retain 时，Velero 备份的生命周期内保留存储系统中的卷快照，并防止在发生灾难时删除存储系统中的卷快照，其中命名空间与VolumeSnapshot 对象可能会丢失。 当 Velero 备份到期时，VolumeSnapshot 对象将被删除，VolumeSnapshotContent 对象的 DeletionPolicy 将更新 Delete，以释放存储系统上的空间。","link":"/2022/01/10/2022-01-10%20Velero%20%E6%93%8D%E4%BD%9C%E5%8D%B7%E6%95%B0%E6%8D%AE%EF%BC%88CSI%EF%BC%89/"},{"title":"「 Rust 」变量与可变性","text":"可变性 声明变量使用 let 关键字 默认情况下，变量是不可变的（Immutable） 声明变量时，在变量前面加上 mut，就可以使变量可变 12345fn main() { let mut foo = 1; foo = 2; println!(&quot;bar is {}&quot;, foo)} 变量与常量常量（constant）在绑定值以后也是不可变的，但是它与不可变的变量有很多区别： 不可以使用 mut，常量永远都是不可变的 声明常量使用 const 关键字，它的类型必须被标注 常量可以在任何作用域内进行声明，包括全局作用域 常量只可以绑定到常量表达式，无法绑定到函数的调用结果或只能在运行时才能计算出的值 在程序运行期间，常量在其声明的作用域内一直有效 命名规范：Rust 里常量使用全大写字母，每个单词之间用下划线分开，例如：MAX_POINTS 12// rust 里可以通过下划线，增强数字的可读性const MAX_POINTS: u32 = 100_1000; Shadow（隐藏）Rust 中允许使用同名的变量来覆盖之前的变量，区别于 Golang，不仅可以用于覆盖值，可以类型也可以不一样。一般用于在不额外声明变量的场景下，进行类型转换。 12345fn main() { let x = 5; let x = x + 1; println!(&quot;x is {}&quot;, x)} 使用 mut 关键字 12345fn main() { let mut x = 5; x = x + 1; println!(&quot;x is {}&quot;, x)} shadow 和把变量标记为 mut 是不一样的 如果不使用 let 关键字，那么重新给非 mut 的变量赋值会导致编译时错误 使用 let 声明的同名新变量，也是不可变的 使用 let 声明的同名新变量，它的类型可以与之前不同 123456fn main() { let name = &quot;Arthur Morgan&quot;; let name = name.len(); println!(&quot;{}&quot;, name)} 12$ cargo run13 使用 mut 关键字 123456fn main() { let mut name = &quot;Arthur Morgan&quot;; name = name.len(); println!(&quot;{}&quot;, name)} 123456$ cargo runerror[E0308]: mismatched types --&gt; src/main.rs:3:12 |3 | name = name.len(); | ^^^^^^^^^^ expected `&amp;str`, found `usize`","link":"/2022/01/15/2022-01-15%20Rust%20%E5%8F%98%E9%87%8F%E4%B8%8E%E5%8F%AF%E5%8F%98%E6%80%A7/"},{"title":"「 Velero 」源码走读 — Location","text":"based on v1.6.3 BackupStorageLocationAPI backup-locationpkg/cmd/cli/backuplocation/backup-location.go velero backup-location 包括 4 个子命令：create、delete、get 和 set。 createpkg/cmd/cli/backup-location/create.go 校验规则 –provider 和 –bucket 为必需参数 在指定 –backup-sync-period 参数时，其值必须要大于等于 0 在指定 –credential 参数时，其值仅能包含一对 key-value 主体流程 根据命令行参数，构建 BackupStorageLocation 对象，下发至集群中创建，后续由 BackupStorageLocation Controller 负责维护 如果新创建的 BackupStorageLocation 为默认的，则将集群中其余的 BackupStorageLocation 设为非默认 deletepkg/cmd/cli/backup-location/delete.go 校验规则 name、–all 和 –selector 参数有且仅能有一个 要删除的 BackupStorageLocation 在集群中必须存在 主体流程 根据命令行参数，获取集群中的 BackupStorageLocation 资源并删除 getpkg/cmd/cli/backup-location/get.go 校验规则 如果显式指定要获取的 BackupStorageLocation，则其在集群中必须存在 流程逻辑 获取到 BackupStorageLocation 资源，根据 –output 指定的样式格式化输出 setpkg/cmd/cli/backup-location/set.go 校验规则 在指定 –credential 参数时，其值仅能包含一对 key-value 主体流程 根据命令行提供的参数信息，将其更新至集群中的 BackupStorageLocation 对象中 其中，如果参数信息是将其设置为默认（–default），则会将集群中其余的 BackupStorageLocation 设为非默认 ResticRepositoryAPI ResticRepository 不支持通过命令行手动创建，而是在备份流程中，由 Backup Controller 调用 ensureRepo，针对每一个卷命名空间，创建一个该对象。注意是 Pod 所在的命名空间（即卷命名空间），而非 Velero 或者 Restic 所在的命名空间 repopkg/cmd/cli/restic/repo/repo.go getpkg/cmd/cli/restic/repo/get.go 校验规则 如果显式指定要获取的 ResticRepository，则其在集群中必须存在 主体流程 获取到 ResticRepository 资源，根据 –output 指定的样式格式化输出 serverpkg/cmd/cli/restic/server.go server 本身是 velero restic 的 hidden 类型的命令，是 Restic 服务的启动命令。 由于 Pod 会将所在所在节点的 /var/lib/kubelet/pods 挂在到容器内 /host_pods 目录下，因此，会校验当前节点的所有 Pod 卷是否均已挂载到容器内 启动 Restic 服务，内部会启动 1 个 PodVolumeBackupController 和 1 个 PodVolumeRestoreController，用于处理卷备份与恢复的流程 BackupStorageLocation Controllerpkg/controller/backup_storage_location_controller.go ReconcileReconcile 源码 针对集群中的每一个 BackupStorageLocation，如果不存在默认的时，会根据 Velero 服务启动命令中的信息设置一个默认 在 Velero 之前版本中，Velero 服务启动时可以设置默认的 BackupStorageLocation，在 2.0 之后废弃，改用 velero backup-location set –default 的方式，而在 Controller 中仍然保留了这段逻辑，用于向后兼容 计算 BackupStorageLocation 是否已经准备好进行验证（即是否到达上次验证时间 + 验证频率），规则大致为 频率等于 0 时，不作验证 频率小于 0 时，为不合法场景，将频率重置为默认的 1 分钟 如果未做过验证（即第一次尝试验证时），无视其设置的验证频率，直接返回 true，表示立即开始验证流程 构建请求 StorageProvider 中所需要 BackupStorageLocation 对象，其中如果 BackupStorageLocation 中设置了 Credential 信息，则会获取位于 Velero 命名空间下的 Secret，将 Secret 内容以 &lt;Secret Name&gt; - &lt;Secret Key&gt; 的形式持久化到磁盘中，并将 BackupStorageLocation 的 credentialsFile 字段指向该文件通常位于 /tmp/credentials/velero 目录中 通过 StorageProvider 的 IsValid 接口判断 BackupStorageLocation 是否可用 更新集群中 BackupStorageLocation 状态和上次验证时间 最终无论结果如何，都会重新入队 ResticRepository Controllerpkg/controller/restic_repository_controller.go NewResticRepositoryControllerNewResticRepositoryController 源码 工厂函数 注册 Generic Controller 中的 syncHandler 和 resyncFunc 监听 ResticRepository 资源的 Add 事件，将 ResticRepository 以 key（namespace/name）的形式加入 Generic Controller 的 queue 中 processQueueItemprocessQueueItem 源码 注册在 Generic Controller 中 syncHandler 的实现 函数入参就是 Generic Controller 的 queue 中待处理的 ResticRepository key，通过解析获取的 namespace 和 name 查询到集群中的 ResticRepository 对象 如果 ResticRepository 对象状态为空或者是 New，则调用 initializeRepo 初始化一个 Restic 仓库 否则会进一步判断 ResticRepository 对象状态 如果状态为 Ready，则执行 restic prune 命令，判断是否可以建立连接，并更新 ResticRepository 上次维护时间信息 如果状态为 NotReady，则调用 ensureRepo 尝试检查或初始化一个仓库，并根据返回结果更新 ResticRepository 的状态为 Ready 或者 NotReadyrestic prune 的执行失败并不会影响到主流程，只是会在 ResticRepository 对象中记录错误信息 initializeRepoinitializeRepo 源码 尝试初始化 Restic 仓库的主体流程（真正初始化的动作位于 ensureRepo） ResticRepository 对象中有 BackupStorageLocation 的信息，根据这个信息获取集群中的 BackupStorageLocation 对象，如果获取失败，则更新 ResticRepository 对象的状态为 NotReady 调用 GetRepoIdentifier 获取 Restic 仓库信息（即 –repo 所需的信息），并更新至 ResticRepository 对象中，如果获取失败，则将 ResticRepository 对象的状态设置为 NotReady 调用 ensureRepo 尝试检查或初始化一个仓库，并根据返回结果更新 ResticRepository 的状态和上次维护时间信息 ensureRepoensureRepo 源码 检查 Restic 仓库是否存在，如果不存在则尝试初始化一个 通过执行 restic snapshots 的结果，来确保 Restic 仓库存在并且权限可达 如果命令执行返回错误信息中包含 “Is there a repository at the following location?” 字符串，表示 Restic 仓库不存在，会通过 restic init 命令初始化一个新仓库 GetRepoIdentifierGetRepoIdentifier 源码 构建 Restic 命令行需要的 –repo 参数内容 根据 BackupStorageLocation 的 Provider 信息获取到对应后端类型（velero.io/provider），Velero Restic 支持的类型有 velero.io/aws、velero.io/azure 和 velero.io/gcp，拼接后作为 RepoPrefix RepoPrefix 拼接上 ResticRepository 中的 VolumeNamespace 信息作为 RepoIdentifier，也就是 Restic 原生命令中的 –repo 参数 enqueueAllRepositoriesenqueueAllRepositories 源码 注册在 Generic Controller 中 resyncFunc 的实现，周期为 5 分钟 获取集群中所有的 ResticRepository 对象，全量加入到 Generic Controller 的 queue 中之所以是全量，是因为网络连接的不确定性，需要重新判断所有的 ResticRepository 可达状态","link":"/2022/01/17/2022-01-17%20Velero%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20Location/"},{"title":"「 Rust 」数据类型","text":"前言Rust 是静态编译语言，在编译时必须知道所有变量的类型 基于使用的值，编译器通常能够推断出它的具体类型 但如果可能的类型比较多，例如把 String 转为整数的 parse 方法，就必须添加类型的标注，否则编译会报错 1234fn main() { let guess: u32 = &quot;42&quot;.parse().expect(&quot;Not a number&quot;); println!(&quot;{}&quot;, guess);} 由于针对数字 42 在 Rust 中有很多数据类型可以将其包含在内，如 i32 和 u32 等等，所以要给变量具体指明类型，如果未指明，则会编译报错： 123456$ cargo runerror[E0282]: type annotations needed --&gt; src/main.rs:2:9 |2 | let guess = &quot;42&quot;.parse().expect(&quot;Not a number&quot;); | ^^^^^ consider giving `guess` a type 标量类型一个标量类型代表一个单个的值。 Rust 有四个主要的标量类型： 整数类型 浮点类型 布尔类型 字符类型 整数类型整数类型没有小数部分，无符号整数类型以 u（usize）开头，有符号整数类型以 i （integer）开头，例如 u32 就是一个无符号的整数类型，占据 32 位的空间。 整数类型表 每种都分 i 和 u，以及固定的位数 有符号的范围是 -(2^n^ - 1) 到 2^n-1^ - 1 无符号范围：0 到 2^n^ -1 Length Signed Unsigned 8-bit i8 u8 16-bit i16 u16 32-bit i32 u32 64-bit i64 u64 128-bit i128 u128 arch isize usize isize 和 usize 类型的位数由程序运行的计算机的架构所决定，如果是 64 位计算机，那就是 64 位的。使用场景比如，对某个集合进行索引操作。 整数字面值 Number literals Example Decimal 98_222 Hex 0xff Octal 0o77 Binary 0b1111_0000 Byte (u8 only) b’A’ 除了 byte 类型外，所有的数字字面值都允许使用类型后缀，例如 57u8 12345fn main() { // 此时，foo 的类型为 u8，值为 57 let foo = 57u8; println!(&quot;{}&quot;, foo)} 整数的默认类型就是 i32 整数溢出例如，u8 的范围是 0 - 255，如果把一个 u8 类型变量的值设为 256，那么： 调试模式下编译： Rust 会检查整数溢出，如果发生溢出，程序在运行时就会 panic 发布模式下编译： Rust 不会检查可能导致 panic 的整数溢出，如果发生溢出，Rust 会执行环绕操作，也就是 256 变为 1，257 变为 2，以此类推 12345fn main() { let mut foo: u8 = 255; foo = foo + 2; println!(&quot;{}&quot;, foo)} cargo run 123$ cargo runthread 'main' panicked at 'attempt to add with overflow', src/main.rs:3:11note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace cargo build –release 12$ ./target/release/hello-world1 浮点类型Rust 的浮点类型使用了 IEEE-754 标准来表述，有两种基础的浮点类型，也就是含有小数部分的类型： f32，单精度 f64，双精度，Rust 中默认的浮点类型 布尔类型Rust 的布尔类型（true &amp; false）占用 1 字节大小，符号为 bool。 字符类型Rust 中 char 类型用来描述语言中最基础的单个字符，字符类型的字面值使用单引号，占用 4 字节的大小，是 Unicode 的标量值，可以表示比 ASCII 多得多的字符内容，如拼音，中日韩文，零长度空白字符，emoji 表情等。 范围是 U+0000 到 U+D7FF 和 U+E000 到 U+10FFFF。 12345fn main() { let x = 'z'; let y: char = '字'; let z = '😄';} 复合类型复合类型可以将多个值放到一个类型里。 Rust 有两个主要的复合类型： 元组 数组 元组（Tuple）元组可以将多个类型的多个值放到一个类型里，并且长度固定，一旦声明不可改变。 元组的类型为：(类型1,类型2,...) 访问元组中的元素值可以使用模式匹配（destructure）和点标记法。 模式匹配 123456fn main() { let tup: (i32, bool, char) = (500, false, 'A'); // x, y, z 的类型与值分别对应 tup 中的三个元素，即 let x: 132 = 500 let (x, y, z) = tup; println!(&quot;{}, {}, {}&quot;, x, y, z);} 点标记法 1234fn main() { let tup: (i32, bool, char) = (500, false, 'A'); println!(&quot;{}, {}, {}&quot;, tup.0, tup.1, tup.2)} 点标记法的“索引“不可以是变量。 12345fn main() { let tup: (i32, bool, char) = (500, false, 'A'); let i = 0; println!(&quot;{}&quot;, tup.i)} 123456$ cargo runerror[E0609]: no field `i` on type `(i32, bool, char)` --&gt; src/main.rs:4:24 |4 | println!(&quot;{}&quot;, tup.i) | ^ 和 Python 等其他语言一样，当元组中只有一个元素，类型和值均需要加逗号，当没有逗号时，编译器会认为其是一个标量类型，括号会被视为多余。 1234fn main() { let tup: (i32,) = (1,); println!(&quot;{}&quot;, tup.0)} 数组数组可以将多个值放到一个类型中，但是数组中每个元素的类型必须相同，并且长度固定，一旦声明不可改变。 数组的类型为：[类型;长度] 123fn main() { let a: [i32;5] = [1, 2, 3, 4, 5];} 除了上述的声明方式外，如果数组中的每个元素值都相同，那么可以快速声明为： 1234fn main() { // 相当于 let a = [3, 3, 3, 3, 3] let a = [3;5];} 类似于 Golang，访问数组中的元素值可以使用索引法。 索引越界 如果访问的索引超出了数组的范围，处理方式和 Golang 类似，也就是： 编译时会通过，但是不是绝对的，Rust 编译器无法直接判断出是否越界等较复杂的情况 运行时会报错，区别于 C 和 C++ 等，虽然数组在内存中为连续的地址，但是越界的内存空间不属于该数组，所以无法访问","link":"/2022/01/16/2022-01-16%20Rust%20%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"title":"「 Velero 」源码走读 — Backup","text":"based on v1.6.3 BackupAPI backuppkg/cmd/cli/backup/backup.go velero backup 包括 6 个子命令：create、delete、describe、download、get 和 logs。 createpkg/cmd/cli/backup/create.go 校验规则 必须指定 Backup 名称，除非指定了 –from-schedule 参数 在指定 –storage-location 参数时，其在集群中必须存在 在指定 –volume-snapshot-locations 参数时，其在集群中必须存在 主体流程 根据命令行参数，构建 Backup 对象，下发至集群中创建，后续由 Backup Controller 负责维护 如果开启了 –wait，则启动 informer 监听 Backup 对象状态，阻塞直至状态不再是 New 或者 InProgress 如果 Backup 是基于定时任务（Schedule）创建的，则忽略其他所有的 filter 信息，以 Schedule 规格为准，除此之外，Backup 的名称也可以不指定，默认格式为 schedule-timestamp。 deletepkg/cmd/cli/backup/delete.go 校验规则 name、–all 和 –selector 参数有且仅能有一个 要删除的 Backup 在集群中必须存在 主体流程 根据命令行参数，构建 DeleteBackupRequest 对象，下发至集群中创建，后续由 BackupDeletion Controller 负责维护 describepkg/cmd/cli/backup/describe.go 校验规则 要获取的 Backup 在集群中必须存在 主体流程 获取到 Backup 的删除事件 获取到卷备份的信息和 CSI 快照信息（如果 Velero 服务开启了 CSI 特性） 将以上信息和 Backup 的元信息、规格、状态等汇总作为描述信息格式化输出在开启 –details 时，会构建 DownloadRequest 对象获取 BackupResourceList 的信息 downloadpkg/cmd/cli/backup/download.go 校验规则 要获取的 Backup 在集群中必须存在 主体流程 根据命令行参数，构建 DownloadRequest 对象，下发至集群中创建，后续由 DownloadRequest Controller 负责维护，获取 BackupContents 的信息 阻塞直至 DownloadRequest 的 DownloadURL 被设置，将内容写入 –output 指定的位置 getpkg/cmd/cli/backup/get.go 校验规则 要获取的 Backup 在集群中必须存在 主体流程 获取到 Backup 资源，根据 –output 指定的样式格式化输出 logspkg/cmd/cli/backup/logs.go 校验规则 要获取的 Backup 在集群中必须存在 Backup 的状态必须为 Completed、PartiallyFailed 或者 Failed 主体流程 根据命令行参数，构建 DownloadRequest 对象，下发至集群中创建，后续由 DownloadRequest Controller 负责维护，获取 BackupLog 的信息 阻塞直至 DownloadRequest 的 DownloadURL 被设置，将内容写入 stdout 中 PodVolumeBackupAPI 对象不支持手动创建，而是在备份流程中，由 Backup Controller 调用 backupPodVolumes，针对每一个 Pod 卷，创建一个该对象。 ScheduleAPI schedulepkg/cmd/cli/schedule/schedule.go velero schedule 包括 4 个子命令：create、delete、describe 和 get。 createpkg/cmd/cli/schedule/create.go 校验规则 –schedule 为必需参数 在指定 –storage-location 参数时，其在集群中必须存在 在指定 –volume-snapshot-locations 参数时，其在集群中必须存在 创建定时备份任务时并不会校验 schedule 表达式的合法性，而是交给 Schedule Controller 作后续处理。 主体流程 根据命令行参数，构建 Schedule 对象，下发至集群中创建，后续由 Scheduler Controller 负责维护 deletepkg/cmd/cli/schedule/delete.go 校验规则 name、–all 和 –selector 参数有且仅能有一个 要删除的 Schedule 在集群中必须存在 主体流程 根据命令行参数，获取集群中的 Schedule 资源并删除 describepkg/cmd/cli/schedule/describe.go 校验规则 要获取的 Schedule 在集群中必须存在 主体流程 获取到 Schedule 资源，格式化输出 getpkg/cmd/cli/schedule/get.go 校验规则 要获取的 Schedule 在集群中必须存在 主体流程 获取到 Schedule 资源，根据 –output 指定的样式格式化输出 Backup Controllerpkg/controller/backup_controller.gopkg/backup/backup.go NewBackupControllerNewBackupController 源码 工厂函数 注册 Generic Controller 中的 syncHandler 和 resyncFunc 监听 Backup 资源的 Add 事件，将状态是空或者 New 的 Backup 以 key（namespace/name）的形式加入 Generic Controller 的 queue 中 processBackupprocessBackup 源码 注册在 Generic Controller 中 syncHandler 的实现 函数入参就是 Generic Controller 的 queue 中待处理的 Backup key，通过解析获取的 namespace 和 name 查询到集群中的 Backup 对象 仅处理状态是空或者 New 的 Backup 对象 调用 prepareBackupRequest 做一些校验准备工作，并根据校验的结果设置 Backup 的状态为 FailedValidation 或者 InProgress 过滤掉校验失败的 Backup，调用 runBackup，执行备份和上传备份信息的流程，执行结果决定了备份是否顺利完成，如果有错误返回，则记录 Backup 状态为 Failed prepareBackupRequestprepareBackupRequest 源码 在不破坏集群中原 Backup 对象（以下称为 original）的情况下，构建了一个 BackupRequest 对象（以下称为 request），这个对象包含了 original 的详细规格（即 original 的深拷贝），并且包含一些丰富处理流程的中间态信息，并且在整个备份流程中的操作都是基于 request，在备份完成后，会将 request 信息同步更新至集群中的 Backup 对象。 12345678910111213141516// Request is a request for a backup, with all references to other objects// materialized (e.g. backup/snapshot locations, includes/excludes, etc.)type Request struct { *velerov1api.Backup StorageLocation *velerov1api.BackupStorageLocation SnapshotLocations []*velerov1api.VolumeSnapshotLocation NamespaceIncludesExcludes *collections.IncludesExcludes ResourceIncludesExcludes *collections.IncludesExcludes ResourceHooks []hook.ResourceHook ResolvedActions []resolvedAction VolumeSnapshots []*volume.Snapshot PodVolumeBackups []*velerov1api.PodVolumeBackup BackedUpItems map[itemKey]struct{}} 对 request 赋值和校验的操作 将 original 深拷贝至 request 中的 Backup 中，并设置 request 版本、过期时间、是否将卷数据备份至 Restic、StorageLocation 等信息 校验 BackupStorageLocation 的合法性，以及 access mode 是否为预期的 ReadWrite 检验 volume snapshot location 的合法性backup.Spec.VolumeSnapshotLocation 为 []string 类型，支持多个 location，但是要求 location 和 VolumeSnapshotter 必须是一对一的关系（也就是说不允许多个 location 对应同一个 VolumeSnapshotter）。默认情况下，–snapshotvolume 为 true，所以只要存在一个合法的 default vsl，则最终的 backup.Spec.VolumeSnapshotLocation 均会包含这个 default vsl，详细逻辑参考 TestValidateAndGetSnapshotLocations 设置 request 的注解和 resources &amp; namespaces 的 included &amp; excluded 检验信息 runBackuprunBackup 源码 备份的整体流程 基于临时文件句柄生成 gzip writer ，并指定 stdout 和 gzip writer 为 logger 输出，初始化用于统计日志级别数量的 counter因此，日志不仅会输出在 Velero Pod 中，并且会生成 BackupLog，后续会上传至 BackupStorageLocation 中 生成一个用于存放 Backup 版本和内容的临时文件后续在调用 Backup 时会传入该临时文件 获取注册的 BackupItemAction 插件后续在调用 Backup 时会传入该 action 信息 通过 StorageProvider 的 BackupExists 接口判断远端存储中是否有同名备份 如果存在，则设置 Backup 状态为 Failed，本次备份失败 如果不存在，则调用 Backup 进行准备与备份 如果 Velero 开启了 CSI 特性，则获取集群中与该 Backup 相关的 VolumeSnapshots 和 VolumeSnapshotContents 信息后续会上传至 BackupStorageLocation 中 设置 request 的卷快照总量与成功量、完成时间戳、Warnings 和 Errors 的个数以及备份的状态等等，关闭日志文件，本次备份任务日志记录完毕备份的状态（Failed、PartiallyFailed 或者 Completed）是根据日志输出级别的统计决定的，fatalErrs 记录了调用 backupper.Backup 产生的错误日志信息，不仅如此，在后续上传备份文件的时候，如果发生异常，也会记录，所以 request 的状态是否为 Failed 是 fatalErrs 决定的，而状态是否为 Completed 和 PartiallyFailed 是根据日志中 Error 级别的输出数量决定的，该日志也包括调用 StorageProvider 中产生的日志级别信息 123456789101112131415backup.Status.Warnings = logCounter.GetCount(logrus.WarnLevel)backup.Status.Errors = logCounter.GetCount(logrus.ErrorLevel)// Assign finalize phase as close to end as possible so that any errors// logged to backupLog are captured. This is done before uploading the// artifacts to object storage so that the JSON representation of the// backup in object storage has the terminal phase set.switch {case len(fatalErrs) &gt; 0: backup.Status.Phase = velerov1api.BackupPhaseFailedcase logCounter.GetCount(logrus.ErrorLevel) &gt; 0: backup.Status.Phase = velerov1api.BackupPhasePartiallyFaileddefault: backup.Status.Phase = velerov1api.BackupPhaseCompleted} 重新获取 StorageProvider，避免长时间备份中认证信息变动， 调用 StorageProvider 的 PutBackup 接口将备份信息上传至 BackupStorageLocation 中，具体文件的对应关系如下： 名称 BackupStorageLocation 中的文件 数据源 Metadata velero-backup.json backup.Backup 对象 Content &lt;backup&gt;.tar.gz 步骤 2 中的临时文件内容 Log &lt;backup&gt;-logs.gz 步骤 6 中最终生成的 log 文件 PodVolumeBackups &lt;backup&gt;-podvolumebackups.json backup.PodVolumeBackups VolumeSnapshots &lt;backup&gt;-volumesnapshots.json.gz backup.VolumeSnapshots BackupResourceList &lt;backup&gt;-resource-list.json.gz backup.BackedUpItems CSIVolumeSnapshots &lt;backup&gt;-csi-volumesnapshots.json.gz 步骤 5 中 volume snapshots CSIVolumeSnapshotContents &lt;backup&gt;-csi-volumesnapshotcontents.json.gz 步骤 5 中 volume snapshot contents 这里需要区分 PodVolumeBackups 和 VolumeSnapshots PodVolumeBackups 是描述了备份的 Pod 中的卷数据信息，与之关联的是 Restic 相关概念，数据最终会写入 ResticRepository 中 VolumeSnapshots 是相比于 CSI 而言属于 Velero 原生的卷快照，用于描述一个 PV 快照的信息，本身作为 Backup 的一部分，数据最终会由对应的 SnapshotProvider 处理 即使两者最终的数据均不会存放在 BackupStorageLocation 中，但是仍然会在 BackupStorageLocation 中记录其基础信息 BackupBackup 源码 备份动作本身的流程 基于传入的临时文件，生成 gzip writer，最终会以 backup.tar.gz 格式存放在 BackupStorageLocation 中作为 Content，该 tar.gz 包括两个主要内容：metadata 和 resources，前者用于存放版本，后者用于存放被备份资源的详细规格信息区别于记录 Backup 本身的 Metadata，此处的 metdata 仅为一个目录层级；在后续 backupItem 流程中，会将资源信息写入 resources 文件中 在 metadata 目录下写入版本信息 version，固定值为 1.1.0 设置 request 的 resource &amp; namespace 的 included &amp; excluded、resources hook 以及 resolve action（BackupItemAction） namespace 和 resource 的处理方式并不一样，是因为 namespace 只要做匹配即可，存在与不存在很容易定性，但是 resource 有很多种表示方式，例如 pv 和 persistentvolumes，对于复杂的资源来讲，还有 ApiGroup 的概念，因此，需要 RESTmapping 的 ResourceFor 匹配出最合适且规范的一个 GVR，这也就是为什么使用时可以在合理范围任意指定资源名称均会匹配正确的原因 BackupItemAction 有两个接口，一个是 AppliesTo，一个是 Execute，前者用于返回 labelSelector，用于备份阶段的过滤筛选，后者用于执行 BackupItemAction 中定义的额外动作 生成临时空文件，供后续 itemCollector 使用，itemCollector 根据 Backup 规格信息通过 K8s API 收集待备份的资源详细信息并写入空文件中该空文件用于 getAllItems 时，作为每一个 item 的解构文件的根目录 getAllItems 通过 discovery 获取到所有的 API group（例如 batch/v1beta1，networking.k8s.io/v1beta1 等等），然后根据每个 group 获取到 resource（例如 cronjobs，networkpolicies 等等），根据 namespace 和 resource 的 include 和 exclude 以及标签选择器规则进行过滤资源，过滤后的结果就是待备份的对象（item）item 的内容（unstructured.Unstructured 对象）已经写入了步骤 4 的临时目录里，同时，这个文件的路径会记录在 item 的 path 中，后续在 backupItem 时，会解析作为解构状态的 item 传入 更新 Backup 的待备份资源的总量信息 生成 update 队列，用于记录 Backup 状态信息，同时启动一个 Goroutine 监听队列，每秒钟获取一次 update 队列中的进度并更新至 Backup 中 遍历每一个待备份的 item，调用 backupItem 函数，进行备份，并将进度信息写入 update 队列中，完成进度上报 如果备份规格中指定了备份集群级别资源（IncludeClusterResources），则额外备份 CRD 资源这里的 CRD 资源其实是有限制条件的，就是仅处理已经备份了与 CRD 相关的 CR 资源时，才会备份对应的 CRD 更新集群中 Backup 对象的备份进度信息至 100% backupItembackupItem 源码 单个 item 资源的备份流程 传入的 item 是解构状态的资源信息（runtime.Unstructured），解构数据来源于 item 的 path 字段，需要重新构建成 metav1.Object，提取信息 对于以下情况跳过备份，并且返回 false 标识，表示资源未参与备份未参与备份的资源，即使在指定了备份集群级别资源时也不会备份相关的 CRD 标签中有 velero.io/exclude-from-backup 字段的资源 资源属于 Backup 规格中指定的 namespace 和 groupResource 的 非 namespace 的资源即是集群级别的，但是备份规格中的 IncludeClusterResources 为 false 资源处于删除状态 对于已经备份的资源，自然也会跳过备份，但是返回 true 标识，表示资源已经备份 执行 pre hook 动作（只有类型是 Pod 的 资源才会真正的处理 pre hook），hook 作用的对象需要满足 BackupItemAction 中定义的 applyTo 接口的标签选择，hook command 的执行（即 execute 接口）是通过 K8s rest API exec 实现 针对类型是 Pod 的资源，在 spec.Volumes 中获取需要借助 Restic 备份的卷由于 PV 和 PVC 是 1 对 1，PVC 和 Pod 是 1 对 n 的关系，所以在这里通过跟踪 PVC 的备份情况即可判断卷是否被备份过，备份过的卷，会在相关的 tracker 中以 PVC 为 key 作记录 调用 BackupItemAction 的 execute 接口，进行额外的操作，如更新资源等，此后的流程基于接口返回的对象继续操作，执行失败时，会执行 post hook 动作 针对类型是 PV 的资源，调用 takePVSnapshot，忽略已经被 Restic 备份的 PV，初始化 SnapshotProvider，通过一系列的接口，完成卷快照的操作，并将快照信息记录在 request 的 VolumeSnapshots 中在 Velero 开启 CSI 特性时，需要额外加载一个 plugin（https://github.com/vmware-tanzu/velero-plugin-for-csi），该 plugin 就是 SnapshotProvider 类型。因此在这步时，便会对 PV 做快照操作 针对类型是 Pod 的资源，调用 BackupPodVolumes，借助 Restic 能力实现对 Pod 卷数据的备份 执行 post hook 动作 在 resources 目录下写入备份的资源信息，资源会根据 kind、 namepace 等信息归类，文件内容为 item 的 runtime.Unstructured 形式，以 json 格式存储文件是上层调用传入的 Content 文件 至此，针对单一的 item 备份流程结束，其中包含了存储在 kube-apiserver 中的结构化元信息和卷数据的备份 BackupPodVolumesbackupPodVolumes 源码 借助 Restic 能力备份卷数据的流程 每一个 Pod 卷所属的 namespace 必须有且仅有一个对应的 ResticRepository 对象，如果不存在，则会创建一个，由 ResticRepository Controller 会负责维护状态，而 Backup Controller 会阻塞直至 ResticRepository 超时或者 ready创建对象时会指定基于的 BackupStorageLocation，并将 BackupStorageLocation 转换成 RepoIdentifier 信息，也就是 Restic 原生命令中的 -r 参数 过滤掉 hostPath 类型的卷，对其余的合法卷会创建对应的 PodVolumeBackup 对象，其中 PodVolumeBackup 中会设置 velero.io/backup-name 标签以及 PVC 的 UID 信息此时，该对象的 spec.RepoIdentifier 已被设置，例如 s3:http://minio.velero.svc:9000/velero/restic/velero。此外，由于在 velero 1.6.3 中不会判断 Pod 的状态，因此依旧会对 pending 状态的 Pod 创建 PodVolumeBackup 对象，但是此 PodVolumeBackup 对象没有 nodeName 属性，导致 Restic 不作处理，从而阻塞 Velero 直至超时。除此之外，restic 状态异常也会导致类似问题，参考：https://github.com/vmware-tanzu/velero/issues/4874 PodVolumeBackup Controller 会负责卷的备份，而 Backup Controller 会阻塞直至卷备份返回完成或者失败 resyncresync 源码 注册在 Generic Controller 中 resyncFunc 的实现，周期为 1 分钟 获取集群中所有的 Backup 对象，更新 backup_total 指标，value 为集群中所有 Backup 总数 针对每一个状态已经完成且归属于某一个 Schedule 的 Backup，设置 backup_last_successful_timestamp 指标，key 为 Schedule 名称，value 为最近的一次备份时间戳 PodVolumeBackup Controllerpkg/controller/pod_volume_backup_controller.gopkg/restic/exec_commands.go NewPodVolumeBackupControllerNewPodVolumeBackupController 源码 工厂函数 注册 Generic Controller 中的 syncHandler，并将 PodVolumeBackup、Pod 和 PVC 添加到 cacheSyncWaiters，等待同步完成 监听 PodVolumeBackup 资源的 Add 和 Update 事件，将状态是空或者 New 并且位于当前节点的 PodVolumeBackup 资源以 key （namespace/name） 的形式加入 Generic Controller 的 queue 中PodVolumeBackup Controller 运行在 DaemonSet 形式的 Restic 服务中，挂载所在节点的 Pod 卷，因此 PodVolumeBackup 具有节点属性，PodVolumeBackup Controller 仅处理当前节点的 PodVolumeBackup 对象 processQueueItemprocessQueueItem 源码 注册在 Generic Controller 中 syncHandler 的实现 函数入参就是 Generic Controller 的 queue 中待处理的 PodVolumeBackup key，通过解析获取的 namespace 和 name 查询到集群中的 PodVolumeBackup 对象 仅处理状态为空或者 New 的 PodVolumeBackup 对象，调用 processBackup，执行卷数据的备份 ProcessBackupProcessBackup 源码 卷数据备份的整体流程 更新 PodVolumeBackup 状态为 InProgress 校验 PodVolumeBackup 对象规格中声明的卷源 Pod 是否存在，获取到 Pod 卷在 host 上子目录信息例如 /host_pods/e4ccf918-76d7-4972-a54a-39b39f15b53b/volumes/kubernetes.io~empty-dir/plugins，/host_pods 为 Restic Pod 中的目录，挂载了 host 的 /var/lib/kubelet/pods/， 详细逻辑参考 TestGetVolumeDirectorySuccess 生成用于连接 Restic Repo 所需要的临时密码文件，文件固定为 /tmp/credentials/velero/velero-restic-credentials-repository-password，内容为 static-passw0rd，用于 restic 原生命令中的 –password 参数密码会以 Secret 的形式存储在集群中，名为 velero-restic-credentials，位于 velero 命名空间内 构建 restic backup 命令 如果 BackupStorageLocation 有 caCert 证书信息，会将其临时写入到磁盘中，供 Restic 认证使用，并设置在 restic backup 命令中因为本质上，Restic 的 repo 和 Velero 的 BackupStorageLocation 为同一个 给 restic backup 命令设置 Restic 原生所需的环境变量信息 在备份流程中生成 PodVolumeBackup 对象时，如果 Pod 卷源于 PVC，则会对 PodVolumeBackup 加一个 velero.io/pvc-uid 的 label，值为 PVC 的 uid。因此，在这里会通过这个 label 判断卷是否源于 PVC，如果是，则会获取集群中所有带有此 label、状态已经完成并且 BackupStorageLocation 相同的 PodVolumeBackup 对象，如果存在则表示这个卷已经备份过，后续的备份会基于最近的一次备份点进行增量备份，反之则全量备份，这里的增量备份借助了 Restic 原生功能（–parent）详细逻辑参考 getParentSnapshot 调用 RunBackup，执行 Restic 原生的卷数据备份流程 更新 PodVolumeBackup 对象的状态、完成时间、快照 ID 和卷数据路径等信息 RunBackupRunBackup 源码 调用 Restic 原生备份命令 restic backup 将入参的 Command 对象构建成可执行命令并执行 启动一个 Goroutine，每 10 秒钟解析一次 restic backup 命令的标准输出，更新 PodVolumeBackup 的备份进度信息（待备份总文件大小和当前备份文件大小） 通过解析标准输出判断如果 Restic 备份成功，则更新 PodVolumeBackup 进度至 100% Schedule Controllerpkg/controller/schedule_controller.go NewScheduleControllerNewScheduleController 源码 工厂函数 注册 Generic Controller 中的 syncHandler 和 resyncFunc 监听 Schedule 资源的 Add 事件，将状态是空、New 或者 Enabled 的 Schedule 以 key（namespace/name）的形式加入 Generic Controller 的 queue 中 processScheduleprocessSchedule 源码 注册在 Generic Controller 中 syncHandler 的实现 函数入参就是 Generic Controller 的 queue 中待处理的 Schedule key，通过解析获取的 namespace 和 name 查询到集群中的 Schedule 对象 仅处理状态为空、New 或者 Enabled 的 Schedule 对象 检验 cron 表达式的合法性，根据校验结果更新 Schedule 的状态为 FailedValidation 或者 Enabled 判断是否到达定时任务的下次执行时间，如果达到则立即创建一个备份。另外，如果定时任务从未执行，也会立即创建一个备份任务 enqueueAllEnabledSchedulesenqueueAllEnabledSchedules 源码 注册在 Generic Controller 中 resyncFunc 的实现，周期为 1 分钟 获取集群中所有的 Schedule 对象 将状态是 Enabled 的 Schedule 对象加入到 Generic Controller 的 queue 中 BackupDeletion Controllerpkg/controller/backup_deletion_controller.gointernal/delete/delete_item_action_handler.go 删除 Backup 时，与 Backup 相关联的各种资源基本上都是通过 velero.io/backup-name 标签获取到。因此，在备份的时候，创建的相关资源也都会打上该标签。 NewBackupDeletionControllerNewBackupDeletionController 源码 工厂函数 注册 Generic Controller 中的 syncHandler 和 resyncFunc 监听 DeleteBackupRequest 资源的 Add 事件，将 DeleteBackupRequest 以 key（namespace/name）的形式加入 Generic Controller 的 queue 中 processQueueItemprocessQueueItem 源码 注册在 Generic Controller 中 syncHandler 的实现 函数入参就是 Generic Controller 的 queue 中待处理的 DeleteBackupRequest key，通过解析获取的 namespace 和 name 查询到集群中的 DeleteBackupRequest 对象 仅处理状态不为 Processed 的 DeleteBackupRequest 对象，调用 processRequest，执行 Backup 的删除 processRequestprocessRequest 源码 备份删除的整体流程 如果 DeleteBackupRequest 对象所属的 Backup 信息不存在，则认为 DeleteBackupRequest 处理完成，即将其状态设为 Processed，并设置错误信息 删除集群中针对该 Backup 的其余 DeleteBackupRequest 对象，仅处理当前的 如果要删除的 Backup 处于 InProgress 状态或者不存在，则将 DeleteBackupRequest 状态设为 Processed，并设置错误信息 如果 Backup 所属的 BackupStorageLocation 不存在或者模式为 ReadOnly 时，则将 DeleteBackupRequest 状态设为 Processed，并设置错误信息 至此，校验工作已经完成，将 DeleteBackupRequest 状态设置为 InProgress，并设置 velero.io/backup-name 和 velero.io/backup-uid 标签 设置 Backup 的状态为 Deleting 获取注册的 DeleteItemAction 插件，如果获取到了，则下载 BackupStorageLocation 中的 Content 文件，构建运行 DeleteItemAction 所需要环境变量信息，调用 InvokeDeleteActions 处理 DeleteItemAction 中定义的逻辑 调用 StorageProvider 的 GetBackupVolumeSnapshots 方法获取 BackupVolumeSnapshotsKey（也就是 backup-volumesnapshots.json.gz） 的内容，调用 SnapshotProvider 的 DeleteSnapshot 接口删除 PV 快照信息 获取到与 Backup 相关联的 PodVolumeBackup 信息，进而获取到 Restic 的 snapshots，执行 restic forget 删除快照创建备份的时候，会根据 Pod 卷创建 PodVolumeBackup 对象，并会设置 velero.io/backup-name 标签 调用 StorageProvider 的 DeleteBackup 接口删除 BackupStorageLocation 中 Backup 所在的目录 如果 Velero 开启了 CSI 特性，那么也会删除与 Backup 相关联的 VolumeSnapshot 和 VolumeSnapshotContent 对象删除之前会将 VolumeSnapshotContent 回收策略置为 Delete 调用 StorageProvider 的 DeleteRestore 方法，删除 BackupStorageLocation 上基于该 Backup 创建的 Restore 文件，删除基于该 Backup 创建的 Restore 对象 如果以上步骤均无错误返回，则删除集群中相关的 Backup 对象 更新 DeleteBackupRequest 状态设为 Processed，并设置错误信息 如果以上步骤均无报错返回，则删除与该 Backup 相关的所有 DeleteBackupRequest 对象 InvokeDeleteActionsInvokeDeleteActions 源码 执行 DeleteItemAction 中的动作 如果未定义 action 信息，则直接返回，继续处理删除流程 将传入的 BackupStorageLocation 中的 Content 文件解压至临时文件下，通过 discovery API 将文件内容转换成 GroupResource 遍历 GroupResource 中的所有资源，执行 DeleteItemAction 中声明的动作 deleteExpiredRequestsdeleteExpiredRequests 源码 注册在 Generic Controller 中 resyncFunc 的实现，周期为 1 小时 获取集群中所有的 DeleteBackupRequest 对象，将状态已经处于 Processed，并且 age 超过 24 小时的 DeleteBackupRequest 对象删除因为并非所有的 DeleteBackupRequest 对象在走完 syncHandler 流程后均会被删除 BackupSync Controllerpkg/controller/backup_sync_controller.go NewBackupSyncControllerNewBackupSyncController 源码 工厂函数 注册 Generic Controller 中的 resyncFunc runrun 源码 注册在 Generic Controller 中 resyncFunc 的实现，周期为 30 秒 获取集群中所有的 BackupStorageLocation 对象，构建一个默认的 BackupStorageLocation 位于第一位的列表 遍历步骤 1 中的 BackupStorageLocation 列表，如果同步周期等于 0 代表该 BackupStorageLocation 不作同步操作，直接跳过即可，否则判断其是否到达下次同步时间 调用 StorageProvider 的 ListBackups 方法获取所有的 Backup；同时获取集群中所有的 Backup 获取在 BackupStorageLocation 中但是不在集群中的 Backup，这些就是待同步的 Backup 针对每一个待同步的 Backup，调用 StorageProvider 的 GetBackupMetadata 方法，获取 Metadata 文件（即 velero-backup.json），解析内容得到 Backup 对象 设置 Backup 的 namespace、resourceVersion、storageLocation 和 label 等信息，并在集群中创建既然已经存在于 BackupStorage Location，状态必然为完成状态（即不是空或者 New），因此不会被 Backup Controller 重复处理 调用 StorageProvider 的 GetPodVolumeBackups 方法获取与该 Backup 相关的 PodVolumeBackup 设置 PodVolumeBackup 的 namespace、resourceVersion、ownerReferences 和 label 等信息，并在集群中创建同理，不会被 PodVolumeBackup Controller 重复处理 如果 Velero 开启了 CSI 特性，通过 StorageProvider 的 GetCSIVolumeSnapshotContents 方法获取与该 Backup 相关的 VolumeSnapshotContents 设置 VolumeSnapshotContent 的 resourceVersion 等信息，并在集群中创建 删除孤儿 Backup，也就是在集群中存在，状态为 Completed，但是在 BackupStorageLocation 中不存在的 Backup 更新集群中该 BackupStorageLocation 的上次同步时间实际上步骤 4 获取 BackupStorageLocation 的 Backup 时可以作为同步操作 GC Controllerpkg/controller/gc_controller.go NewGCControllerNewGCController 源码 工厂函数 注册 Generic Controller 中的 syncHandler 和 resyncFunc 监听 Backup 资源的 Add 和 Update 事件，将 Backup 以 key（namespace/name）的形式加入 Generic Controller 的 queue 中 processQueueItemprocessQueueItem 源码 注册在 Generic Controller 中 syncHandler 的实现 函数入参就是 Generic Controller 的 queue 中待处理的 Backup key，通过解析获取的 namespace 和 name 查询到集群中的 Backup 对象 仅处理已经过期的 Backup 对象 获取 Backup 所属的 BackupStorageLocation，判断其模式是否为 ReadWrite 获取集群中和该 Backup 相关的 DeleteBackupRequest 对象，如果其中存在状态为空、New 和 InProgress 的，则认为正在删除，本次不做处理；否则，构建一个 DeleteBackupRequest 对象，下发至集群中创建，后续由 BackupDeletion Controller 负责维护 enqueueAllBackupsenqueueAllBackups 源码 注册在 Generic Controller 中 resyncFunc 的实现，周期为 1 小时 获取集群中所有的 Backup 对象，全量加入到 Generic Controller 的 queue 中","link":"/2022/01/27/2022-01-27%20Velero%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20Backup/"},{"title":"「 Velero 」源码走读 — Restore","text":"based on v1.6.3 RestoreAPI restorepkg/cmd/cli/restore/restore velero restore 包括 5 个子命令：create、delete、describe、get 和 logs。 createpkg/cmd/cli/restore/create.go 校验规则 –from-backup 和 –from-schedule 参数有且仅能有一个 在指定 –from-backup 或者 –from-schedule 参数时，其在集群中必须存在 在指定 –from-schedule 参数时，则由该 Schedule 创建出的 Backup 至少有一个 主体流程 如果指定了 –from-schedule 参数并且 –allow-partially-failed 参数为 true 时，获取集群中由该 Schedule 创建出的状态为 Completed 或者 PartiallyFailed 的最新 Backup，作为恢复的基准，并且将 –from-schedule 信息置空；否则，则直接透传 –from-schedule 信息用于后续构建 Restore 对象 根据命令行参数，构建 Restore 对象，下发至集群中创建，后续由 Restore Controller 负责维护 如果开启了 –wait，则启动 informer 监听 Restore 对象状态，阻塞直至状态不再是 New 或者 InProgress deletepkg/cmd/cli/restore/delete.go 校验规则 name、–all 和 –selector 参数有且仅能有一个 要删除的 Restore 在集群中必须存在 主体流程 根据命令行参数，获取集群中的 Restore 资源并删除 describepkg/cmd/cli/restore/describe.go 校验规则 要获取的 Restore 在集群中必须存在 主体流程 获取到 PodVolumeRestore 信息 将以上信息和 Restore 的元信息、规格、状态以及 Pod 卷数据恢复等汇总作为描述信息格式化输出 如果 Restore 中有 Error 或者 Warning 的日志时，会构建 DownloadRequest 对象获取 RestoreResults 的信息，展示原因 在开启 –details 时，会输出恢复的 Pod 卷信息 getpkg/cmd/cli/restore/get.go 校验规则 要获取的 Restore 在集群中必须存在 主体流程 获取到 Restore 资源，根据 –output 指定的样式格式化输出 logspkg/cmd/cli/restore/logs.go 校验规则 要获取的 Restore 在集群中必须存在 Restore 的状态必须为 Completed、PartiallyFailed 或者 Failed 主体流程 根据命令行参数，构建 DownloadRequest 对象，下发至集群中创建 RestoreLog，下发至集群中创建，后续由 DownloadRequest Controller 负责维护，获取 RestoreLog 的信息 阻塞直至 DownloadRequest 的 DownloadURL 被设置，将内容写入 stdout 中 PodVolumeRestoreAPI 对象不支持手动创建，而是在恢复流程中，由 Restore Controller 调用 restoreItem，针对每一个 Pod 卷，创建一个该对象。 Restore Controllerpkg/controller/restore_controller.gopkg/restore/restore.go NewRestoreControllerNewRestoreController 源码 工厂函数 注册 Generic Controller 中的 syncHandler 和 resyncFunc 监听 Restore 资源的 Add 事件，将状态是空或者 New 的 Restore 以 key（namespace/name）的形式加入 Generic Controller 的 queue 中 processQueueItemprocessQueueItem 源码 注册在 Generic Controller 中 syncHandler 的实现 函数入参就是 Generic Controller 的 queue 中待处理的 Restore key，通过解析获取的 namespace 和 name 查询到集群中的 Restore 对象 仅处理状态为空或者 New 的 Restore 对象，调用 processRestore，执行恢复 processRestoreprocessRestore 源码 恢复的整体流程 调用 validateAndComplete 做一些校验准备工作，并根据校验结果设置状态为 Restore 的状态为 FailedValidation 或者 InProgress 过滤掉校验失败的 Restore，调用 runValidatedRestore，执行恢复和上传恢复信息的流程 根据步骤 2 的恢复结果，设置 Restore 的状态 如果有错误返回（函数返回 error），则认为恢复失败，将 Restore 状态设为 Failed 如果 Restore 的 Errors 存在错误信息，则为部分失败，将 Restore 状态设为 PartiallyFailed 否则认为 Restore 已完全恢复，将 Restore 状态设为 Completed runValidatedRestore 执行恢复过程中会设置 Restore 对象的 status.Errors 和 status.Warning 信息 validateAndCompletevalidateAndComplete 源码 校验 Restore 对象并返回 backupInfo 信息 12345type backupInfo struct { backup *api.Backup location *velerov1api.BackupStorageLocation backupStore persistence.BackupStore} 在 Restore 对象的 ExcludedResources 中追加以下资源（以下资源会被备份，但是不会被恢复） nodes events events.events.k8s.io backups.velero.io restores.velero.io resticrepositories.velero.io 校验 Restore 对象的 IncludedResources 中是否包含上述资源 校验 included/excluded resources/namespaces 信息以及 –from-backup 和 –from-schedule 是否有且仅有一个 如果指定了 –from-schedule，则校验并获取 Schedule 下最新的一次 Backup，并设置在 Restore 中如果没有设置，则 –from-backup 必然设置了，因此就不需要设置 Restore 的 Backup 信息了 根据 Restore 的 Backup 信息，查询并返回 BackupStorageLocation、Backup 和 StorageProvider 信息 runValidatedRestorerunValidatedRestore 源码 恢复的整体流程 获取注册的 RestoreItemActions 插件后续在调用 Restore 函数恢复时会使用到 调用 StorageProvider 的 GetBackupContents 接口，获取到 Backup 内容文件信息，写入临时文件中 根据 Restore 中的 Backup 名称信息，获取到集群中 PodVolumeBackup 信息 调用 StorageProvider 的 GetBackupVolumeSnapshots 接口，获取到 VolumeSnapshot 信息 用以上信息构建 Restore Request（原理类似 Backup 的 Request，不做赘述），其中 BackupReader 为步骤 2 中的临时文件句柄 123456789type Request struct { *velerov1api.Restore Log logrus.FieldLogger Backup *velerov1api.Backup PodVolumeBackups []*velerov1api.PodVolumeBackup VolumeSnapshots []*volume.Snapshot BackupReader io.Reader} 调用 Restore 进行恢复，返回恢复的结果 重新获取 StorageProvider，避免长时间恢复中认证信息变动 关闭日志文件，本次恢复任务日志记录完毕，调用 StorageProvider 的 PutRestoreLog 接口，将恢复任务的日志信息上传至 BackupStorageLocation 统计 Warning 和 Error 级别日志数量，更新至 Restore 对象中，并构建 Result 对象（即代码块中的 m 对象），记录过程中产生的日志信息 1234567891011121314restore.Status.Warnings = len(restoreWarnings.Velero) + len(restoreWarnings.Cluster)for _, w := range restoreWarnings.Namespaces { restore.Status.Warnings += len(w)}restore.Status.Errors = len(restoreErrors.Velero) + len(restoreErrors.Cluster)for _, e := range restoreErrors.Namespaces { restore.Status.Errors += len(e)}m := map[string]pkgrestore.Result{ &quot;warnings&quot;: restoreWarnings, &quot;errors&quot;: restoreErrors,} 调用 StorageProvider 的 PutRestoreResults 接口，将 Result 信息上传至 BackupStorageLocation 中 调用 StorageProvider 接口上传的具体文件对应关系如下： 名称 BackupStorageLocation 中的文件 数据源 Log restore-&lt;restore&gt;-logs.gz 步骤 8 中最终生成的 log 文件 Results restore-&lt;restore&gt;-results.gz 步骤 9 中最终生成的 Result 对象 RestoreRestore 源码 恢复动作本身的流程 获取到 resource &amp; namespace 的 included &amp; excluded、resources hook 以及 resolvedActions（RestoreItemAction），构建 Restore Context，Context 是用于执行恢复所需要的上下文信息 将 request 中的 BackupReader 解压，并将内容解析成 Backup 的资源信息 如果 Velero 开启了 APIGroupVersions 特性，调用 chooseAPIVersionsToRestore 对多 API Group Versions 的资源选择一个要恢复的版本 生成 update 队列，用于记录 Restore 状态信息，同时启动一个 Goroutine 监听队列，每秒钟获取一次 update 队列中的进度并更新至集群的 Restore 中 首先恢复 CRD 资源，调用 getOrderedResourceCollection 先获取要恢复的 CRD 资源集合，调用 processSelectedResource 开始恢复 接下来调用 getOrderedResourceCollection 获取剩余待恢复资源的有序集合，并调用 processSelectedResource 恢复内置的恢复顺序为：1. customresourcedefinitions2. namespaces3. storageclasses4. volumesnapshotclass.snapshot.storage.k8s.io5. volumesnapshotcontents.snapshot.storage.k8s.io6. volumesnapshots.snapshot.storage.k8s.io7. persistentvolumes8. persistentvolumeclaims9. secrets10. configmaps11. serviceaccounts12. limitranges13. pods14. replicasets.apps15. clusters.cluster.x-k8s.io16. clusterresourcesets.addons.cluster.x-k8s.io 元数据恢复已经完成，更新集群中 Restore 的进度信息 等待 Restic 恢复所有的 Pod 卷数据PodVolumeRestore 的创建位于步骤 6 中 等待 post restore exec hook 执行完毕 getOrderedResourceCollectiongetOrderedResourceCollection 源码 构建要恢复的资源对象的有序集合 构建需要恢复的资源列表，列表中元素为资源的名称，比如 customresourcedefinitions 等 针对每一个资源名称，构建 GroupResource，跳过已经处理的、被 included 和 excluded 排除在外的、类型为 namespace 的 GroupResource 针对该 GroupResource 的每一组资源实例（这组资源实例是以 namespace 做的划分），如果该资源实例的命名空间被排除，则忽略；否则，根据恢复时的命名空间映射关系，获取最终要创建恢复资源的命名空间 如果最终要恢复到的命名空间为空，代表该资源是集群级别的，并且在未指定包含全部命名空间或者指定了特定恢复的命名空间的情况，则忽略 针对每一组资源实例中的具体资源，根据 Backup 的 Content 目录读取该资源文件信息，反序列化成对象结构，最终针对该资源组实例，构建出一个可恢复、待恢复的资源集合 processSelectedResourceprocessSelectedResource 源码 单个 item 资源的恢复流程 针对每一个要恢复的 item 资源，在要恢复到的命名空间不存在的情况下，提前创建 根据 Backup 的 Content 目录读取该资源文件信息，反序列化成对象结构，调用 restoreItem 进行恢复 每恢复完一个对象，则更新一下 update 队列，上报恢复进度 restoreItemrestoreItem 源码 单个 item 资源的恢复流程 传入的 item 是解构状态的资源信息（runtime.Unstructured） 如果最终要恢复到的命名空间不为空，需要根据实际情况提前创建；如果为空，代表该资源是集群级别的，在不恢复集群级别资源时，会忽略掉该资源 以下资源视为“完成”，不会进行恢复 相位为 Succeed 或者 Failed 的 Pod 已经有完成时间的 Job 已经恢复的资源 mirror Pod 如果恢复的资源是 PV 类型，需要考虑以下场景 如果要恢复的 PV 中有快照信息，代表着备份时 SnapshotProvider 执行过快照操作先判断是否需要对其重命名，以下情况不需要重命名 恢复时未指定命名空间映射关系 要恢复的 PV 未被认领 认领该 PV 的命名空间不需要被映射 当前集群中不存在重名 PV如果恢复时，指定了命名空间映射关系，需要根据映射关系，将认领 PV 的命名空间（即 spec.ClaimRef.Namespace）进行更新，如果判断后不需要对 PV 重命名时，则说明该 PV 应该被恢复，反之需要判断该 PV 对象是否应该被恢复，具体规则如下 当前集群中不存在重名 PV，则应该被恢复 重名 PV 处于 Release 状态，等待直至超时 重名 PV 没有被认领，则不应该被恢复 重名 PV 被认领，但是没找到相关 PVC，则等待 PVC 创建直至超时 认领重名 PV 的 PVC 处于删除状态，等待直至超时 认领重名 PV 的 PVC 所在命名空间不存在或者处于删除状态，等待直至超时 默认超时时间为 10 分钟，未超时期间内会周期性判断，一旦超时则表示不应该被恢复如果 PV 应该被恢复，则重置之前的绑定信息，调用 SnapshotProvider 的一系列接口恢复卷数据信息，参考 executePVAction，并根据需要重命名 PV 如果要恢复的 PV 的卷数据是由 Restic 负责备份的（即 PodVolumeBackup annotation 信息中记录了 pv.Spec.ClaimRef.Name），则不会恢复，而是交给 StorageClass 重新动态供应 如果要恢复的 PV 的回收策略为 Delete，则不会恢复，而是交给 StorageClass 重新动态供应 如果并非以上任意场景，则不需要额外的特殊操作，重置绑定信息，进行后续流程直接恢复即可 删除掉不关键的元信息，仅保留 name、namespace、labels 和 annotation，并删除对象 status 信息，执行 RestoreItemAction 的动作 如果恢复的资源是 PVC 类型，则重置之前的和 PV 的绑定信息以及 K8s 设置的 annotation，如果认领的 PV 重命名了，则同步更新 PVC 信息 根据命名空间映射关系，设置 item 的新命名空间，并设置 velero.io/backup-name 和 velero.io/restore-name 标签信息 在当前集群中创建 item 资源，如果 item 资源已经存在（但是却又不深度一致）并且是 ServiceAccount 类型，则会以集群当前的 ServiceAccount 资源为准合并 item 资源并更新，而对于其他类型的资源则认为恢复失败，原因是备份的版本和恢复的版本不一致；而如果已存在的 item 和待恢复的 item 深度一致，也就是两者版本一致，不会恢复，也不会视为错误 如果恢复的资源是 Pod 类型，会针对每一个被 Restic 备份的卷，创建一个 PodVolumeRestore 对象，后续由 PodVolumeRestore Controller 负责维护，同时，主进程会阻塞直到 PodVolumeRestore 有状态返回，表示卷数据恢复完成 如果恢复的资源是 Pod 类型，执行 post restore hook 操作 如果恢复的资源是 CRD 类型，则等待直至 CRD 资源变得可用才继续执行后续恢复流程 PodVolumeRestore Controllerpkg/controller/pod_volume_restore_controller.gopkg/restic/exec_commands.go NewPodVolumeRestoreControllerNewPodVolumeRestoreController 源码 工厂函数 注册 Generic Controller 中的 syncHandler，并将 PodVolumeRestore、Pod 和 PVC 添加到 cacheSyncWaiters，等待同步完成 监听 PodVolumeRestore 资源的 Add 和 Update 事件，根据状态是 New 的 PodVolumeRestore 资源获取到相关联的 Pod，如果 Pod 运行在本节点，并且 Restic Init Container 处于 Running，则将 PodVolumeRestore 以 key（namespace/name） 的形式加入 Generic Controller 的 queue 中restic-wait 处于运行状态表示，该 Pod 正在等待 Restic 为其恢复卷数据 监听 Pod 资源的 Add 和 Update 事件，针对运行在本节点，并且 Restic Init Container 处于 Running，获取到相关联的 PodVolumeRestore 资源，将状态是 New 的 PodVolumeRestore 以 key（namespace/name） 的形式加入 Generic Controller 的 queue 中 processQueueItemprocessQueueItem 源码 注册在 Generic Controller 中 syncHandler 的实现 函数入参就是 Generic Controller 的 queue 中待处理的 PodVolumeRestore key，通过解析获取的 namespace 和 name 查询到集群中的 PodVolumeRestore 对象 调用 processRestore，执行卷数据的备份 processRestoreprocessRestore 源码 卷数据恢复的整体流程 更新 PodVolumeRestore 状态为 InProgress 获取到 PodVolumeRestore 相关联的 Pod，进一步获取到 Pod 内部的挂载卷目录信息 调用 restorePodVolume 执行卷数据的恢复，如果调用结果有错误返回，则设置 PodVolumeRestore 状态为 Failed 更新 PodVolumeRestore 状态为 Completed restorePodVolumerestorePodVolume 源码 借助 Restic 能力恢复卷数据的流程 根据 Pod 内部的挂载卷信息和 Restic Pod 内部的 /host_pods 目录，拼接出到 Pod 挂载数据卷目录最终数据会从 Restic Repo 将卷数据恢复至这个目录，例如 /host_pods/new-pod-uid/volumes/volume-plugin-name/volume-dir 生成用于连接 Restic Repo 所需要的临时密码文件，文件固定为 /tmp/credentials/velero/velero-restic-credentials-repository-password，内容为 static-passw0rd，用于 restic 原生命令中的 –password 参数密码会以 Secret 的形式存储在集群中，名为 velero-restic-credentials，位于 velero 命名空间内 构建 restic restore 命令 如果 BackupStorageLocation 有 caCert 证书信息，会将其临时写入到磁盘中，供 Restic 认证使用，并设置在 restic restore 命令中 给 restic restore 命令设置 Restic 原生所需的环境变量信息 调用 RunRestore，执行 Restic 原生的卷数据恢复流程 为了保险起见，先移除卷目录下的 .velero 目录，然后重新创建，并写入 Restore 的 UID，Restic 的 Init Container 会一致等待直到读取到这个文件的生成 RunRestoreRunRestore 源码 调用 Restic 原生备份命令 restic restore 通过 restic stats 命令获取到总快照的大小，并更新至 PodVolumeRestore 中 将入参的 Command 对象构建成可执行命令并执行 启动一个 Goroutine，每 10 秒钟获取一次已经恢复的数据目录总大小，并更新 PodVolumeRestore 总恢复进度（待恢复总文件大小和当前恢复文件大小） 恢复完成后，更新 PodVolumeRestore 进度至 100%此处未判断恢复成功与否，只是将进度设为 100%，并返回 restic restore 的 stdout，stderr 和 err 信息，由上层调用方判断","link":"/2022/02/04/2022-02-04%20Velero%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20Restore/"},{"title":"「 Velero 」源码走读 — Request","text":"based on v1.6.3 DownloadRequestAPI DownloadTargetKind代表着要从 BackupStorageLocation 中下载的文件，映射关系如下 DownloadTargetKind BackupStorageLocation 中的文件 BackupLog backups/&lt;backup&gt;/&lt;backup&gt;-logs.gz BackupContents backups/&lt;backup&gt;/&lt;backup&gt;.tar.gz BackupVolumeSnapshots backups/&lt;backup&gt;/&lt;backup&gt;-volumesnapshots.json.gz BackupResourceList backups/&lt;backup&gt;/&lt;backup&gt;-resource-list.json.gz RestoreLog restores/&lt;restore&gt;/&lt;restore&gt;-logs.gz RestoreResults restores/&lt;restore&gt;/restore-&lt;restore&gt;-results.gz ServerStatusRequestAPI ServerStatusRequest 不支持通过命令行手动创建，而是在获取 Velero 组件状态时，会自动生成该对象，由 ServerStatusRequest Controller 维护。 DownloadRequest Controllerpkg/controller/download_request_controller.gopkg/cmd/util/downloadrequest/downloadrequest.go ReconcileReconcile 源码 如果 DownloadRequest 状态不为空并且有过期时间的，代表该对象不是已经处理过，则需要进一步判断其是否达到过期时间 如果已过期，则从集群中删除 如果未过期，并且状态为 Processed，则不做处理虽然已经处理过了，但是并没有直接删除是因为有可能 log 文件流还在使用 如果 DownloadRequest 状态是空或者 New 的，调用 StorageProvider 的 GetDownloadURL 接口获取 DownloadURL 信息并设置到 DownloadRequest 中，同时更新其状态为 Processed，重新设置过期时间为 10 分钟如果调用 GetDownloadURL 时出现异常，则需要终止流程，并且重新入队，交给下次流程处理 最终，以上步骤中如果存在流程异常或者对象被合理删除，则不再重新入队，此后流程中，不再处理；否则， StreamStream 源码 上层应用方式，并非 DownloadRequest Controller 逻辑 根据函数入参信息构建 DownloadRequest 对象，下发至集群中创建，后续由 DownloadRequest Controller 负责维护 每 25 毫秒检测一下，直至 DownloadRequest 的 DownloadURL 被设置该动作就是 DownloadRequest Controller 核心工作内容 根据 DownloadURL 信息，构建 HTTP GET 请求，获取到 StorageProvider 中的文件数据，写入签名提供的 io.Writer 中 应用的场景包括 velero backup download velero backup/restore describe velero backup/restore logs ServerStatusRequest Controllerpkg/controller/server_status_request_controller.go ReconcileReconcile 源码 如果 ServerStatusRequest 状态是空或者 New，则更新其状态为 Processed，设置处理时间戳以及 Velero 服务中安装的插件信息 如果 ServerStatusRequest 状态是 Processed，则判断其是否达到过期时间 如果已过期，则从集群中删除 如果未过期，则设置下次入队时间为 5 分钟之后 如果 ServerStatusRequest 状态不满足上述，则不再重新入队，此后流程中，不再处理 GetServerStatusGetServerStatus 源码 上层应用方式，非 ServerStatusRequest Controller 逻辑 根据函数入参信息构建 ServerStatusRequest 对象，下发至集群中创建，后续由 ServerStatusRequest Controller 负责维护 每 250 毫秒检测一下，直至 ServerStatusRequest 的状态为 Processed，返回 ServerStatusRequest 对象信息该动作就是 ServerStatusRequest Controller 核心工作内容 应用的场景包括 velero plugin get","link":"/2022/02/05/2022-02-05%20Velero%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20Request/"},{"title":"「 Velero 」源码走读 — Provider","text":"based on v1.6.3 Storage Providerpkg/persistence/object_store.go StorageProvider 提供了一系列的封装了 ObjectStore Plugin 的接口，用于操作位于 BackupStorageLocation 上数据。 IsValid调用接口 ListCommonPrefixes(&lt;bucket&gt;, &lt;prefix&gt;/, /) 主体逻辑 针对获取到的公共前缀子目录，获取其子目录层级是否为 backups、restores、restic、metadata 和 plugins 之一，如果不是则返回 error，认为 BackupStorageLocation 不可用。 例如，BackupStorageLocation 上的目录层级为 &lt;bucket&gt;/&lt;prefix&gt;/backups/xxx 和 &lt;bucket&gt;/&lt;prefix&gt;/invalid/xxx，调用 ListCommonPrefixes，获取到的公共前缀子目录层级有 prefix/backups 和 prefix/invalid，进一步处理后，获取到的子目录层级为 backups 和 invalid，其中，invalid 不满足 5 个固定的名称之一，因此认为 BackupStorageLocation 不可用。 应用场景 BackupStorageLocation Controller 会通过该接口周期性检查 BackupStorageLocation 是否可用 ListBackups调用接口 ListCommonPrefixes(&lt;bucket&gt;, &lt;prefix&gt;/backups/, /) 主体逻辑 针对获取到的公共前缀子目录，其子目录层级为 Backup 的名称，聚合并返回。 例如，BackupStorageLocation 上的目录层级为 &lt;bucket&gt;/&lt;prefix&gt;/backups/backupA 和 &lt;bucket&gt;/&lt;prefix&gt;/backups/backupB，调用 ListCommonPrefixes，获取到的公共前缀子目录层级有 &lt;prefix&gt;/backups/BackupA 和 &lt;prefix&gt;/backups/BackupB，进一步处理后，获取到的子目录层级为 BackupA 和 BackupB，则返回包含两者的列表。 应用场景 BackupSync Controller 在同步 Backup 时，用于获取 BackupStorageLocation 中的 Backup PutBackup调用接口 PutObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/&lt;backup&gt;.logs.gz, &lt;log&gt;) PutObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/velero-backup.json, &lt;metadata&gt;) PutObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/&lt;backup&gt;.tar.gz, &lt;content&gt;) DeleteObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/velero-backup.json) PutObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/&lt;backup&gt;-podvolumebackups.json.gz, &lt;podvolumebackups&gt;) PutObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/&lt;backup&gt;-volumesnapshots.json.gz, &lt;volumesnapshots&gt;) PutObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/&lt;backup&gt;-resource-list.json.gz, &lt;resource-list&gt;) PutObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/&lt;backup&gt;-csi-volumesnapshots.json.gz, &lt;csi-volumesnapshots&gt;) PutObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/&lt;backup&gt;-csi-volumesnapshotcontents.json.gz, &lt;csi-volumesnapshotcontents&gt;) 主体逻辑 按照以下顺序上传文件 Backup Logs备份过程中，Velero 服务产生的日志 Backup Metadata描述 Backup 本身的信息 Backup Content被备份的资源内容 PodVolumeBackups被备份的 Pod 卷信息，由 Restic 创建并维护 PodVolumeSnapshots被备份的 Pod 卷快照信息，由 SnapshotProvider 创建并维护 Backup ResourcesList被备份的资源清单 CSI VolumeSnapshot被备份的 Pod 卷快照信息，由 CSI 创建并维护 CSI VolumeSnapshotContents被备份的 Pod 卷快照内容信息，由 CSI 创建并维护 第一步日志上传失败时，仅会打印错误日志，而不会中断上传流程，是因为该步骤不影响备份的主体逻辑。此后如果有步骤失败，会影响到 Backup 的状态，并会清空之前的操作，即调用 DeleteObject 删除已上传的数据。 应用场景 Backup Controller 在备份完成时，备份的资源信息上传至 BackupStorageLocation 时 GetBackupMetadata调用接口 GetObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;velero-backup.json) 主体逻辑 获取到 velero-backup.json 文件，读取内容，解析成 Backup 对象格式。 应用场景 BackupSync Controller 在同步 Backup 时，用于构建待同步的 Backup 对象 GetBackupVolumeSnapshots调用接口 ObjectExists(&lt;bucket&gt;, &lt;prefix&gt;/&lt;backups&gt;/&lt;backup&gt;/&lt;backup&gt;-volumesnapshots.json.gz) GetObject(&lt;bucket&gt;, &lt;prefix&gt;/&lt;backups&gt;/&lt;backup&gt;/&lt;backup&gt;-volumesnapshots.json.gz) 主体逻辑 判断 BackupVolumeSnapshots 是否存在，如果存在，则获取并解析返回。 应用场景 Restore Controller 在恢复时，会获取 Backup 相关联的 VolumeSnapshot 并恢复 BackupDeletion Controller 在删除备份时，会一并删除 VolumeSnapshot 信息 GetBackupVolumeBackups调用接口 ObjectExists(&lt;bucket&gt;, &lt;prefix&gt;/&lt;backups&gt;/&lt;backup&gt;/&lt;backup&gt;-podvolumebackups.json.gz) GetObject(&lt;bucket&gt;, &lt;prefix&gt;/&lt;backups&gt;/&lt;backup&gt;/&lt;backup&gt;-podvolumebackups.json.gz) 主体逻辑 判断 PodVolumeBackup 是否存在，如果存在，则获取并解析返回。 应用场景 BackupSync Controller 在处理需要同步的 Backup 时，也会判断是否有相关联的 PodVolumeBackup 需要被同步 GetBackupContents调用接口 GetObject(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/&lt;backup&gt;.tar.gz) 主体逻辑 调用接口，判断指定的备份内容。 应用场景 Restore Controller 在恢复时，会将备份的内容下载到临时文件中，恢复创建资源 BackupDeletion Controller 在删除备份时，如果定义了 action，会将备份的内容下载到临时文件中，获取到 action 定义的动作并执行 GetCSIVolumeSnapshots调用接口 ObjectExists(&lt;bucket&gt;, &lt;prefix&gt;/&lt;backups&gt;/&lt;backup&gt;/&lt;backup&gt;-csi-volumesnapshots.json.gz) GetObject(&lt;bucket&gt;, &lt;prefix&gt;/&lt;backups&gt;/&lt;backup&gt;/&lt;backup&gt;-csi-volumesnapshots.json.gz) 主体逻辑 调用 ObjectExists 判断 VolumeSnapshots 是否存在，如果存在，则获取并解析返回 应用场景 无 GetCSIVolumeSnapshotContents调用接口 ObjectExists(&lt;bucket&gt;, &lt;prefix&gt;/&lt;backups&gt;/&lt;backup&gt;/&lt;backup&gt;-csi-volumesnapshotscontents.json.gz) GetObject(&lt;bucket&gt;, &lt;prefix&gt;/&lt;backups&gt;/&lt;backup&gt;/&lt;backup&gt;-csi-volumesnapshotscontents.json.gz) 主体逻辑 判断 VolumeSnapshotsContents 是否存在，如果存在，则获取并解析返回 应用场景 BackupSync Controller 在获取到需要同步 Backup 时，如果 Velero 开启了 CSI 特性，用于获取 VolumeSnapshotContents 对象，后续也会同步该对象 BackupExists调用接口 ObjectExists(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/velero-backup.json) 主体逻辑 调用接口，判断给定的备份是否存在。 应用场景 Backup Controller 在备份时，会判断本次创建的备份在 BackupStorageLocation 中是否已经存在，如果存在则将备份状态设为 Failed DeleteBackup调用接口 ListObjects(&lt;bucket&gt;, &lt;prefix&gt;/backups/&lt;backup&gt;/) DeleteObject(&lt;bucket&gt;, &lt;key&gt;) 主体逻辑 调用 ListObjects，获取指定备份名称目录下的所有文件，即 key，遍历所有文件，调用 DeleteObject 删除 key。可以看到，Velero 在删除备份时仅会调用 DeleteBackup 删除所有的子文件，也就是所有的 key，但是不会删除这个备份目录，因此如果先要实现删除最后的空目录，需要在 StorageProvider 的 DeleteObject 接口实现。 应用场景 BackupDeletion Controller 删除指定备份时，用于删除 BackupStorageLocation 中的 Backup 数据 PutRestoreLog调用接口 PutObject(&lt;bucket&gt;, &lt;prefix&gt;/restores/&lt;restore&gt;/restore-&lt;restore&gt;-logs.gz, &lt;log&gt;) 主体逻辑 调用接口，上传恢复日志文件。 应用场景 Restore Controller 在恢复完成时，用于上传恢复过程中 Velero 产生的日志 PutRestoreResults调用接口 PutObject(&lt;bucket&gt;, &lt;prefix&gt;/restores/&lt;restore&gt;/restore-&lt;restore&gt;-logs.gz, &lt;result&gt;) 主体逻辑 调用接口，上传恢复结果信息。 应用场景 Restore Controller 在恢复完成时，用于将本次恢复过程中产生的 warning 和 error 信息上传 DeleteRestore调用接口 ListObject(&lt;bucket&gt;, &lt;prefix&gt;/restores/&lt;restore&gt;/) DeleteObject(&lt;bucket&gt;, &lt;key&gt;) 主体逻辑 调用 ListObjects，获取指定恢复名称目录下的所有文件，即 key，遍历所有文件，调用 DeleteObject 删除 key。可以看到，Velero 在删除恢复时仅会调用 DeleteBackup 删除所有的子文件，也就是所有的 key，但是不会删除这个恢复目录，因此如果先要实现删除最后的空目录，需要在 StorageProvider 的 DeleteObject 接口实现。 应用场景 BackupDeletion Controller 在删除 Backup 时，会同步删除相关联的 Restore，并删除 BackupStorageLocation 中的 Restore 信息BackupStorageLocation 中 Restore 的删除仅在该场景下，手动删除 Restore，不会触发 DeleteRestore 流程，也就是通过 velero restore delete xxx 是无法删除 BackupStorageLocation 中 Restore 文件的。 GetDownloadURL调用接口 CreateSignedURL(&lt;bucket&gt;, &lt;target&gt;, 10min)target 表示要根据 DownloadRequest 对象要获取的目标文件，具体参考 DownloadRequest 章节 主体逻辑 根据 DownloadRequest 的对象类别，即 target，构建不同的目标文件路径，调用接口，获取 DownloadURL。 应用场景 DownloadRequest Controller 在处理 DownloadRequest 对象时，会通过该接口构建 DownloadURL，并回写至 DownloadRequest 对象中 Snapshot Providerpkg/backup/item_backupper.gopkg/restore/pv_restorer.go Velero 并未像 Storage Provider 封装一些上层接口，而是将底层接口的调用简单封装了以下两个函数，用于备份和恢复时，对于 PV 类型的资源做的快照和恢复的操作。 takePVSnapshot调用接口 init(snapshotLocation.Spec.Config) GetVolumeID(&lt;Unstructured PV&gt;) GetVolumeInfo(&lt;volumeID&gt;, &lt;volumeZone&gt;) CreateSnapshot(&lt;volumeID&gt;, &lt;volumeZone&gt;, &lt;tags&gt;) 主体逻辑 判断 Backup 的 SnapshotVolumes 是否开启，如果开启则表示需要对卷做快照，继续执行以下逻辑 如果 PV 已经被认领，则需要判断是否被 Restic 已经备份（没有被认领的 PV 肯定不会被 Restic 备份），如果已经备份，则不会再次创建卷快照 通过 PV 的 label 获取 PV 的 zone 信息（topology.kubernetes.io/zone 或者 failure-domain.beta.kubernetes.io/zone） 针对 backup 中每一个 Snapshot Location，初始化一个 volumeSnapshotter，调用 GetVolumeID 尝试获取 VolumeIDPV 卷肯定是由 Snapshot Provider 创建出来的，所以 GetVolumeID 肯定会有记录保存 根据 Backup 的 label 创建 Tag，并追加 velero.io/backup=&lt;backupName&gt; 和 velero.io/pv=&lt;pvName&gt; 调用 GetVolumeInfo 接口，获取到卷信息，包括卷类型和 IOPS 根据以上信息构建 volumeSnapshot 对象，调用 CreateSnapshot 接口，创建快照 更新 volumeSnapshot 状态等信息 executePVAction调用接口 init(snapshotLocation.Spec.Config) CreateVolumeFromSnapshot(&lt;snapshotID&gt;, &lt;volumeType&gt;, &lt;volumeZone&gt;, &lt;volumeIOPS&gt;) SetVolumeID(&lt;pv&gt;, &lt;VolumeID&gt;) 主体逻辑 校验 PV 的合法性，判断名称是否存在 判断是否需要通过快照恢复卷，即 Backup 中是否指定了备份卷（backup.snapshotVolumes）以及 Restore 中是否指定了恢复卷（restore.restorePVs） 根据 PV 名称以及 Restore 相关信息获取 snapshot 对象 如果获取到 Snapshot 对象，则初始化 volumeSnapshotter 根据 snapshot 对象中的卷信息，如卷类型，zone，IOPS 等信息，调用 CreateVolumeFromSnapshot 创建卷，并获取 VolumeID 信息 调用 SetVolumeID，给 PV 设置 VolumeID，并返回 PV 的解构类型","link":"/2022/02/20/2022-02-20%20Velero%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20Provider/"},{"title":"「 Velero 」源码走读 — Server","text":"based on v1.6.3 Configpkg/client/factory.gopkg/client/config.gopkg/cmd/velero/velero.go Velero 的全局配置参数，比如 namespaces，features，cacert 和 colorized 等信息，是在初始化 velero 根 command 时，解析位于 host 的 &lt;HOME&gt;/.config/velero/config.json 的配置文件获取配置对象，初始化 factory 对象，将其透传给下级子 command 实现，由于 velero 服务的启动也是 velero 的子命令（即 velero server），因此实现了全局配置透传功能。 Generic Controllerpkg/controller/generic_controller.gopkg/controller/interface.gointernal/util/managercontroller/managercontroller.go 顾名思义，Generic Controller 定义所有 Controller 的通用行为，本身负责周期性调用 Controller 注册的方法处理 Key，维护 Controller Key 的生命周期。 每一个 Controller 都继承了 Generic Controller，主要包括注册 syncHandler 和 resyncFunc，以及 queue 和 cacheSyncWaiters 等。 Generic Controller 主要包含以下核心属性： queue 默认使用 K8s 提供的 NewNamedRateLimitingQueue，队列中就是需要处理的 Key，格式为 &lt;namespace&gt;/&lt;name&gt; 或者 &lt;name&gt;（取决于对象是否是 namespaced scope）。 Generic Controller 提供了 enqueue 的方法，用于 Key 的入队（本质上就是 queue 的 Add 方法，只不过转换成了上述的格式）。 syncHandler Generic Controller 会周期性的调用 Controller 注册的 syncHandler，处理 queue 中的 Key。 resyncFunc Generic Controller 会根据 resyncPeriod 周期性的调用 Controller 注册的 resyncFunc，执行额外声明的逻辑。 cacheSyncWaiters Generic Controller 在执行 syncHandler 和 resyncFunc 之前会等待注册在 cacheSyncWaiters 全部缓存完成（本质上，就是一组 func() bool 均返回 true 即可，只不过传入的均为 podInformer.HasSynced 函数）。 RunRun 源码 Generic Controller 的核心逻辑 校验 syncHandler 和 resyncFunc 是否至少注册了一个 如果注册了 cacheSyncWaiters，则等待其缓存同步完成PodVolumeBackup Controller 和 PodVolumeRestore Controller 均注册了 cacheSyncWaiters，用于同步 Pod、PVC 以及 PodVolumeBackup（PodVolumeRestore） 启动指定 worker 数量的 Goroutine，每 1 秒钟处理一次以下逻辑该逻辑本身是死循环，只有在 queue 关闭时返回 false，因此隔 1 秒钟还会重新执行 从 queue 中获取 Key（Get） 调用 syncHandler 注册的 Handler，处理 Key 如果处理成功，则在 queue 中移除（Forget） 如果处理失败，则限制速率重新加入 queue 中（AddRateLimited） 每隔 resyncPeriod 执行一次 resyncFunc 逻辑resyncFunc 的处理不一定和 Key 相关，可以执行一些指标上报等操作，例如 Backup Controller 的 resyncFunc 实现 RunableRunable 源码 用于将调用 Run 函数启动 Controller 的方法封装成 manager.Runable 返回，供 manager.Add 以及 manager.Start 使用manager 为 controller-runtime 的 manager Velero Serverpkg/cmd/server/server.go 本质上是 velero cli 的 server 子命令，根据 install 以及更多的自定义参数，启动 Velero 服务。 newServernewServer 源码 工厂函数 设置 client 的 QPS 和 Burst，最终会作用在 Kube Client，Velero Client 和 Dynamic Client 初始化 Kube Client，Velero Client 和 Dynamic Client 初始化 PluginRegistry，发现注册在 /plugins 目录下的所有插件，并调用 velero run-plugins 命令启动插件的 GRPC 服务插件包括 item-action、objectStore 以及 volumeSnapshotter 等 如果 Velero 开启了 CSI 特性，则初始化 CSI Snapshot Client 构建 controller-runtime 的 manager 对象 初始化 CredentialFileStore，用于操作认证文件信息 根据以上内容，构建 server 对象 runrun 源码 server 运行的主体逻辑 如果配置了 profile 地址，则启动 pprof 服务 检查 Velero namespace 是否存在，如果不存在则报错 初始化 DiscoveryHelper，每 5 分钟刷新一次，获取可以备份的对象信息 检查 Velero 服务所需要的 CRD 是否存在，如果不存在则报错 检查 Restic 是否存在，如果不存在则输出 warning 信息，确保 restic 所需要的 secret 存在（即 velero-restic-credentials），初始化 RepositoryManager 调用 runControllers，启动所有的 Controller runControllersrunControllers 源码 启动 controller 以及其他服务 启动 promHttp 服务，对接 Prometheus 初始化 pluginManager，提供与 Velero Plugin 交互的原生接口 初始化 backupStoreGetter，提供操作 Backup 和 Restore 的上层接口即 Provider 章节中的 StorageProvider 按需初始化 CSI Snapshot Lister 和 CSI SnapshotContent Lister 根据以上内容，初始化 Backup Controller、BackupSync Controller、Schedule Controller、GC Controller、BackupDeletion Controller、Restore Controller 以及 ResticRepo Controller，并将这些初步设定为默认开启的 Controller 此外，ServerStatusRequest Controller 和 DownloadRequest Controller 作为服务运行时的状态 Controller，也会作为默认开启该类型的 Controller 与步骤 5 中的 Controller 处理逻辑相同，但是是分开处理和启动的 如果 Velero 服务为 restoreOnly 模式，则禁用 Backup Controller、Schedule Controller、GC Controller 以及 BackupDeletion Controller 将启用的 Controller 和禁用的 Controller 取差集后，即为最终 Velero 服务中启动的 Controller 信息 等待 Velero Client 和 CSI Snapshot Client 同步缓存（waitForCacheSync） 启动 BackupStorageLocation Controller（Reconciler 方式），按需启动 ServerStatusRequest Controller 和 DownloadRequest Controller 启动剩余的 Controller（不包含 Request 类型的 Controller），所有的 Controller 默认均为 1 worker Restic Serverpkg/cmd/cli/restic/server.go 本质上是 velero restic cli 的 server 子命令，根据自定义参数，启动 Restic 服务。 newResticServernewResticServer 源码 初始化 KubeClient 和 Velero Client 初始化 PodInformer，仅获取调度在本节点上的 Pod 构建 controller-runtime 的 manager 对象 根据以上内容，构建 restic server 对象 判断挂载在 restic 服务中 /hosts_pods 目录下所有的 Pod 信息和集群中的所有的 Pod 是否一一对应 runrun 源码 启动 promHttp 服务，对接 Prometheus 初始化 CredentialFileStore，用于操作认证文件信息 根据以上内容，初始化 PodVolumeBackup Controller 和 PodVolumeRestore Controller 启动 PodVolumeBackup Controller 和 PodVolumeRestore Controller，默认均为 1 worker Velero Restic Restore Helpercmd/velero-restic-restore-helper/main.go 本质是 Velero 项目的另一个 binary 执行文件（还有一个是 velero 本身），在恢复 Pod 卷数据时，会给该 Pod 注入一个 InitContainer，该 binary 就是 InitContainer 所用镜像（velero/velero-restic-restore-helper:&lt;velero-version&gt;）中的启动服务。 mainmain 源码 每一个待恢复卷数据的 Pod 的 InitContainer 中运行的服务 命令行接受一个参数，即 Restore 的 UID 启动死循环定时器，每一秒钟检查 InitContainer 的 /restores 目录下每一个子目录的 .velero 目录下是否有所提供的 Restore UID 文件，如果每一个子目录都有该 UID 文件，则认为恢复完成，退出死循环，该 InitContainer 生命周期结束，否则，继续等待，无超时时间/restores 下每一个子目录代表一个待恢复的卷，命名为 Pod 使用的 PVC volumeMount 名称","link":"/2022/03/06/2022-03-06%20Velero%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20Server/"},{"title":"「 Kubebuilder 」快速开始","text":"based on v3.3.0 OverviewKubebuilder 是一个基于 CRD 来构建 Kubernetes API 的框架，可以使用 CRD 来构建 API、Controller 和 Admission Webhook。类似于 Ruby on Rails 和 SpringBoot 之类的 Web 开发框架，Kubebuilder 可以提高速度并降低开发人员管理的复杂性，以便在 Golang 中快速构建和发布 Kubernetes API。它建立在用于构建核心 Kubernetes API 的规范技术的基础之上，以提供减少样板和麻烦的简单抽象。 与 Kubebuilder 类似的项目还有 Operator SDK，两者的区别可以参考 https://github.com/operator-framework/operator-sdk/issues/1758，目前两个社区的在做功能整合。 Where they differ is: Operator SDK also has support for Ansible and Helm operators, which make it easy to write operators without having to learn Go and if you already have experience with Ansible or Helm Operator SDK includes integrations with the Operator Lifecycle Manager (OLM), which is a key component of the Operator Framework that is important to Day 2 cluster operations, like managing a live upgrade of your operator. Operator SDK includes a scorecard subcommand that helps you understand if your operator follows best practices. Operator SDK includes an e2e testing framework that simplifies testing your operator against an actual cluster. Kubebuilder includes an envtest package that allows operator developers to run simple tests with a standalone etcd and apiserver. Kubebuilder scaffolds a Makefile to assist users in operator tasks (build, test, run, code generation, etc.); Operator SDK is currently using built-in subcommands. Each has pros and cons. The SDK team will likely be migrating to a Makefile-based approach in the future. Kubebuilder uses Kustomize to build deployment manifests; Operator SDK uses static files with placeholders. Kubebuilder has recently improved its support for admission and CRD conversion webhooks, which has not yet made it into SDK. Code Generator 是用于实现 Kubernetes 风格 API 类型的 Golang 代码生成器。可以利用该工程自动生成指定K8s 资源的 clientset、informers 和 listers API 接口，本身是位于 Kubernetes 项目中的工具，内部包含了大量的生成器，例如 client-gen，deepcopy-gen，informer-gen，lister-gen 等等。 deepcopy-gen 生成深度拷贝对象方法，例如 DeepCopy，DeepCopyObject，DeepCopyInto 等 client-gen 生成 clientSet 对象，为资源生成标准的操作方法，如 get，list，watch，create，update，patch 和 delete informer-gen 生成 informer 对象，提供事件监听机制，如 AddFunc，UpdateFunc，DeleteFunc lister-gen 生成 lister 对象，为 get 和 list 方法提供只读缓存层 Kubebuilder 与 Code Generator 都可以为 CRD 生成 Kubernetes API 相关代码，从代码生成层面来讲， 两者的区别在于： Kubebuilder 不会生成 informers、listers、clientsets，而 Code Generator 会 Kubebuilder 会生成 Controller、Admission Webhooks，而 Code Generator 不会 Kubebuilder 会生成 manifests yaml，而 Code Generator 不会 Kubebuilder 还带有一些其他便利性设施 Kubebuilder环境要求 go version v1.15+ (kubebuilder v3.0 &lt; v3.1). go version v1.16+ (kubebuilder v3.1 &lt; v3.3). go version v1.17+ (kubebuilder v3.3+). docker version 17.03+. kubectl version v1.11.3+. Access to a Kubernetes v1.11.3+ cluster. 安装12$ curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH)$ chmod +x kubebuilder &amp;&amp; mv kubebuilder /usr/local/bin/ 初始化首先借助 Kubebuilder 初始化一个项目。 123456789101112131415161718192021222324252627282930313233343536$ kubebuilder init --domain huayun.io --repo huayun.io/ake/android-operator --owner AKE --project-name android-operator$ tree.├── config│ ├── default│ │ ├── kustomization.yaml│ │ ├── manager_auth_proxy_patch.yaml│ │ └── manager_config_patch.yaml│ ├── manager│ │ ├── controller_manager_config.yaml│ │ ├── kustomization.yaml│ │ └── manager.yaml│ ├── prometheus│ │ ├── kustomization.yaml│ │ └── monitor.yaml│ └── rbac│ ├── auth_proxy_client_clusterrole.yaml│ ├── auth_proxy_role_binding.yaml│ ├── auth_proxy_role.yaml│ ├── auth_proxy_service.yaml│ ├── kustomization.yaml│ ├── leader_election_role_binding.yaml│ ├── leader_election_role.yaml│ ├── role_binding.yaml│ └── service_account.yaml├── Dockerfile├── go.mod├── go.sum├── hack│ └── boilerplate.go.txt├── main.go├── Makefile└── PROJECT6 directories, 24 files 可选的 flags 包括 名称 含义 –component-config –domain group 的 domain 信息，默认为 my.domain –fetch-deps 确保依赖已下载，默认为 true –license boilerplate.txt 中使用的 license，可选的有 apache2 和 none，默认为 apache2 –owner copyright 中的所有者名称 –project-name 项目名称 –project-version 项目版本，默认为 3 –repo go module 中的 module 名称 –skip-go-version-check 如果指定，则跳过 Golang 版本检查 生成 API设置 Kubebuilder 开启 multigroup，即 api 支持多 group，例如 apps/policy，apps/admission 等。 1$ kubebuilder edit --multigroup=true 可选的 flags 包括 名称 含义 –multigroup 是否启用多 api group 组 生成 CRD API 对象。 12$ kubebuilder create api --group android --version v1 --kind AnFile --controller --resource --namespaced --plural anfiles$ kubebuilder create api --group android --version v1 --kind AnImage --controller --resource --namespaced --plural animages 可选的 flags 包括 名称 含义 –controller 是否不询问默认生成 controller，默认为 true –force 强制生成资源 –group 资源 group 组，例如 storage，events 等，最终会和 domain 一起作为 api group 信息 –kind 资源 kind 信息，即资源名称，如 Pod，Service 等 –make 生成文件后，是否执行一次 make generate，默认为 true –namespaced 是否是命名空间级别的资源 –plural 资源的复数信息 –resource 是否不询问默认生成 resource，默认为 true –version 资源版本信息，如 v1，v1beta1 最终的资源结构为 123456apiVersion: android.huayun.io/v1kind: AnImagemetadata: name: animage-samplespec: # TODO(user): Add fields here make manifest 会在 ./config/crd/bases 下根据 API 声明信息生成 CRD 的基础模板，在 API 变动后需要更新。 生成 webhookTODO 目录结构12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364$ tree.├── apis│ └── android│ └── v1│ ├── anfile_types.go│ ├── animage_types.go│ ├── groupversion_info.go│ └── zz_generated.deepcopy.go├── bin│ └── controller-gen├── config│ ├── crd│ │ ├── kustomization.yaml│ │ ├── kustomizeconfig.yaml│ │ └── patches│ │ ├── cainjection_in_anfiles.yaml│ │ ├── cainjection_in_animages.yaml│ │ ├── webhook_in_anfiles.yaml│ │ └── webhook_in_animages.yaml│ ├── default│ │ ├── kustomization.yaml│ │ ├── manager_auth_proxy_patch.yaml│ │ └── manager_config_patch.yaml│ ├── manager│ │ ├── controller_manager_config.yaml│ │ ├── kustomization.yaml│ │ └── manager.yaml│ ├── prometheus│ │ ├── kustomization.yaml│ │ └── monitor.yaml│ ├── rbac│ │ ├── anfile_editor_role.yaml│ │ ├── anfile_viewer_role.yaml│ │ ├── animage_editor_role.yaml│ │ ├── animage_viewer_role.yaml│ │ ├── auth_proxy_client_clusterrole.yaml│ │ ├── auth_proxy_role_binding.yaml│ │ ├── auth_proxy_role.yaml│ │ ├── auth_proxy_service.yaml│ │ ├── kustomization.yaml│ │ ├── leader_election_role_binding.yaml│ │ ├── leader_election_role.yaml│ │ ├── role_binding.yaml│ │ └── service_account.yaml│ └── samples│ ├── android_v1_anfile.yaml│ └── android_v1_animage.yaml├── controllers│ └── android│ ├── anfile_controller.go│ ├── animage_controller.go│ └── suite_test.go├── Dockerfile├── go.mod├── go.sum├── hack│ └── boilerplate.go.txt├── main.go├── Makefile└── PROJECT15 directories, 44 files MarkersKubebuilder 提供了众多 Markers，支持对 CRD 的校验，生成等操作，具体可以参考官方文档。 Code Generator参考了 Kubernetes，Velero 等开源项目风格，将 apis 目录移至 pkg 层级下。 文件准备doc.go在 pkg/apis/android/v1 目录下创建 doc.go 文件，参考内容如下 12345// +k8s:deepcopy-gen=package// Package v1 is the v1 version of the API.// +groupName=android.huayun.iopackage v1 groupName 的名称为 &lt;group&gt;.&lt;domain&gt; register.go在 pkg/apis/android/v1 目录下创建 register.go 文件，参考内容如下 12345678910111213package v1import ( &quot;k8s.io/apimachinery/pkg/runtime/schema&quot;)// SchemeGroupVersion is group version used to register these objects.var SchemeGroupVersion = GroupVersion// Resource takes an unqualified resource and returns a Group qualified GroupResourcefunc Resource(resource string) schema.GroupResource { return SchemeGroupVersion.WithResource(resource).GroupResource()} &lt;CRD&gt;_types.go以 pkg/apis/android/v1/animage_types.go 为例，新增以下 tag 信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/*Copyright 2022 AKE.Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.*/package v1import ( metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;)// EDIT THIS FILE! THIS IS SCAFFOLDING FOR YOU TO OWN!// NOTE: json tags are required. Any new fields you add must have json tags for the fields to be serialized.// AnImageSpec defines the desired state of AnImagetype AnImageSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster // Important: Run &quot;make&quot; to regenerate code after modifying this file // Foo is an example field of AnImage. Edit animage_types.go to remove/update Foo string `json:&quot;foo,omitempty&quot;`}// AnImageStatus defines the observed state of AnImagetype AnImageStatus struct { // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster // Important: Run &quot;make&quot; to regenerate code after modifying this file}// +genclient// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// +kubebuilder:object:root=true// +kubebuilder:subresource:status// AnImage is the Schema for the animages APItype AnImage struct { metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;` Spec AnImageSpec `json:&quot;spec,omitempty&quot;` Status AnImageStatus `json:&quot;status,omitempty&quot;`}// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// +kubebuilder:object:root=true// AnImageList contains a list of AnImagetype AnImageList struct { metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ListMeta `json:&quot;metadata,omitempty&quot;` Items []AnImage `json:&quot;items&quot;`}func init() { SchemeBuilder.Register(&amp;AnImage{}, &amp;AnImageList{})} 在 AnImage struct 上，新增 +genclient 在 AnImage 和 AnImageList struct 上，新增 +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object tools.go该文件主要用于追踪构建阶段所需的依赖包，而非运行阶段，因此，文件位置、名称和内容等信息根据具体情况而定。 在 hack 目录下创建 tools.go 文件，参考内容如下 123456//go:build tools// +build toolspackage toolsimport _ &quot;k8s.io/code-generator&quot; update-codegen.sh该文件用于调用 Code Generator 的 generate-groups.sh 脚本，生成代码。参考内容如下，逻辑参考脚本注释 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#!/usr/bin/env bash##Copyright 2022 AKE.##Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);#you may not use this file except in compliance with the License.#You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0##Unless required by applicable law or agreed to in writing, software#distributed under the License is distributed on an &quot;AS IS&quot; BASIS,#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.#See the License for the specific language governing permissions and#limitations under the License.set -o errexitset -o nounsetset -o pipefail# this script expects to be run from the root of the android-operator repo.# Corresponding to go mod init &lt;module&gt;.MODULE=huayun.io/ake/android-operator# Corresponding to kubebuilder create api --group &lt;group&gt; --version &lt;version&gt;.GROUP_VERSION=android:v1# Generated output package.OUTPUT_PKG=pkg/generated# API package.APIS_PKG=pkg/apisSCRIPT_ROOT=$(dirname &quot;${BASH_SOURCE[0]}&quot;)/..# Sync k8s.io/code-generator to go.modgo mod tidy # Grab code-generator version from go.mod.CODEGEN_VERSION=$(grep 'k8s.io/code-generator' go.mod | awk '{print $2}')CODEGEN_PKG=$(go env GOPATH)/pkg/mod/k8s.io/code-generator@${CODEGEN_VERSION}# Prepare code-generator.if [[ ! -d ${CODEGEN_PKG} ]]; then echo &quot;${CODEGEN_PKG} is missing. Running 'go mod download'.&quot; go mod downloadfiecho &quot;&gt;&gt; Using ${CODEGEN_PKG}&quot;# code-generator does work with go.mod but makes assumptions about# the project living in `$GOPATH/src`. To work around this and support# any location; create a temporary directory, use this as an output# base, and copy everything back once generated.TEMP_DIR=$(mktemp -d)cleanup() { echo &quot;&gt;&gt; Removing ${TEMP_DIR}&quot; rm -rf &quot;${TEMP_DIR}&quot;}trap &quot;cleanup&quot; EXIT SIGINTecho &quot;&gt;&gt; Temporary output directory ${TEMP_DIR}&quot;chmod +x &quot;${CODEGEN_PKG}&quot;/generate-groups.sh# generate the code with:# --output-base because this script should also be able to run inside the vendor dir of# k8s.io/kubernetes. The output-base is needed for the generators to output into the vendor dir# instead of the $GOPATH directly. For normal projects this can be dropped.#cd &quot;${SCRIPT_ROOT}&quot;&quot;${CODEGEN_PKG}&quot;/generate-groups.sh \\ all \\ ${MODULE}/${OUTPUT_PKG} \\ ${MODULE}/${APIS_PKG} \\ ${GROUP_VERSION} \\ --output-base &quot;${TEMP_DIR}&quot; \\ --go-header-file &quot;${SCRIPT_ROOT}&quot;/hack/boilerplate.go.txt# Copy everything back.cp -a &quot;${TEMP_DIR}/${MODULE}/.&quot; &quot;${SCRIPT_ROOT}/&quot; MakefileMakefile 中新增 .PHONY，集成构建流程。 123.PHONY: update-codegenupdate-codegen: ## Generate code by code-generator bash ./hack/update-codegen.sh 代码生成1$ make update-codegen 文件结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647$ tree pkg/generated/pkg/generated/├── clientset│ └── versioned│ ├── clientset.go│ ├── doc.go│ ├── fake│ │ ├── clientset_generated.go│ │ ├── doc.go│ │ └── register.go│ ├── scheme│ │ ├── doc.go│ │ └── register.go│ └── typed│ └── android│ └── v1│ ├── android_client.go│ ├── anfile.go│ ├── animage.go│ ├── doc.go│ ├── fake│ │ ├── doc.go│ │ ├── fake_android_client.go│ │ ├── fake_anfile.go│ │ └── fake_animage.go│ └── generated_expansion.go├── informers│ └── externalversions│ ├── android│ │ ├── interface.go│ │ └── v1│ │ ├── anfile.go│ │ ├── animage.go│ │ └── interface.go│ ├── factory.go│ ├── generic.go│ └── internalinterfaces│ └── factory_interfaces.go└── listers └── android └── v1 ├── anfile.go ├── animage.go └── expansion_generated.go16 directories, 26 files","link":"/2022/04/13/2022-04-13%20Kubebuilder%20%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/"},{"title":"「 Velero 」源码走读 — Plugin","text":"based on v1.6.3 ObjectStorepkg/plugin/velero/object_store.go Velero 无该内置类型的 plugin，具体参考 velero-plugin-for-aws、velero-plugin-for-microsoft-azure、 velero-plugin-for-gcp。 ObjectStore 包含以下接口 Init 初始化 Object如果接口返回 error，BackupStorageLocation Controller 则无法调用 IsValid 判断 Provider 的可用性 PutObject 上传 key（io.Reader）到 Provider 中 ObjectExists 判断 key 在 Provider 中是否存在 GetObject 获取 key 的内容（io.ReadCloser）Velero 会在调用后负责 Close 该 io 操作，无需 plugin 操作 ListCommonPrefixes 获取满足给定公共前缀的所有 key ListObjects 获取满足给定前缀的所有 key DeleteObject 删除 Provider 中的指定 key仅会删除 key 本身，如果对于文件存储类型的 Provider，需要在 plugin 中实现删除最后一个 key 时，同时删除目录的功能 CreateSignedURL 生成具有过期时间的 pre-signed url，用于获取 Provider 中的文件对于文件存储类型等不具备提供对外服务的 Provider，该接口需要有其他服务支撑 VolumeSnapshotterpkg/plugin/velero/volume_snapshotter.go Velero 无该内置类型的 plugin，具体参考 velero-plugin-for-aws、velero-plugin-for-microsoft-azure、 velero-plugin-for-gcp。 VolumeSnapshotter 包含以下接口 Init 初始化 VolumeSnapshotshotter CreateVolumeFromSnapshot 根据指定的 zone、IOPS 信息从快照恢复卷 Velero 创建卷的动作为同步处理，会一直等待，直至完成或者失败 GetVolumeID 根据指定的 PV 获取 VolumeID SetVolumeID 对指定的 PV 设置 VolumeID GetVolumeInfo 获取 Volume 信息 CreateSnapshot 对指定的卷创建快照Velero 创建 PV 快照的动作为同步处理，会一直等待，直至完成或者失败 DeleteSnapshot 删除快照 DeleteItemActionpkg/plugin/velero/delete_item_action.go Velero 无该内置类型的 plugin。 DeleteItemAction 包含以下接口 AppliesTo 返回应该对哪些资源执行额外操作（通过 Included/Excluded Namespaces/Resources 实现），结果会由 Execute 处理执行额外操作 Execute 根据自定义的逻辑判断是否要对函数入参 Item（即符合 AppliesTo 过滤后的资源对象）在删除 Backup 时，执行一些额外的操作 BackupItemActionpkg/plugin/velero/backup_item_action.go BackupItemAction 包含以下接口 AppliesTo 返回应该对哪些资源执行额外操作（通过 Included/Excluded Namespaces/Resources 实现），该过滤结果会和 Backup 本身的过滤取交集，结果会由 Execute 处理执行额外操作 Execute 根据自定义的逻辑判断是否要对函数入参 Item（即符合 AppliesTo 过滤后的资源对象）做额外操作，函数会返回两个核心内容 更新后的 item，此后的流程会以此 item 为基准 需要额外操作的对象，会加入此后备份流程中执行备份 velero.io/pvpkg/backup/backup_pv_action.go AppliesTo IncludedResources: persistentvolumeclaims Execute PVC item 未更新 获取 PVC 绑定的 PV 作为额外操作的对象返回 velero.io/podpkg/backup/pod_action.go AppliesTo IncludedResources: pods Execute Pod item 未更新 获取 Pod 中使用的 PVC 作为额外操作的对象返回 velero.io/service-accountpkg/backup/service_account_action.go AppliesTo IncludedResources: serviceaccounts Execute Service Account item 未更新 获取 Service Account 关联的 ClusterRole 和 ClusterRoleBinding 作为额外操作的对象返回 velero.io/crd-remap-versionpkg/backup/remap_crd_version_action AppliesTo IncludedResources: customresourcedefinition.apiextensions.k8s.io Execute 如果 CRD 的 apiVersion 为 apiextensions.k8s.io/v1，并且如果是单一版本的 CRD，或者存在位解构的字段或者预留字段时，则将 v1 版本的 CRD item 转换为 v1beta1 版本，并返回参考：https://kubernetes.io/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions 无额外操作的对象 RestoreItemActionpkg/plugin/velero/restore_item_action.go RestoreItemAction 包含以下接口 AppliesTo 返回应该对哪些资源执行额外操作（通过 Included/Excluded Namespaces/Resources 实现），该过滤结果会和 Restore 本身的过滤取交集，结果会由 Execute 处理执行额外操作 Execute 根据自定义的逻辑判断是否要对 Item（即符合 AppliesTo 过滤后的资源对象）做额外操作，函数会返回三个核心内容 更新后的 item，此后的流程会以此 item 为基准 需要额外操作的对象，会加入此后恢复流程中执行恢复 是否跳过恢复的标识 skipRestore velero.io/jobpkg/restore/job_action.go AppliesTo IncludedResources: jobs Execute 删除 Job item 中的 controller-uid 字段（job.Spec.Selector.MatchLabels 以及 job.Spec.Template.ObjectMeta.Labels），并返回 无额外操作的对象 skipRestore 为 false velero.io/podpkg/restore/pod_action.go AppliesTo IncludedResources: pods Execute 清空 Pod item 的 nodeName、priority 和无需保留卷、卷挂载信息，并返回如果 Pod 卷或卷挂载以 pod.Spec.ServiceAccountName + “-token-“ 开头，则无需保留。 无额外操作的对象 skipRestore 为 false velero.io/resticpkg/restore/restic_restore_action.go AppliesTo IncludedResources: pods Execute 获取到 Pod item 中的被 Restic 备份的卷信息，如果有的话，则会根据一系列的配置信息对 Pod item 注入一个 Init Container（restic-wait），并返回 无额外操作的对象 skipRestore 为 false velero.io/init-restore-hookpkg/restore/init_restorehook_pod_action.go AppliesTo IncludedResources: pods Execute 获取 Pod 中有关 init.hook.restore.velero.io/xxx 的 annotation 信息，构建一个 Init Container，如果 Pod 没有相关的 annotation，则获取 Restore 中的 hook 信息，根据 selector 的匹配结果，构建一个 Init Container。以上构建的 Init Container 会追加在 Pod 中，顺序为：restic-wait（如果 Pod 已经存在），hook1，hook2…，追加作为更新后的 Pod item 返回 无额外操作的对象 skipRestore 为 false velero.io/servicepkg/restore/service_action.go AppliesTo IncludedResources: services Execute 如果 ClusterIP 不为 None（也就是说分配了一个 Service IP），则删除 ClusterIP，如果 Restore 没有指定 –preserve-nodeports 参数，则删除 Service 的 NodePort 信息，并返回 无额外操作的对象 skipRestore 为 false velero.io/service-accountpkg/restore/service_account_action.go AppliesTo IncludedResources: serviceaccounts Execute 删除 Service Account 中的运行状态时生成的 &lt;serviceAccountName&gt;-token，并返回 无额外操作的对象 skipRestore 为 false velero.io/add-pvc-from-podpkg/restore/add_pvc_from_pod_action.go AppliesTo IncludedResources: pods Execute Pod item 未更新 获取 Pod 中使用的 PVC 作为额外的操作对象返回 skipRestore 为 false velero.io/add-pv-from-pvcpkg/restore/add_pv_from_pod_action.go AppliesTo IncludedResources: persistentvolumeclaims Execute PVC item 未更新 获取 PVC 绑定的 PV 作为额外的操作对象返回 skipRestore 为 false velero.io/change-storage-classpkg/restore/change_storageclass_action.go AppliesTo IncludedResources: persistentvolumeclaims, persistentvolumes Execute 获取集群中有关 StorageClass 映射关系的唯一的 ConfigMap 信息（即 ConfigMap label 中有 velero.io/plugin-config=”” 和 velero.io/change-storage-class=RestoreItemAction，如果不存在则不做映射关系），根据 PV 或 PVC 中的 StorageClassName 获得映射后的新 StorageClassName，更新 PV 或 PVC，并返回 12345678910apiVersion: v1kind: ConfigMapmetadata: name: change-storage-class namespace: velero labels: velero.io/plugin-config: &quot;&quot; velero.io/change-storage-class: RestoreItemActiondata: ussvd-sc: local-sc 无额外操作的对象 skipRestore 为 false velero.io/role-bindingspkg/restore/rolebinding_action.go AppliesTo IncludedResources: rolebindings Execute 根据 Restore 中的 namespace 映射关系，修改 RoleBinding 的 namespace，并返回 无额外操作的对象 skipRestore 为 false velero.io/cluster-role-bindingspkg/restore/clusterrolebinding_action.go AppliesTo IncludedResources: clusterrolebindings Execute 根据 Restore 中的 namespace 映射关系，修改 ClusterRoleBinding 的 namespace，并返回 无额外操作的对象 skipRestore 为 false velero.io/crd-preserve-fieldspkg/restore/crd_v1_preserve_unknown_fields_action.go AppliesTo IncludedResources: customresourcedefinition.apiextensions.k8s.io Execute 设置版本为 apiextensions.k8s.io/v1 的 CRD 的 PreserveUnknownFields 信息，并返回 无额外操作的对象 skipRestore 为 false velero.io/change-pvc-node-selectorpkg/restore/change_pvc_node_selector.go AppliesTo IncludedResources: persistentvolumeclaims Execute 获取集群中有关 PVC NodeSelector 映射关系的唯一 ConfigMap 信息，根据 PVC annotation 中的 volume.kubernetes.io/selected-node 获得映射后的新 Node，如果存在映射关系，则认为新的 Node 存在；如果不存在映射关系，则判断旧 Node 是否存在，不存在则删除 volume.kubernetes.io/selected-node 信息，存在则设置 PVC annotation nodeSelector 为旧 Node。无论怎样，均更新 PVC，并返回 无额外操作的对象 skipRestore 为 false velero.io/apiservicepkg/restore/apiservice_action.go AppliesTo IncludedResrouces: apiservices LabelSelector: kube-aggregator.kubernetes.io/automanaged Execute apiservices item 未更新 无额外操作的对象 skipRestore 为 true，因为这些 apiservices 由 kube-aggergator 负责维护，无需恢复","link":"/2022/03/20/2022-03-20%20Velero%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20Plugin/"},{"title":"「 Golang 」优雅处理枚举类型","text":"Open Sourcehttps://github.com/vmware-tanzu/velero（Backup and migrate Kubernetes applications and their persistent volumes）。是用于备份与恢复集群资源的工具，支持命令行操作。其中关于命令行传入的枚举参数作了优雅校验。 Samplehttps://github.com/shenxianghong/shenxianghong.github.io/tree/main/elegant-coding/enum Structureenum.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package flagimport ( &quot;github.com/pkg/errors&quot;)// Enum is a Cobra-compatible wrapper for defining a string flag that can be one of a specified set of values.type Enum struct { allowedValues []string value string}// NewEnum returns a new enum flag with the specified list of allowed values, and the specified default value if none is set.func NewEnum(defaultValue string, allowedValues ...string) *Enum { return &amp;Enum{ allowedValues: allowedValues, value: defaultValue, }}// String returns a string representation of the enum flag.func (e *Enum) String() string { return e.value}// Set assigns the provided string to the enum receiver. It returns an error if the string is not an allowed value.func (e *Enum) Set(s string) error { for _, val := range e.allowedValues { if val == s { e.value = s return nil } } return errors.Errorf(&quot;invalid value: %q&quot;, s)}// Type returns a string representation of the Enum type.func (e *Enum) Type() string { // we don't want the help text to display anything regarding // the type because the usage text for the flag should capture // the possible options. return &quot;&quot;}// AllowedValues returns a slice of the flag's valid values.func (e *Enum) AllowedValues() []string { return e.allowedValues} 封装了枚举类型，包含默认值以及允许的枚举值，Enum 结构体实现了 pflag 的 Value 接口（String()，Set()，Type()），在执行 flags.StringSliceVar 等操作时，pflag 会调用 Set 函数设置值，因此 Set 可以用于校验枚举值的合法性。 format_flag.go定义了日志格式的枚举值、默认值以及对外的工厂函数。 12345678910111213141516171819202122232425262728293031323334package main// Format is a string representation of the desired output format for logstype Format stringconst ( FormatText Format = &quot;text&quot; FormatJSON Format = &quot;json&quot; defaultValue Format = FormatText)// FormatFlag is a command-line flag for setting the logrus// log format.type FormatFlag struct { *Enum defaultValue Format}// NewFormatFlag constructs a new log level flag.func NewFormatFlag() *FormatFlag { return &amp;FormatFlag{ Enum: NewEnum( string(defaultValue), string(FormatText), string(FormatJSON), ), defaultValue: defaultValue, }}// Parse returns the flag's value as a Format.func (f *FormatFlag) Parse() Format { return Format(f.String())} level_flags.go原理类似 format_flag.go。 default_logger.go1234567891011121314151617181920212223package mainimport ( &quot;github.com/sirupsen/logrus&quot; &quot;os&quot;)// DefaultLogger returns a Logger with the default properties.// The desired output format is passed as a LogFormat Enum.func DefaultLogger(level logrus.Level, format Format) *logrus.Logger { logger := logrus.New() if format == FormatJSON { logger.Formatter = new(logrus.JSONFormatter) } // Make sure the output is set to stdout so log messages don't show up as errors in cloud log dashboards. logger.Out = os.Stdout logger.Level = level return logger} 根据传入的枚举值，初始化一些上层使用的对象。 main.go上层用法参考。 1234567891011121314151617181920package mainimport ( &quot;fmt&quot; &quot;strings&quot;)func main() { logLevelFlag := LogLevelFlag() formatFlag := NewFormatFlag() fmt.Printf(&quot;The level at which to log. Valid values are %s.\\n&quot;, strings.Join(logLevelFlag.AllowedValues(), &quot;, &quot;)) fmt.Printf(&quot;The format for log output. Valid values are %s.\\n&quot;, strings.Join(formatFlag.AllowedValues(), &quot;, &quot;)) logLevel := logLevelFlag.Parse() format := FormatJSON logger := DefaultLogger(logLevel, format) logger.Infof(&quot;Hello World&quot;)}","link":"/2022/04/21/2022-04-21%20Golang%20%E4%BC%98%E9%9B%85%E5%A4%84%E7%90%86%E6%9E%9A%E4%B8%BE%E7%B1%BB%E5%9E%8B/"},{"title":"「 Istio 」快速开始","text":"based on 1.15.0 简介Istio 是一个开源服务网格，它透明地分层到现有的分布式应用程序上。 Istio 强大的特性提供了一种统一和更有效的方式来保护、连接和监视服务。 Istio 是实现负载平衡、服务到服务身份验证和监视的路径——只需要很少或不需要更改服务代码。它强大的控制平面带来了重要的特点，包括： 使用 TLS 加密、强身份认证和授权的集群内服务到服务的安全通信 自动负载均衡的 HTTP、gRPC、WebSocket，和 TCP 流量 通过丰富的路由规则、重试、故障转移和故障注入对流量行为进行细粒度控制 一个可插入的策略层和配置 API，支持访问控制、速率限制和配额 对集群内的所有流量（包括集群入口和出口）进行自动度量、日志和跟踪 Istio 是为可扩展性而设计的，可以处理不同范围的部署需求。Istio 的控制平面运行在 Kubernetes 上，您可以将部署在该集群中的应用程序添加到您的网格中，将网格扩展到其他集群，甚至连接 VM 或运行在 Kubernetes 之外的其他端点。 安装环境准备Istio 与 Kubernetes 版本映射关系参考：https://istio.io/latest/docs/setup/platform-setup/ Version Currently Supported Release Date End of Life Supported Kubernetes Versions Tested, but not supported master No, development only 1.15 Yes August 31, 2022 ~March 2023 (Expected) 1.22, 1.23, 1.24, 1.25 1.16, 1.17, 1.18, 1.19, 1.20, 1.21 1.14 Yes May 24, 2022 ~January 2023 (Expected) 1.21, 1.22, 1.23, 1.24 1.16, 1.17, 1.18, 1.19, 1.20 1.13 Yes February 11, 2022 ~October 2022 (Expected) 1.20, 1.21, 1.22, 1.23 1.16, 1.17, 1.18, 1.19 1.12 Yes November 18, 2021 ~June 2022 (Expected) 1.19, 1.20, 1.21, 1.22 1.16, 1.17, 1.18 1.11 Yes August 12, 2021 ~Mar 2022 (Expected) 1.18, 1.19, 1.20, 1.21, 1.22 1.16, 1.17 1.10 No May 18, 2021 Jan 7, 2022 1.18, 1.19, 1.20, 1.21 1.16, 1.17, 1.22 1.9 No February 9, 2021 Oct 8, 2021 1.17, 1.18, 1.19, 1.20 1.15, 1.16 1.8 No November 10, 2020 May 12, 2021 1.16, 1.17, 1.18, 1.19 1.15 1.7 No August 21, 2020 Feb 25, 2021 1.16, 1.17, 1.18 1.15 1.6 and earlier No profile 概念参考：https://istio.io/latest/docs/setup/additional-setup/config-profiles/ 123456789$ istioctl profile listIstio configuration profiles: default demo empty external minimal openshift preview profile 描述了 Istio 控制平面与数据平面的配置信息，以下为 Istio 内建的 profile 类别，同时也支持自定义的 profile。 default根据 IstioOperator API 的默认设置启动组件。 建议用于生产部署和 Multicluster Mesh 中的 Primary Cluster demo这一配置具有适度的资源需求，旨在展示 Istio 功能的配置。它适合运行 Bookinfo 应用程序和相关任务 empty什么都不部署。可以作为自定义配置的基本配置文件 external用于配置远程集群由一个外部控制平面或者通过控制平面主要集群多集群网格 minimal与默认配置文件相同，但仅安装控制平面组件。这允许您使用单独的配置文件配置控制平面和数据平面组件（例如网关） openshift preview包含实验性功能。旨在探索 Istio 的新功能。不能保证稳定性、安全性和性能 不同的 profile 包含的核心组件 default demo minimal external empty preview Core components istio-egressgateway ✔ istio-ingressgateway ✔ ✔ ✔ istiod ✔ ✔ ✔ ✔ 展示 profile 具体内容 可以将 profile dump 出来，便于浏览和修改配置信息，可以看到 profile 的本质就是 IstioOperator 资源（详见下文）。 12345$ istioctl profile dump demoapiVersion: install.istio.io/v1alpha1kind: IstioOperatorspec: ... 也可以进一步通过 --config-path 参数（更多配置参考：https://istio.io/latest/docs/reference/config/），选择配置文件中指定路径的局部内容 12345678910$ istioctl profile dump --config-path components.pilot demoenabled: truek8s: env: - name: PILOT_TRACE_SAMPLING value: &quot;100&quot; resources: requests: cpu: 10m memory: 100Mi 展示 profile 之间差别 可以看到 default 和 demo profile 之间的差别之一是，default 不会部署 istio-egressgateway 组件。 12345678910111213141516$ istioctl profile diff default demospec: components: base: enabled: true cni: enabled: false egressGateways:- - enabled: false+ - enabled: true+ k8s:+ resources:+ requests:+ cpu: 10m+ memory: 40Mi name: istio-egressgateway Istio 准备Istio 可以通过 release 下载，也可以通过 Istio 官方提供的 downloadIstio 脚本下载，两者等价 1234$ curl -L https://istio.io/downloadIstio | sh -# 脚本本身也提供了诸多参数，例如指定版本、指定架构下载$ curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.15.0 TARGET_ARCH=x86_64 sh - 下载后包括以下内容，其中 istioctl 位于 bin 目录中，示例位于 samples 目录中 12$ lsbin LICENSE manifests manifest.yaml README.md samples tools 环境预检123$ istioctl x precheck✔ No issues found when checking the cluster. Istio is safe to install or upgrade! To get started, check out https://istio.io/latest/docs/setup/getting-started/ 通过 Istioctl 安装安装默认的 Istio（即 profile 为 default），该 profile 常用于生产环境。 1$ istioctl install 也可以进行一些定制化（–set），例如： 1$ istioctl install --set meshConfig.accessLogFile=/dev/stdout 该效果等价于（-f），使用 Istio Operator API 的方式更适用于生产环境。 1234567$ istioctl install -f - &lt;&lt;EOFapiVersion: install.istio.io/v1alpha1kind: IstioOperatorspec: meshConfig: accessLogFile: /dev/stdoutEOF 为了更好的演示 Istio 的特性，因此使用 demo 的 profile 安装 Istio。 123456$ istioctl install --set profile=demo -y✔ Istio core installed✔ Istiod installed✔ Egress gateways installed✔ Ingress gateways installed✔ Installation complete 通过 Istio Operator 安装类似于 Prometheus Operator，Istio 提供了通过 Operator 安装 Istio 的方式，只需要简单的更新 Istio Operator 声明的 API 对象，便可以非常便捷地处理多版本 Istio。 部署 Istio Operator12345$ istioctl operator initInstalling operator controller in namespace: istio-operator using image: docker.io/istio/operator:1.15.0Operator controller will watch namespaces: istio-system✔ Istio operator installed✔ Installation complete 在部署 Istio Operator 时，支持指定详细规格（更多配置参考：https://istio.io/latest/docs/reference/config/）。例如，在指定命名空间观测资源。 1$ istioctl operator init --watchedNamespaces=istio-namespace1,istio-namespace2 部署 Istio Operator 后会产生以下资源 CRD 12$ kubectl get crd | grep istioistiooperators.install.istio.io 2022-05-14T03:11:20Z Deployment 123$ kubectl get deploy -n istio-operatorNAME READY UP-TO-DATE AVAILABLE AGEistio-operator 1/1 1 1 6m20s Service（用于暴露 Istio Operator 指标） 12345$ kubectl get svc -n istio-operatorNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEistio-operator ClusterIP 10.96.212.28 &lt;none&gt; 8383/TCP 6m47s$ curl 10.96.212.28:8383/metrics RBAC（细节不做展示） 创建 IstioOperator 资源123456789$ kubectl apply -f - &lt;&lt;EOFapiVersion: install.istio.io/v1alpha1kind: IstioOperatormetadata: namespace: istio-system name: example-istiocontrolplanespec: profile: demoEOF Istio Operator watch 到 IstioOperator 资源，然后根据 spec 中声明的详细规格（具体参考：更多配置参考：https://istio.io/latest/docs/reference/config/），安装部署 Istio 服务。 安装内容安装 Istio（profile 为 demo）后会产生以下资源 Deployment 12345$ kubectl get deploy -n istio-systemNAME READY UP-TO-DATE AVAILABLE AGEistio-egressgateway 1/1 1 1 4m14sistio-ingressgateway 1/1 1 1 4m14sistiod 1/1 1 1 4m23s Service 12345$ kubectl get svc -n istio-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEistio-egressgateway ClusterIP 10.96.206.121 &lt;none&gt; 80/TCP,443/TCP 85sistio-ingressgateway LoadBalancer 10.96.159.246 &lt;pending&gt; 15021:31641/TCP,80:31235/TCP,443:30268/TCP,31400:32741/TCP,15443:31149/TCP 85sistiod ClusterIP 10.96.2.143 &lt;none&gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 2m24s CRD 12345678910111213141516$ kubectl get crd | grep istioauthorizationpolicies.security.istio.io 2022-09-13T05:55:57Zdestinationrules.networking.istio.io 2022-09-13T05:55:57Zenvoyfilters.networking.istio.io 2022-09-13T05:55:57Zgateways.networking.istio.io 2022-09-13T05:55:57Zistiooperators.install.istio.io 2022-09-13T05:55:57Zpeerauthentications.security.istio.io 2022-09-13T05:55:57Zproxyconfigs.networking.istio.io 2022-09-13T05:55:57Zrequestauthentications.security.istio.io 2022-09-13T05:55:58Zserviceentries.networking.istio.io 2022-09-13T05:55:58Zsidecars.networking.istio.io 2022-09-13T05:55:58Ztelemetries.telemetry.istio.io 2022-09-13T05:55:58Zvirtualservices.networking.istio.io 2022-09-13T05:55:58Zwasmplugins.extensions.istio.io 2022-09-13T05:55:58Zworkloadentries.networking.istio.io 2022-09-13T05:55:58Zworkloadgroups.networking.istio.io 2022-09-13T05:55:58Z webhook 123456789$ kubectl get mutatingwebhookconfigurationsNAME WEBHOOKS AGEistio-revision-tag-default 4 8m57sistio-sidecar-injector 4 9m19s$ kubectl get validatingwebhookconfigurationsNAME WEBHOOKS AGEistio-validator-istio-system 1 9m27sistiod-default-validator 1 9m4s RBAC，PodDisruptionBudget，ConfigMap，EnvoyFilter（细节不做展示） 也可以查看 IstioOperator 对象来获取安装细节，installed-state 为 istioctl 安装的默认名称。 123$ kubectl get istiooperator -n istio-systemNAME REVISION STATUS AGEinstalled-state 10m 如果 Istio 集群由 Istio Operator 创建，那么 Istio Operator 会维护 IstioOperator 对象。 123$ kubectl get istiooperator -n istio-systemNAMESPACE NAME REVISION STATUS AGEistio-system example-istiocontrolplane HEALTHY 2m59s 此外，也可以生成安装的资源清单，并校验安装是否完整。 1234567$ istioctl manifest generate --set profile=demo | istioctl verify-install -f -...✔ Service: istio-ingressgateway.istio-system checked successfully✔ Service: istiod.istio-system checked successfullyChecked 15 custom resource definitionsChecked 3 Istio Deployments✔ Istio is installed and verified successfully 配置校验123$ istioctl analyze --all-namespaces✔ No validation issues found when analyzing all namespaces. 例如，当 VirtualService 指向的 Gateway 不存在时： 12345$ istioctl analyze --all-namespacesError [IST0101] (VirtualService default/bookinfo) Referenced gateway not found: &quot;bookinfo-gateway-error&quot;Warning [IST0132] (VirtualService default/bookinfo) one or more host [*] defined in VirtualService default/bookinfo not found in Gateway default/bookinfo-gateway-error.Error: Analyzers found issues when analyzing all namespaces.See https://istio.io/v1.15/docs/reference/config/analysis for more information about causes and resolutions. 卸载卸载 Istio Operator12345678$ istioctl operator removeRemoving Istio operator... Removed Deployment:istio-operator:istio-operator. Removed Service:istio-operator:istio-operator. Removed ServiceAccount:istio-operator:istio-operator. Removed ClusterRole::istio-operator. Removed ClusterRoleBinding::istio-operator.✔ Removal complete 卸载 Istio--purge 会移除所有 Istio 资源，包括集群级别的（会对其他共享资源的 Istio 集群造成破坏），更多配置参考：https://istio.io/latest/docs/reference/config/ 1$ istioctl uninstall --purge 也可以通过安装的资源清单，执行卸载操作。 1$ istioctl manifest generate --set profile=demo | kubectl delete -f - 默认情况下，istio-system namespace 不会被删除，需要手动删除。 1$ kubectl delete ns istio-system BookInfo 示例这个示例部署了一个用于演示多种 Istio 特性的应用，该应用由四个单独的微服务构成。这个应用模仿在线书店的一个分类，显示一本书的信息。 页面上会显示一本书的描述，书籍的细节（ISBN、页数等），以及关于这本书的一些评论。 BookInfo 应用分为四个单独的微服务 productpage 会调用 details 和 reviews 两个微服务，用来生成页面 details 包含了书籍的信息 reviews 包含了书籍相关的评论。它还会调用 ratings 微服务 ratings 包含了由书籍评价组成的评级信息 reviews 微服务有 3 个版本 v1 版本不会调用 ratings 服务 v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息 v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息 该应用的端到端架构如下 BookInfo 应用中的几个微服务是由不同的语言编写的。 这些服务对 Istio 并无依赖，但是构成了一个有代表性的服务网格的例子：它由多个服务、多个语言构成，并且 reviews 服务具有多个版本。 所有的微服务都和 Envoy sidecar 集成在一起，被集成服务所有的出入流量都被 sidecar 所劫持，这样就为外部控制准备了所需的 Hook，然后就可以利用 Istio 控制平面为应用提供服务路由、遥测数据收集以及策略实施等功能。 根据名为 istio-ingressgateway 的 Service 的信息，可以明确 http 的访问地址为 http://178.104.162.69:31235。 123$ kubectl get svc -n istio-system istio-ingressgatewayNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEistio-ingressgateway LoadBalancer 10.96.159.246 &lt;pending&gt; 15021:31641/TCP,80:31235/TCP,443:30268/TCP,31400:32741/TCP,15443:31149/TCP 3m34s 注入 SidecarIstio 默认自动注入 Sidecar，目前为 default 命名空间打上标签 istio-injection=enabled。 1$ kubectl label namespace default istio-injection=enabled 部署测试应用123456789101112131415$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yamlservice/details createdserviceaccount/bookinfo-details createddeployment.apps/details-v1 createdservice/ratings createdserviceaccount/bookinfo-ratings createddeployment.apps/ratings-v1 createdservice/reviews createdserviceaccount/bookinfo-reviews createddeployment.apps/reviews-v1 createddeployment.apps/reviews-v2 createddeployment.apps/reviews-v3 createdservice/productpage createdserviceaccount/bookinfo-productpage createddeployment.apps/productpage-v1 created 状态检查，可以看到所有的 Pod 均为 2 容器，其中就包含了 Sidecar 容器。 12345678910111213141516$ kubectl get podNAME READY STATUS RESTARTS AGEdetails-v1-79f774bdb9-m9njc 2/2 Running 2 20mproductpage-v1-6b746f74dc-8xps7 2/2 Running 2 20mratings-v1-b6994bb9-54w28 2/2 Running 2 20mreviews-v1-545db77b95-57wkb 2/2 Running 2 20mreviews-v2-7bf8c9648f-qj7zg 2/2 Running 2 20mreviews-v3-84779c7bbc-fl2mx 2/2 Running 1 20m$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdetails ClusterIP 10.96.15.22 &lt;none&gt; 9080/TCP 13mkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 43dproductpage ClusterIP 10.96.59.210 &lt;none&gt; 9080/TCP 13mratings ClusterIP 10.96.136.207 &lt;none&gt; 9080/TCP 13mreviews ClusterIP 10.96.254.165 &lt;none&gt; 9080/TCP 13m 通过访问可以看到，Bookinfo 服务正在运行。 12$ kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o &quot;&lt;title&gt;.*&lt;/title&gt;&quot;&lt;title&gt;Simple Bookstore App&lt;/title&gt; 创建 Gateway 与默认的 VirtualService 为了保证服务流量可以进入到服务网格，需要创建 Gateway 资源，同时创建一个默认的 VirtualService。 123456789$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml$ kubectl get gatewayNAME AGEbookinfo-gateway 9s$ kubectl get virtualserviceNAME GATEWAYS HOSTS AGEbookinfo [&quot;bookinfo-gateway&quot;] [&quot;*&quot;] 23m 创建的 Gateway，本质上是将入站流量交给满足标签 istio=ingressgateway 的 Pod 的流量接管。 也就是说流量从 istio-ingressgateway 的 Service 80 端口入站，转发到后端的 istio-ingressgateway 的 Pod。创建 Gateway 之后 istio-ingressgateway 会动态绑定 Gateway 中声明的 port 端口，因此需要保证流量可以从路由至该端口（即 istio-ingressgateway Service 的 Endpoint 端口中也同样为 80） 12345678910spec: selector: istio: ingressgateway servers: - hosts: - '*' port: name: http number: 80 protocol: HTTP 创建的 VirtualService，绑定了上面创建的 Gateway，本质上是将匹配到的路由定向到 productpage Service 的 9080 端口。 12345678910111213141516171819202122spec: gateways: - bookinfo-gateway hosts: - '*' http: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 此时访问 http://178.104.162.69:31235/productpage，review 部分在 v1, v2, v3 版本平均切换 其实可以看到 VirtualService 将流量路由到 productpage 的 Service 后，此后的流量的路径还是 K8s 原生的路由方式 创建 DestinationRule12345678$ kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml$ kubectl get destinationruleNAME HOST AGEdetails details 4m41sproductpage productpage 4m42sratings ratings 4m41sreviews reviews 4m41s 此时 BookInfo 的 VirtualService 只是将流量转发到 productpage 上，而 productpage 只有 v1 版本，其他 DestinationRule 只是创建，未被引用，可以理解成 DestinationRule 注册了 productpage 的 v1 版本以及 reviews 的 v1，v2 和 v3 版本，如果后续需要使用，需要创建 VirtualService 路由流量。 123456789101112131415161718192021222324252627apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: productpagespec: host: productpage subsets: - name: v1 labels: version: v1---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: reviewsspec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 由于没有 VirtualService 路由访问 Service 的流量，因此，此时访问页面，页面中 Review 微服务还是会三个版本平均切换的。 创建智能路由的 VirtualService123456789$ kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml$ kubectl get virtualserviceNAME GATEWAYS HOSTS AGEbookinfo [&quot;bookinfo-gateway&quot;] [&quot;*&quot;] 5d22hdetails [&quot;details&quot;] 114sproductpage [&quot;productpage&quot;] 114sratings [&quot;ratings&quot;] 114sreviews [&quot;reviews&quot;] 114s 创建的 VirtualService 的本质上是将访问各组件的所有的流量路由至对应的 v1 版本。 123456789101112131415161718192021222324kind: VirtualServicemetadata: name: productpagespec: hosts: - productpage http: - route: - destination: host: productpage subset: v1---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviewsspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 此时访问页面，页面中的 Review 只会有 v1 版本样式，即没有任何星星。","link":"/2022/05/04/2022-05-04%20Istio%20%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/"},{"title":"「 Istio 」流量管理 — 整体概述","text":"based on 1.15.0 Istio 的流量路由规则可以很容易的控制服务之间的流量和 API 调用。Istio 简化了服务级别属性的配置，比如熔断器、超时和重试，并且能轻松的设置重要的任务，如 A/B 测试、金丝雀发布、基于流量百分比切分的概率发布等。它还提供了开箱即用的故障恢复特性，有助于增强应用的健壮性，从而更好地应对被依赖的服务或网络发生故障的情况。 Istio 的流量管理模型源于和服务一起部署的 Envoy 代理。网格内服务发送和接收的所有流量（data plane 流量）都经由 Envoy 代理，这让控制网格内的流量变得异常简单，而且不需要对服务做任何的更改。 整体概述为了在网格中导流，Istio 需要知道所有的 endpoint 在哪和属于哪个服务。为了定位到 Service Registry（服务注册中心），Istio 会连接到一个服务发现系统。例如，如果在 Kubernetes 集群上安装了 Istio，那么它将自动检测该集群中的服务和 endpoint。 使用此服务注册中心，Envoy 代理可以将流量定向到相关服务。大多数基于微服务的应用程序，每个服务的工作负载都有多个实例来处理流量，称为负载均衡池。默认情况下，Envoy 代理基于轮询调度模型在服务的负载均衡池内分发流量，按顺序将请求发送给池中每个成员，一旦所有服务实例均接收过一次请求后，重新回到第一个池成员。 Istio 基本的服务发现和负载均衡能力提供了一个可用的服务网格，但它能做到的远比这多的多。在许多情况下，可能希望对网格的流量情况进行更细粒度的控制。作为 A/B 测试的一部分，可能想将特定百分比的流量定向到新版本的服务，或者为特定的服务实例子集应用不同的负载均衡策略。可能还想对进出网格的流量应用特殊的规则，或者将网格的外部依赖项添加到服务注册中心。通过使用 Istio 的流量管理 API 将流量配置添加到 Istio，就可以完成所有这些甚至更多的工作。 和其他 Istio 配置一样，这些 API 也使用 Kubernetes CRD 来声明，可以像示例中看到的那样使用 YAML 进行配置。 核心 API 包括：Virtual Services，Destination Rules，Gateways，Service Entries 和 Sidecars。 Virtual ServiceVirtual Service 和 Destination Rule 是 Istio 流量路由功能的关键拼图。Virtual Service 配置如何在服务网格内将请求路由到服务，这基于 Istio 和平台提供的基本的连通性和服务发现能力。每个 Virtual Service 包含一组路由规则，Istio 按顺序评估它们，Istio 将每个给定的请求匹配到 Virtual Service 指定的实际目标地址。网格可以有多个 Virtual Service，也可以没有，取决于使用场景。 为什么要使用 Virtual ServiceVirtual Service 在增强 Istio 流量管理的灵活性和有效性方面，发挥着至关重要的作用，通过对客户端请求的目标地址与真实响应请求的目标工作负载进行解耦来实现。Virtual Service 同时提供了丰富的方式，为发送至这些工作负载的流量指定不同的路由规则。 如果没有 Virtual Service，Envoy 会在所有的服务实例中使用轮询的负载均衡策略分发请求，而有些场景下有着不同的分发需求。例如，有些可能代表不同的版本（A/B 测试），希望在其中配置基于不同服务版本的流量百分比路由，或者路由内部用户到特定实例集的流量。 使用 Virtual Service，可以为一个或多个主机名指定流量行为。在 Virtual Service 中使用路由规则，告诉 Envoy 如何发送 Virtual Service 的流量到适当的目标。路由目标地址可以是同一服务的不同版本，也可以是完全不同的服务。 一个典型的用例是将流量发送到指定服务子集的不同版本。客户端将 Virtual Service 视为一个单一实体，将请求发送至 Virtual Service 主机，然后 Envoy 根据 Virtual Service 规则把流量路由到不同的版本。例如，将 20% 的调用转到新版本或者将这些用户的调用转到版本 2。例如，创建一个金丝雀发布，逐步增加发送到新版本服务的流量百分比。流量路由完全独立于实例部署，这意味着实现新版本服务的实例可以根据流量的负载来伸缩，完全不影响流量路由。 Virtual Service 可以 通过单个 Virtual Service 处理多个应用程序服务。如果网格使用 Kubernetes，可以配置一个 Virtual Service 处理特定命名空间中的所有服务。映射单一的 Virtual Service 到多个真实 Service，可以在不需要客户适应转换的情况下，将单体应用转换为微服务构建的复合应用系统 和 Gateway 整合并配置流量规则来控制出入流量 在某些情况下，还需要配置 Destination Rule（用于指定服务子集）来使用这些特性。在一个单独的对象中指定服务子集和其它特定目标策略，有利于在 Virtual Service 之间更简洁地重用这些规则。 Virtual Service 示例下面的 Virtual Service 根据请求是否来自特定的用户，把它们路由到服务的不同版本。 1234567891011121314151617181920apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviewsspec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v3 hosts使用 hosts 字段列举 Virtual Service 的主机——即用户指定的目标或是路由规则设定的目标。这是客户端向服务发送请求时使用的一个或多个地址。 12hosts:- reviews Virtual Service 主机名可以是 IP 地址、DNS 名称，或者依赖于平台的一个简称（例如 Kubernetes 服务的短名称），隐式或显式地指向一个完全限定域名（FQDN）。也可以使用通配符（“*”）前缀，创建一组匹配所有服务的路由规则。Virtual Service 的 hosts 字段实际上不必是 Istio 服务注册的一部分，它只是虚拟的目标地址。可以为没有路由到网格内部的虚拟主机建模。 路由规则在 http 字段包含了 Virtual Service 的路由规则，用来描述匹配条件和路由行为，它们把 HTTP/1.1、HTTP2 和 gRPC 等流量发送到 hosts 字段指定的目标（也可以用 tcp 和 tls 为 TCP 和未终止的 TLS 流量设置路由规则）。一个路由规则包含了指定的请求要流向哪个目标地址，具有 0 或多个匹配条件，取决于使用场景。 匹配条件示例中的第一个路由规则有一个条件，因此以 match 字段开始。在本例中，希望此路由应用于来自 jason 用户的所有请求，所以使用 headers、end-user 和 exact 字段匹配相关请求。 1234- match: - headers: end-user: exact: jason 目的地route 部分的 destination 字段指定了符合此条件的流量的实际目标地址。与 Virtual Service 的 hosts 不同，destination 的 host 必须是存在于 Istio 服务注册中心的实际目标地址，否则 Envoy 不知道该将请求发送到哪里。可以是一个有代理的服务网格，或者是一个通过 Service Entry 被添加进来的非网格服务。 1234route:- destination: host: reviews subset: v2 在示例中，为了简单，使用 Kubernetes 的短名称设置 destination 的 host。在评估此规则时，Istio 会添加一个基于 Virtual Service 命名空间的域后缀，这个 Virtual Service 包含要获取主机的完全限定名的路由规则。在示例中使用短名称也意味着可以复制并在任何命名空间中尝试它们。 只有在目标主机和 Virtual Service 位于相同的 Kubernetes 命名空间时才可以使用这样的短名称。因为使用 Kubernetes 的短名称容易导致配置出错，建议在生产环境中指定完全限定的主机名 destination 还指定了 Kubernetes 服务的子集，将符合此规则条件的请求转入其中。在本例中子集名称是 v2。 路由规则优先级路由规则按从上到下的顺序选择，Virtual Service 中定义的第一条规则有最高优先级。本示例中，不满足第一个路由规则的流量均流向一个默认的目标（即 v2 子集），该目标在第二条规则中指定。因此，第二条规则没有 match 条件，直接将流量导向 v3 子集。 1234- route: - destination: host: reviews subset: v3 建议提供一个默认的”无条件”或基于权重的规则作为每一个 Virtual Service 的最后一条规则，从而确保流经 Virtual Service 的流量至少能够匹配一条路由规则。 路由规则的更多内容路由规则是将特定流量子集路由到指定目标地址的强大工具。可以在流量端口、header 字段、URI 等内容上设置匹配条件。例如，这个 Virtual Service 让用户发送请求到两个独立的服务：ratings 和 reviews。Virtual Service 规则根据请求的 URI 和指向适当服务的请求匹配流量。 12345678910111213141516171819202122232425262728apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: bookinfospec: hosts: - bookinfo.com http: - match: - uri: prefix: /reviews route: - destination: host: reviews - match: - uri: prefix: /ratings route: - destination: host: ratings... http: - match: sourceLabels: app: reviews route:... 有些匹配条件可以使用精确的值，如前缀或正则。 可以使用 AND 向同一个 match 块添加多个匹配条件，或者使用 OR 向同一个规则添加多个 match 块。对于任何给定的 Virtual Service 也可以有多个路由规则。 另外，使用匹配条件可以按百分比”权重“分发请求。这在 A/B 测试和金丝雀发布中非常有用。 12345678910111213spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 75 - destination: host: reviews subset: v2 weight: 25 也可以使用路由规则在流量上执行一些操作，例如： 添加或删除 header 重写 URL 为调用这一目标地址的请求设置重试策略 Destination Rule与 Virtual Service 一样，Destination Rule 也是 Istio 流量路由功能的关键部分。可以将 Virtual Service 视为将流量如何路由到给定目标地址，然后使用 Destination Rule 来配置该目标的流量。在评估 Virtual Service 路由规则之后，Destination Rule 将应用于流量的真实目标地址。 可以使用 Destination Rule 来指定命名的服务子集，例如按版本为所有给定服务的实例分组。然后可以在 Virtual Service 的路由规则中使用这些服务子集来控制到服务不同实例的流量。 Destination Rule 还允许在调用整个目的地服务或特定服务子集时定制 Envoy 的流量策略，比如负载均衡模型、TLS 安全模式或熔断器设置。 负载均衡选项默认情况下，Istio 使用轮询的负载均衡策略，实例池中的每个实例依次获取请求。Istio 同时支持如下的负载均衡模型，可以在 DestinationRule 中为流向某个特定服务或服务子集的流量指定这些模型。 Random：请求以随机的方式转到池中的实例 Weighted：请求根据指定的百分比转到实例 Least requests：请求被转到最少被访问的实例 Destination Rule 示例下面的示例为 my-svc 目标服务配置了 3 个具有不同负载均衡策略的子集： 12345678910111213141516171819202122apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: my-destination-rulespec: host: my-svc trafficPolicy: loadBalancer: simple: RANDOM subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 trafficPolicy: loadBalancer: simple: ROUND_ROBIN - name: v3 labels: version: v3 每个子集都是基于一个或多个 labels 定义的，在 Kubernetes 中它是附加到像 Pod 这种对象上的键/值对。这些标签应用于 Kubernetes 服务的 Deployment 并作为 metadata 来识别不同的版本。 除了定义子集之外，Destination Rule 对于所有子集都有默认的流量策略，而对于该子集，则有特定于子集的策略覆盖它。定义在 subsets 上的默认策略，为 v1 和 v3 子集设置了一个简单的随机负载均衡器。为 v2 子集设置了一个轮询负载均衡器。 Gateway使用 Gateway 为网格来管理入站和出站流量，可以指定要进入或离开网格的流量。网关配置被用于运行在网格边界的独立 Envoy 代理，而不是服务工作负载的 sidecar 代理。 与 Kubernetes Ingress API 这种控制进入系统流量的其他机制不同，Istio Gateway 充分利用流量路由的强大能力和灵活性。可以这么做的原因是 Istio 的 Gateway 资源可以配置 4-6 层的负载均衡属性，如对外暴露的端口、TLS 设置等。作为替代应用层流量路由（L7）到相同的 API 资源，绑定了一个常规的 Istio Virtual Service 到 Gateway 。可以像管理网格中其他数据平面的流量一样去管理网关流量。 Gateway 主要用于管理进入的流量，但也可以配置出口 Gateway 。出口 Gateway 为离开网格的流量配置一个专用的出口节点，这可以限制哪些服务可以或应该访问外部网络，或者启用出口流量安全控制为网格添加安全性。也可以使用 Gateway 配置一个纯粹的内部代理。 Istio 提供了一些预先配置好的 Gateway 代理服务（istio-ingressgateway 和 istio-egressgateway）。 Gateway 示例下面的示例展示了一个外部 HTTPS 入口流量的网关配置： 123456789101112131415161718apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: ext-host-gwyspec: selector: app: my-gateway-controller servers: - port: number: 443 name: https protocol: HTTPS hosts: - ext-host.example.com tls: mode: SIMPLE serverCertificate: /tmp/tls.crt privateKey: /tmp/tls.key 这个 Gateway 配置让 HTTPS 流量从 ext-host.example.com 通过 443 端口流入网格，但没有为请求指定任何路由规则。 为想要生效的 Gateway 指定路由，必须把 Gateway 绑定到 Virtual Service 上。然后就可以为出口流量配置带有路由规则的 Virtual Service。 123456789apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: virtual-svcspec: hosts: - ext-host.example.com gateways: - ext-host-gwy Service Entry使用 Service Entry 来添加一个入口到 Istio 内部维护的服务注册中心。添加了 Service Entry 后，Envoy 代理可以向服务发送流量，就好像它是网格内部的服务一样。配置 Service Entry 允许管理运行在网格外的服务的流量，它包括以下几种能力： 为外部目标 redirect 和转发请求，例如来自 web 端的 API 调用，或者流向遗留老系统的服务 为外部目标定义重试、超时和故障注入策略 添加一个运行在虚拟机的服务来扩展您的网格 不需要为网格服务要使用的每个外部服务都添加 Service Entry。默认情况下，Istio 配置 Envoy 代理将请求传递给未知服务。但是，不能使用 Istio 的特性来控制没有在网格中注册的目标流量。 Service Entry 示例下面的 Service Entry 将外部服务 ext-svc.example.com 添加到 Istio 的服务注册中心： 12345678910111213apiVersion: networking.istio.io/v1alpha3kind: ServiceEntrymetadata: name: svc-entryspec: hosts: - ext-svc.example.com ports: - number: 443 name: https protocol: HTTPS location: MESH_EXTERNAL resolution: DNS 指定的外部资源使用 hosts 字段。可以使用完全限定名或通配符作为前缀域名。 可以配置 Virtual Service 和 Destination Rule，以更细粒度的方式控制到 Service Entry 的流量，这与网格中的任何其他服务配置流量的方式相同。 例如，下面的 Destination Rule 调整了使用 Service Entry 配置的 ext-svc.example.com 外部服务的连接超时： 12345678910apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: ext-res-drspec: host: ext-svc.example.com trafficPolicy: connectionPool: tcp: connectTimeout: 1s Sidecar默认情况下，Istio 让每个 Envoy 代理都可以访问来自和它关联的工作负载的所有端口的请求，然后转发到对应的工作负载。可以使用 Sidecar 配置去做下面的事情： 微调 Envoy 代理接受的端口和协议集 限制 Envoy 代理可以访问的服务集合 可能希望在较庞大的应用程序中限制这样的 Sidecar 可达性，配置每个代理能访问网格中的任意服务可能会因为高内存使用量而影响网格的性能。 可以指定将 Sidecar 配置应用于特定命名空间中的所有工作负载，或者使用 workloadSelector 选择特定的工作负载。 例如，下面的 Sidecar 配置将 bookinfo 命名空间中的所有服务配置为仅能访问运行在相同命名空间和 istio-system 服务： 12345678910apiVersion: networking.istio.io/v1alpha3kind: Sidecarmetadata: name: default namespace: bookinfospec: egress: - hosts: - &quot;./*&quot; - &quot;istio-system/*&quot; 网络弹性和测试除了为网格导流之外，Istio 还提供了可选的故障恢复和故障注入功能，可以在运行时动态配置这些功能。使用这些特性可以让应用程序运行稳定，确保服务网格能够容忍故障节点，并防止局部故障级联影响到其他节点。 超时超时是 Envoy 代理等待来自给定服务的答复的时间量，以确保服务不会因为等待答复而无限期的挂起，并在可预测的时间范围内调用成功或失败。HTTP 请求的默认超时时间是 15 秒，这意味着如果服务在 15 秒内没有响应，调用将失败。 对于某些应用程序和服务，Istio 的缺省超时可能不合适。例如，超时太长可能会由于等待失败服务的回复而导致过度的延迟；而超时过短则可能在等待涉及多个服务返回的操作时触发不必要地失败。为了找到并使用最佳超时设置，Istio 允许使用 Virtual Service 按服务轻松地动态调整超时，而不必修改业务代码。 例如，下面的 Virtual Service 配置对 ratings 服务的 v1 子集的调用指定 10 秒超时： 12345678910111213apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: ratingsspec: hosts: - ratings http: - route: - destination: host: ratings subset: v1 timeout: 10s 重试重试设置指定如果初始调用失败，Envoy 代理尝试连接服务的最大次数。通过确保调用不会因为临时过载的服务或网络等问题而永久失败，重试可以提高服务可用性和应用程序的性能。重试之间的间隔（25ms+）是可变的，并由 Istio 自动确定，从而防止被调用服务被请求淹没。HTTP 请求的默认重试行为是在返回错误之前重试两次。 与超时一样，Istio 默认的重试行为在延迟方面可能不适合应用程序需求（对失败的服务进行过多的重试会降低速度）或可用性。可以在 Virtual Service 中按服务调整重试设置，而不必修改业务代码。还可以通过添加每次重试的超时来进一步细化重试行为，并指定每次重试都试图成功连接到服务所等待的时间量。 例如，下面的 Virtual Service 配置在初始调用失败后最多重试 3 次来连接到服务子集，每个重试都有 2 秒的超时： 123456789101112131415apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: ratingsspec: hosts: - ratings http: - route: - destination: host: ratings subset: v1 retries: attempts: 3 perTryTimeout: 2s 熔断器熔断器是 Istio 为创建具有弹性的微服务应用提供的另一个有用的机制。在熔断器中，设置一个对服务中的单个主机调用的限制，例如并发连接的数量或对该主机调用失败的次数。一旦限制被触发，熔断器就会“跳闸”并停止连接到该主机。使用熔断模式可以快速失败而不必让客户端尝试连接到过载或有故障的主机。 熔断适用于在负载均衡池中的真实网格目标地址，可以在 Destination Rule 中配置熔断器阈值，让配置适用于服务中的每个主机。 例如，下面的 Destination Rule 配置将 v1 子集的 reviews 服务工作负载的并发连接数限制为 100： 1234567891011121314apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: reviewsspec: host: reviews subsets: - name: v1 labels: version: v1 trafficPolicy: connectionPool: tcp: maxConnections: 100 故障注入在配置了网络，包括故障恢复策略之后，可以使用 Istio 的故障注入机制来为整个应用程序测试故障恢复能力。故障注入是一种将错误引入系统以确保系统能够承受并从错误条件中恢复的测试方法。使用故障注入特别有用，能确保故障恢复策略不至于不兼容或者太严格，这会导致关键服务不可用。 目前，故障注入配置不能与同一个 Virtual Service 上的重试或超时配置相结合 与其他错误注入机制（如延迟数据包或在网络层杀掉 Pod）不同，Istio 允许在应用层注入错误。这可以注入更多相关的故障，例如 HTTP 错误码，以获得更多相关的结果。 可以注入两种故障，它们都使用 Virtual Service 配置： 延迟：延迟是时间故障。它们模拟增加的网络延迟或一个超载的上游服务 终止：终止是崩溃失败。它们模仿上游服务的失败。终止通常以 HTTP 错误码或 TCP 连接失败的形式出现 例如，下面的 Virtual Service 示例为千分之一的访问 ratings 服务的请求配置了一个 5 秒的延迟： 1234567891011121314151617apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: ratingsspec: hosts: - ratings http: - fault: delay: percentage: value: 0.1 fixedDelay: 5s route: - destination: host: ratings subset: v1 与应用程序协同工作Istio 故障恢复功能对应用程序来说是完全透明的。在返回响应之前，应用程序不知道 Envoy Sidecar 代理是否正在处理被调用服务的故障。这意味着，如果在应用程序代码中设置了故障恢复策略，那么需要记住这两个策略都是独立工作的，否则会发生冲突。例如，假设设置了两个超时，一个在 Virtual Service 中配置，另一个在应用程序中配置。应用程序为服务的 API 调用设置了 2 秒超时。而在 Virtual Service 中配置了一个 3 秒超时和重试。在这种情况下，应用程序的超时会先生效，因此 Envoy 的超时和重试尝试会失效。 虽然 Istio 故障恢复特性提高了网格中服务的可靠性和可用性，但应用程序必须处理故障或错误并采取适当的回退操作。例如，当负载均衡中的所有实例都失败时，Envoy 返回一个 HTTP 503 状态码。应用程序必须实现回退逻辑来处理 HTTP 503 错误代码。","link":"/2022/07/04/2022-07-04%20Istio%20%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86%20-%20%E6%95%B4%E4%BD%93%E6%A6%82%E8%BF%B0/"},{"title":"「 Istio 」流量管理 — Gateway","text":"based on 1.15.0 Gateway 描述了在网格边缘运行的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。该规范描述了一组应该公开的端口、要使用的协议类型、负载均衡器的 SNI 配置等。 例如，以下 Gateway 配置设置代理以充当负载均衡器，将端口 80 和 9080 (http)、443 (https)、9443 (https) 和端口 2379 (TCP) 用于入口。Gateway 会应用在带有标签 app: my-gateway-controller 的 Pod 上。虽然 Istio 配置代理侦听这些端口，但用户有责任确保允许到这些端口的外部流量进入网格。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950apiVersion: networking.istio.io/v1beta1kind: Gatewaymetadata: name: my-gateway namespace: some-config-namespacespec: selector: app: my-gateway-controller servers: - port: number: 80 name: http protocol: HTTP hosts: - uk.bookinfo.com - eu.bookinfo.com tls: httpsRedirect: true # sends 301 redirect for http requests - port: number: 443 name: https-443 protocol: HTTPS hosts: - uk.bookinfo.com - eu.bookinfo.com tls: mode: SIMPLE # enables HTTPS on this port serverCertificate: /etc/certs/servercert.pem privateKey: /etc/certs/privatekey.pem - port: number: 9443 name: https-9443 protocol: HTTPS hosts: - &quot;bookinfo-namespace/*.bookinfo.com&quot; tls: mode: SIMPLE # enables HTTPS on this port credentialName: bookinfo-secret # fetches certs from Kubernetes secret - port: number: 9080 name: http-wildcard protocol: HTTP hosts: - &quot;*&quot; - port: number: 2379 # to expose internal service via external port 2379 name: mongo protocol: MONGO hosts: - &quot;*&quot; Gateway 描述了负载均衡器的 L4 - L6 属性。然后，可以将 VirtualService 绑定到 Gateway 控制到达特定 host 或 Gateway 端口的流量的转发。 例如，下面的 VirtualService 把流量路径 https://uk.bookinfo.com/reviews、https://eu.bookinfo.com/reviews、http://uk.bookinfo.com:9080/reviews、http://eu.bookinfo.com:9080/reviews 分为了两个版本（prod 和 qa）。另外，包含 user: dev-123 cookie 的请求将发送到 7777 端口的 qa 版本。The same rule is also applicable inside the mesh for requests to the “reviews.prod.svc.cluster.local” service. This rule is applicable across ports 443, 9080. Note that http://uk.bookinfo.com gets redirected to https://uk.bookinfo.com (i.e. 80 redirects to 443). 1234567891011121314151617181920212223242526272829303132333435apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: bookinfo-rule namespace: bookinfo-namespacespec: hosts: - reviews.prod.svc.cluster.local - uk.bookinfo.com - eu.bookinfo.com gateways: - some-config-namespace/my-gateway - mesh # applies to all the sidecars in the mesh http: - match: - headers: cookie: exact: &quot;user=dev-123&quot; route: - destination: port: number: 7777 host: reviews.qa.svc.cluster.local - match: - uri: prefix: /reviews/ route: - destination: port: number: 9080 # can be omitted if it's the only port for reviews host: reviews.prod.svc.cluster.local weight: 80 - destination: host: reviews.qa.svc.cluster.local weight: 20 以下 VirtualService 将到达（外部）端口 27017 的流量转发到端口 5555 上的内部 Mongo 服务器。此规则在网格内部不适用，因为 gateways 中省略了保留名称网格（mesh）。 123456789101112131415161718apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: bookinfo-mongo namespace: bookinfo-namespacespec: hosts: - mongosvr.prod.svc.cluster.local # name of internal Mongo service gateways: - some-config-namespace/my-gateway # can omit the namespace if gateway is in same namespace as virtual service. tcp: - match: - port: 27017 route: - destination: host: mongo.prod.svc.cluster.local port: number: 5555 可以使用 hosts 字段中的 namespace/host 语法来限制可以绑定到 Gateway 服务器的 VirtualService。例如，下面的 Gateway 允许 ns1 命名空间中的任何 VirtualService 绑定到它，同时限制只有 ns2 命名空间中具有 foo.bar.com host 的 VirtualService 绑定。 12345678910111213141516apiVersion: networking.istio.io/v1beta1kind: Gatewaymetadata: name: my-gateway namespace: some-config-namespacespec: selector: app: my-gateway-controller servers: - port: number: 80 name: http protocol: HTTP hosts: - &quot;ns1/*&quot; - &quot;ns2/foo.bar.com&quot; GatewayGateway 描述了在网格边缘运行的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。 Field Description servers Server 集合 selector 一个或多个标签，用以匹配一组特定 pod/VM 上应用此 Gateway。默认情况下，工作负载会根据标签选择器在所有命名空间中进行搜索。这意味着 Namespace foo 中的 Gateway 资源可以根据标签选择 Namespace bar 中的 Pod。这种行为可以通过 Istiod 中的 PILOT_SCOPE_GATEWAY_TO_NAMESPACE 环境变量来控制。如果此变量设置为 true，则标签搜索的范围仅限于 Gateway 资源所在的 Namespace。换言之，Gateway 资源必须与 Gateway 工作负载实例位于相同的 Namespace 中。如果选择器为 nil，则 Gateway 将应用于所有工作负载 ServerServer 描述特定负载均衡端口上的代理属性。 Field Description port 代理监听的请求链接端口信息 bind 绑定到的 IP 或 Unix 域套接字。格式为 x.x.x.x、unix:///path/to/uds 或 unix://@foobar（Linux 抽象命名空间）。使用 Unix 域套接字时，端口号应为 0。用于将此 server 的可达性限制为仅限 Gateway 内部。这通常在 Gateway 需要与另一个网格 server 通信时使用，例如发布指标。在这种情况下，使用指定绑定创建的 server 将不可用于外部 Gateway 客户端 hosts Gateway 暴露的 host 信息。虽然通常适用于 HTTP 服务，但它也表述带有 SNI 的 TLS 的 TCP 服务。host 为带有 &lt;namespace&gt;/ 可选前缀的 dnsName（FQDN 格式，也可以类似如 prod/*.example.com）。将 dnsName 设置为 * 表示从指定的命名空间（例如 prod/*）中选择所有 VirtualService host。namespace 可以设置为 * 或 .，分别代表任何或当前命名空间。例如，*/foo.example.com 从任何可用的命名空间中选择 server，而 ./foo.example.com 仅从 sidecar 的命名空间中选择服务。如果未指定 namespace/ 前缀部分，则默认为 */。所选命名空间中的任何关联 DestinationRule 也将被使用。VirtualService 必须绑定到 Gateway，并且必须有一个或多个与 server 中匹配的 host。匹配可以是完全匹配或后缀匹配。例如，如果 server 的 host 指定 .example.com，则具有 host dev.example.com 或 prod.example.com 的 VirtualService 将匹配。但是，host example.com 或 newexample.com 的 VirtualService 将不匹配 tls 一组控制 server 行为的 TLS 相关选项。使用这些选项来控制是否应将所有 HTTP 请求重定向到 HTTPS，以及要使用的 TLS 模式 name server 的可选名称，设置后在所有 server 中必须是唯一的。可用于例如为使用此名称生成的统计信息添加前缀等 PortPort 描述了 server 的特定端口的属性。 Field Description number 端口号 protocol 端口协议。可选值 HTTP、HTTPS、GRPC、HTTP2、MONGO、TCP、TLS。TLS 意味着连接将根据 SNI 标头路由到目的地，而不会终止 TLS 连接 name 分配给端口的标签 targetPort 接收流量的 endpoint 上的端口号。仅在与 ServiceEntries 一起使用时适用 ServerTLSSettings Field Description httpsRedirect 如果设置为 true，负载均衡器将为所有 HTTP 连接发送 301 重定向，要求客户端使用 HTTPS mode 可选：表明是否应使用 TLS 保护与此端口的连接。该字段的值决定了 TLS 的实施方式 serverCertificate 如果 mode 是 SIMPLE 或 MUTUAL，则为必需字段。保存使用的服务器端 TLS 证书的文件的路径 privateKey 如果 mode 是 SIMPLE 或 MUTUAL，则为必需字段。保存服务器私钥的文件的路径 caCertificates 如果 mode 是 MUTUAL，则为必需字段。包含证书颁发机构证书的文件的路径，用于验证提供的客户端证书 credentialName 对于在 Kubernetes 上运行的网关，包含 TLS 证书（包括 CA 证书）的密钥的名称。仅适用于 Kubernetes。密钥（通用类型）应包含以下键和值：键：&lt;privateKey&gt; 和证书：&lt;serverCert&gt;。对于双向 TLS，cacert: &lt;CACertificate&gt; 可以在同一个密钥或名为 &lt;secret&gt;-cacert 的单独密钥中提供。还支持用于服务器证书的 tls 类型的密钥以及用于 CA 证书的 ca.crt 密钥。只能指定服务器证书和 CA 证书或 credentialName 之一 subjectAltNames 用于验证客户端提供的证书中的主体身份的备用名称列表 verifyCertificateSpki 授权客户端证书的 SKPI 的 base64 编码 SHA-256 哈希的可选列表。注意：当同时指定 verify_certificate_hash 和 verify_certificate_spki 时，匹配任一值的哈希将导致证书被接受 verifyCertificateHash 授权客户端证书的十六进制编码 SHA-256 哈希的可选列表。简单格式和冒号分隔格式都可以接受。注意：当同时指定 verify_certificate_hash 和 verify_certificate_spki 时，匹配任一值的哈希将导致证书被接受 minProtocolVersion 最低 TLS 协议版本 maxProtocolVersion 最高 TLS 协议版本 cipherSuites 如果指定，只支持指定的密码列表。否则默认为 Envoy 支持的默认密码列表 ServerTLSSettings.TLSmode Name Description PASSTHROUGH 客户端提供的 SNI 字符串将用作 VirtualService TLS 路由中的匹配标准，以确定服务注册表中的目标服务 SIMPLE 使用标准 TLS 语义的安全连接 MUTUAL 通过提供服务器证书进行身份验证，使用双向 TLS 保护与下游的连接 AUTO_PASSTHROUGH 与直通模式类似，除了具有此 TLS 模式的服务器不需要关联的 VirtualService 从 SNI 值映射到注册表中的服务。诸如服务/子集/端口之类的目标详细信息在 SNI 值中进行编码。代理将转发到 SNI 值指定的上游（Envoy）集群（一组端点）。此服务器通常用于在不同的 L3 网络中提供服务之间的连接，否则它们各自的端点之间没有直接连接。使用此模式假定源和目标都使用 Istio mTLS 来保护流量 ISTIO_MUTUAL 通过提供服务器证书进行身份验证，使用双向 TLS 保护来自下游的连接。与 Mutual 模式相比，该模式使用 Istio 自动生成的代表网关工作负载身份的证书进行 mTLS 身份验证。使用此模式时，TLSOptions 中的所有其他字段都应为空 ServerTLSSettings.TLSProtocol Name Description TLS_AUTO 自动选择最佳 TLS 版本 TLSV1_0 TLS 1.0 版本 TLSV1_1 TLS 1.1 版本 TLSV1_2 TLS 1.2 版本 TLSV1_3 TLS 1.3 版本","link":"/2022/07/10/2022-07-10%20Istio%20%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86%20-%20Gateway/"},{"title":"「 Kubernetes 」源码走读 - CPU Manager","text":"based on v1.20.12 statepkg/kubelet/cm/cpumanager/state/state_checkpoint.go 基于内存，用于记录 CPU Manager 的状态，后续 Policy 获取 CPU 以及分配情况等均从 state 对象中获得 12345678910// map[pod]map[container]CPU Settype ContainerCPUAssignments map[string]map[string]cpuset.CPUSettype stateMemory struct { sync.RWMutex // 记录 CPU 分配情况 assignments ContainerCPUAssignments // 记录 CPU 信息 defaultCPUSet cpuset.CPUSet} storeState当有 CPU 分配或者回收时，会调用该函数，将 state 对象持久化为 checkpoint 文件 restoreState当 CPU Manager 启动时，会调用该函数，将节点上的 checkpoint 文件恢复为 state 对象 checkpoint基于文件（位于 host 上的 /var/lib/kubelet/cpu_manager_state），用于记录 CPU Manager 的状态，为了避免 state 对象在 Kubelet 重启后内存丢失的问题 Policypkg/kubelet/cm/cpumanager/policy.go 1234567891011121314type Policy interface { // 返回策略名称 Name() string // 针对 State 的校验流程 Start(s state.State) error // 容器 CPU 的分配流程 Allocate(s state.State, pod *v1.Pod, container *v1.Container) error // 容器移除后的回收流程 RemoveContainer(s state.State, podUID string, containerName string) error // 调用 Policy 中的 topologymanager.HintProvider 的实现 GetTopologyHints(s state.State, pod *v1.Pod, container *v1.Container) map[string][]topologymanager.TopologyHint // 调用 Policy 中的 topologymanager.HintProvider 的实现 GetPodTopologyHints(s state.State, pod *v1.Pod) map[string][]topologymanager.TopologyHint} nonepkg/kubelet/cm/cpumanager/policy_none.go 默认策略。 CPU Manager 的 none 策略并未做任何实际的逻辑处理，不提供任何系统调度器默认行为之外的亲和性策略。通过 CFS 配额来实现 Guaranteed Pods 和 Burstable Pods 的 CPU 使用限制。因此，共享池的 CPU 也会包含 Kubelet 预留的部分。 staticpkg/kubelet/cm/cpumanager/policy_static.go 仅针对 QoS 为 Guaranteed 且 CPU 申请量为正整数的 Pod 赋予增强的 CPU 亲和性和独占性。 Start 初始化可分配的 CPU 信息，即获取所有可分配的 CPU（如果开启了 strictReserved，则取全量 CPU 和预留 CPU 的差集） 如果开启了 strictReserved，则校验全量 CPU 和预留 CPU 是否没有重叠；如果未开启，则校验预留 CPU 是否全在全量 CPU 中 校验已分配的 CPU 和可分配的 CPU 是否不重叠 校验可分配的 CPU + 已分配的 CPU 是否等于所有的 CPU - 预留的 CPU（如果开启了 strictReserved，否则忽视预留的 CPU） Allocate 判断 Pod QoS 是否是 Guaranteed 级别，并且 Container 的 CPU request 为整数，如果不满足条件，直接返回，不做处理 从已分配的 CPU 信息中判断该 Pod 是否已经分配过，如果分配过，则本地更新 调用 Topology Manager 获取所有的 hint providers 返回的 hint 获取可申领的 CPU（即可分配 CPU + 步骤二中可复用的 CPU） 如果开启了 NUMA 亲和特性，则获取到涉及到的 NUMA 中的所有 CPU，取 NUMA CPU 之和和申请 CPU 中的最小值作为待对齐分配的 CPU 数量，校验申请的 CPU 数量是否大于 1 且小于所有可用的 CPU， 执行拓扑感知 best-fit 算法，优先对齐能满足 NUMA 的部分参考 pkg/kubelet/cm/cpumanager/cpu_assignment_test.go 单元测试示例 如果请求的 CPU 数量不小于单块 CPU Socket 中 Thread 数量，那么会优先将整块 CPU Socket 中的Thread 分配acc.freeSockets()，返回单 Socket 中所有 Thread 均可用的 Socket 列表 如果剩余请求的 CPU 数量不小于单块物理 CPU Core 提供的 Thread 数量，那么会优先将整块物理 CPU Core 上的 Thread 分配acc.freeCores()，返回单 Core 中所有 Thread 均可用的 Core 列表，按照 SocketID 做升序排列 剩余请求的 CPU 数量则从按照如下规则排好序的 Thread 列表中选择acc.freeCPUs()，返回所有可用的 Thread 列表，按照 SocketID 和 CoreID 做升序排列 相同 Socket 上可用的 Thread 相同 Core 上可用的 Thread CPU ID 升序排列 对于剩余的 CPU，进行如上拓扑感知 best-fit 算法，合并以上两部分，作为最终的 CPU 绑定结果 从共享池 CPU 中去除待分配的 CPU RemoveContainer 获取到容器的 CPU 分配信息，删除掉分配的记录信息，共享池 CPU 中添加 CPU GetTopologyHints 获取容器申请的 CPU 数量 如果容器已经分配了申请 CPU，那么判断申请的和已分配的是否相等，不相等则不给出 hint，直接返回；相等则用已分配的生成 hint 获取可用的 CPU 和可复用的 CPU 信息，两者的合集作为可复用的 CPU，生成 hint GetPodTopologyHints 获取 Pod 申请的 CPU 数量（获取 Init Container 最大值和 Container 之和，取两者最大为 CPU 数量） 遍历 Pod 的每个容器，如果容器已经分配了申请 CPU，那么判断申请的和已分配的是否相等，不相等则不给出 hint，直接返回；如果所有容器的已分配的 CPU 之和等于 Pod 的申请 CPU 数量，则用已分配的生成 hint 获取可用的 CPU 和可复用的 CPU 信息，两者的合集作为可复用的 CPU，生成 hint generateCPUTopologyHints生成 CPU TopologyHint 信息，假设有两个 NUMA 节点（编号为 0 和 1），NUMA0 上有 CPU1 和 CPU2，NUMA1上有 CPU3 和 CPU4，某个 Pod 请求两个 CPU。那么 CPU Manager 这个 HintProvider 会调用 generateCPUTopologyHints 产生如下的 TopologyHint： {01: True} 代表从 NUMA0 取 2 个 CPU，并且是“优先考虑的” {10: True} 代表从 NUMA1 取 2 个 CPU，并且是“优先考虑的” {11: False} 代表从 NUMA0 和 NUMA1 各取一个 CPU，不是“优先考虑的” 获取集群中的所有 NUMA 节点 获取 NUMA 节点组合中涉及到的 CPU 如果 NUMA 节点组合中所涉及到的 CPU 个数比请求的 CPU 数大，并且这个组合所涉及的 NUMA 节点个数是目前为止所有组合中最小的，那么就更新步骤 1 的获取结果 循环统计当前节点可用的 CPU 中，有哪些是属于当前正在处理的 NUMA 节点组合 如果当前 NUMA 组合中可用的 CPU 数比请求的 CPU 小，那么就直接返回，否则就创建一个 TopologyHint，并把它加入到 hints 中 遍历每一个 hint，涉及到的 NUMA 节点个数最少（即步骤 3 中获取的结果）的组合，会标注 preferred 为 true CPU Managerpkg/kubelet/cm/cpumanager/cpu_manager.go 1234567891011121314151617181920212223242526type Manager interface { // Kubelet 初始化时调用，启动 CPU Manager Start(activePods ActivePodsFunc, sourcesReady config.SourcesReady, podStatusProvider status.PodStatusProvider, containerRuntime runtimeService, initialContainers containermap.ContainerMap) error // 将 CPU 分配给容器，必须在 AddContainer() 之前的某个时间点调用，例如在 Pod Admission 时 Allocate(pod *v1.Pod, container *v1.Container) error // 在容器创建和容器启动之间调用，以便可以将初始 CPU 亲和性设置写入第一个进程开始执行之前的容器运行时中 AddContainer(p *v1.Pod, c *v1.Container, containerID string) error // 在 Kubelet 决定杀死或删除一个对象后调用，在此调用之后，CPU Manager 停止尝试协调该容器并且释放绑定于该容器的任何 CPU // 目前未发现调用处 RemoveContainer(containerID string) error // 返回内部 CPU Manager 的状态 State() state.Reader // 调用 topologymanager.HintProvider 的实现，处理 NUMA 资源对齐等逻辑 GetTopologyHints(*v1.Pod, *v1.Container) map[string][]topologymanager.TopologyHint // 获取分配给 Pod 容器的 CPU 信息 GetCPUs(podUID, containerName string) []int64 // 调用 topologymanager.HintProvider 的实现，处理 NUMA 资源对齐等逻辑 GetPodTopologyHints(pod *v1.Pod) map[string][]topologymanager.TopologyHint} Start 初始化 checkpoint 文件，并基于该文件初始化 state 对象 调用 Policy 的 Start 接口，传入 state 如果策略是 none，则直接返回，否则启动 goroutine 定时调和 state Allocate 清理搁浅的资源，也就是获取 state 中记录的 CPU 信息，但是实际上使用的容器已经不是 active 状态 调用 Policy 的 Allocate 接口 AddContainer 从 state 中获取 Pod 容器的 CPU 信息，如果为空直接返回 调用 CRI 的 UpdateContainerResources 接口，更新容器的 CPU Set 信息，如果更新失败调用 Policy 的 RemoveContainer 接口回滚状态，从 containerMap 中移除容器信息 RemoveContainer 调用 Policy 的 RemoveContainer 接口 从 containerMap 中移除 Container 信息 State 返回 state 对象 GetTopologyHints 清理搁浅的资源，也就是获取 state 中记录的 CPU 信息，但是实际上使用的容器已经不是 active 状态 调用 Policy 的 GetTopologyHints 接口 GetCPUs 获取 state 中记录有给定 Pod 和容器的 CPU 分配情况，并返回 GetPodTopologyHints 清理搁浅的资源，也就是获取 state 中记录的 CPU 信息，但是实际上使用的容器已经不是 active 状态 调用 Policy 的 GetPodTopologyHints 接口 reconcileState针对非 none 类型的 Policy 周期性调和 清理搁浅的资源，也就是获取 state 中记录的 CPU 信息，但是实际上使用的容器已经不是 active 状态 遍历所有的 active 状态的 Pod 的容器 检查该 ContainerID 是否在 CPU Manager 维护的 state 中，然后检查对应的 Pod.Status.Phase 是否为 Running 且 DeletionTimestamp 为 nil，如果是，则调用 CPU Manager 的 AddContainer 对该 Container/Pod 进行 QoS 和 CPU request 检查，如果满足 static Policy 的条件，则调用 takeByTopology 为该 Container 分配最佳的 CPU Set，并写入到 state 和 checkpoint 文件中 然后从 State 中获取该 ContainerID 对应的 CPU Set，调用 CRI UpdateContainerResources 接口更新容器的 CPU Set 信息","link":"/2022/07/11/2022-07-11%20Kubernetes%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20CPU%20Manager/"},{"title":"「 Kata Containers 」Block Volume 直通","text":"based on 2.4.3 背景Kubernetes 设计之初对基于 VM 的容器运行时考虑较少，很多场景都默认容器运行时能直接访问宿主机资源，这也使得 Kata Containers 在和 Kubernetes 集成时对某些 Kubernetes 特性的支持存在一些不足或限制，尤其是在存储方面。 Kubernetes 提供了 PV （persistent volume）资源来管理存储卷，制定了 CSI （Container Storage Interface）规范在存储提供者和容器运行时之间来管理存储设备。通常来说，CSI 会将不同类型的存储设备，比如云盘、本地存储、网络文件系统等，以文件系统的方式挂载到宿主机，然后再从宿主机将此文件系统挂载到容器中。在 Kata Containers 中，这个挂载是通过 virtiofs 协议，在宿主机和 guest OS 中实现了该存储卷的文件共享。虽然 virtiofs 在性能上比之前的 9p 有很大提升，但是和直接在宿主机上使用相比，性能损耗成为在生产环境中使用 Kata Containers 的阻碍因素之一。 其次，使用 Kata Containers 在线调整 PV 的大小是很困难的。虽然 PV 可以在 host 上扩展，但更新后的元数据需要传递到 guest OS 中，以便应用程序容器使用扩展的卷。目前，没有办法在不重新启动 Pod sandbox 的情况下将 PV 元数据从 host OS 传递到 guest OS。 一个理想的长期解决方案是 Kubelet 协调 CSI Driver 和 Container Runtime 之间的通信，如 KEP-2857 讨论，但是目前而言，KEP 仍在审查中，并且提议的解决方案有两个弊端： 将 csiPlugin.json 文件写入卷的根路径会带来安全隐患。恶意用户可以将自己的 csiPlugin.json 写入上述位置，从而获得对块设备的未经授权的访问 提案中并没有描述如何在卷和 Kata Containers 中如何建立映射关系，然而这是 CSI 调整卷大小和信息所需的必备 API 对此 Kata Containers 社区提出一个短期/中期的解决方案 — Block Volume 直通。 当前 CSI 挂载方式 CSI 与 Runtime 协调挂载 现阶段缺陷 一个块设备卷一次只能由一个节点上的一个 Pod 使用，其实这是 Kata Containers 用例中最常见的模式。将同一个块设备连接到多个 Kata Pod 也是不安全的。在 Kubernetes 中，需要将 PersistentVolumeClaim (PVC) 的 accessMode 设置为 ReadWriteOncePod 不支持更高级的 Kubernetes 卷功能，例如 fsGroup、fsGroupChangePolicy 和 subPath 实现方案传统 CSI 都会将存储设备挂载到宿主机上，在 Kata Containers 中，由于 VM 的存在，挂载操作需要移动到 guest 中，由 Kata agent 来完成存储卷的挂载。如下所示： 原挂载方案 直通挂载方案 因此，需要 CSI 具备直通卷的挂载能力，Kata Containers 社区提供了一些参考方案 StorageClass 参数中指定直通卷的相关标识，这样可以免去 CSI 查询 PVC 或者 Pod 的信息，但是基于该 StorageClass 供应的 PV 均会视为直通卷 PVC annotation 中注明，需要 CSI Plugin 支持 –extra-create-metadata RuntimeClass 中注明，CSI Driver 在 node publish 阶段通过 Runtime 来获得 Volume 是否需要直接挂载到 guest 中，参考阿里云实现 当 CSI Driver 并不会直接将直通卷挂载给 Kata Containers 使用，而是需要在 CSI 的不同阶段调用 Kata Containers 在 2.4 新增的 direct-volume 命令向 Kata Containers 运行时传递并收集卷信息。 NodePublishVolume调用 kata-runtime direct-volume add –volume-path [volumePath] –mount-info [mountInfo] 将卷挂载信息传递到 Kata Containers 用来执行文件系统挂载操作。 volumePath 是 CSI NodePublishVolumeRequest 中的 target_path。 mountInfo 是一个序列化的 JSON 字符串 NodeGetVolumeStats调用 kata-runtime direct-volume stats –volume-path [volumePath] 获取直通卷的信息 NodeExpandVolume调用 kata-runtime direct-volume resize –volume-path [volumePath] –size [size] 请求 Kata Containers 调整直通卷的大小 NodeStageVolume/NodeUnStageVolume调用 kata-runtime direct-volume remove –volume-path [volumePath] 删除直通卷的元数据信息 源码分析kata-runtime direct-volumesrc/runtime/cmd/kata-runtime/kata-volume.go Kata Containers 在 2.4 版本时，新增了 kata-runtime direct-volume 的命令，用于管理 Kata Containers 所使用的直通卷。 12345678910111213// MountInfo contains the information needed by Kata to consume a host block device and mount it as a filesystem inside the guest VM.type MountInfo struct { // The type of the volume (ie. block) VolumeType string `json:&quot;volume-type&quot;` // The device backing the volume. Device string `json:&quot;device&quot;` // The filesystem type to be mounted on the volume. FsType string `json:&quot;fstype&quot;` // Additional metadata to pass to the agent regarding this volume. Metadata map[string]string `json:&quot;metadata,omitempty&quot;` // Additional mount options. Options []string `json:&quot;options,omitempty&quot;`} add1$ kata-runtime direct-volume add 可选的 flags 包括 名称 含义 –volume-path 待操作的目标卷路径 –mount-info 管理卷挂载的详情信息 主体流程 校验合法性，创建 /run/kata-containers/shared/direct-volumes/&lt;base64 volume path&gt; 目录 将 mount info 序列化为 json，以名为 mountInfo.json 的文件形式保存在该目录下 remove1$ kata-runtime direct-volume remove 可选的 flags 包括 名称 含义 –volume-path 待操作的目标卷路径 主体流程 移除 /run/kata-containers/shared/direct-volumes/&lt;base64 volume path&gt; 目录 stats1$ kata-runtime direct-volume stats 可选的 flags 包括 名称 含义 –volume-path 待操作的目标卷路径 主体流程 获取 /run/kata-containers/shared/direct-volumes/&lt;base64 volume path&gt; 目录下的 sandbox id 名称预期是一个直通卷仅有一个相关联的 sanbox，因此，该目录下，仅有两个文件，一个名为 sandbox id，一个名为 mountInfo.json 解析 /run/kata-containers/shared/direct-volumes/&lt;base64 volume path&gt;/mountInfo.json 文件，构建 mountInfo 对象，获取 volume 的源 device 信息 向 containerd-shim-kata-v2 的 /direct-volume/stats 接口发起 HTTP Get 请求 判断 host 上的该 device 是否存在，根据 sandbox 中的 container 卷挂载的映射信息，获取到位于 guest OS 中的对应的挂载点 向 Kata agent 发起 rpc 请求，获取 guest OS 中的卷信息 resize1$ kata-runtime direct-volume resize 可选的 flags 包括 名称 含义 –volume-path 待操作的目标卷路径 –size 调整后的卷大小，默认为 0 主体流程 获取 /run/kata-containers/shared/direct-volumes/&lt;base64 volume path&gt; 目录下的 sandbox id 名称预期是一个直通卷仅有一个相关联的 sandbox，因此，该目录下，仅有两个文件，一个名为 sandbox id，一个名为 mountInfo.json 解析 /run/kata-containers/shared/direct-volumes/&lt;base64 volume path&gt;/mountInfo.json 文件，构建 mountInfo 对象 向 containerd-shim-kata-v2 的 /direct-volume/resize 接口发起 HTTP Post 请求 判断 host 上的该 device 是否存在，根据 sandbox 中的 container 卷挂载的映射信息，获取到位于 guest OS 中的对应的挂载点 向 Kata agent 发起 rpc 请求，对 guest OS 中指定的卷进行大小调整 containerd-shim-kata-v2createBlockDevicessrc/runtime/virtcontainers/container.go 部分流程 如果不支持块设备的挂载特性，则直接返回错误信息 针对每一个 container spec 中记录的挂载点 mount，进行以下操作 判断是否已经有 BlockDeviceID 信息，如果有代表已经有设备关联挂载点，后续不需要为其创建新的设备 判断 mount 类型是否是 bind 根据 mount source 获取到 mount info 信息，如果获取不到，则不是直通卷 在 /run/kata-containers/shared/direct-volumes/&lt;base64 volume path&gt; 目录下，写入以 sandbox ID 为名的文件，用于后续 CSI 与 runtime 通信 根据 mount info 信息，设置 container mount 的所需信息 …… 1234567891011121314151617181920212223242526272829303132333435363738// Mount describes a container mount.// nolint: govettype Mount struct { // Source is the source of the mount. Source string // Destination is the destination of the mount (within the container). Destination string // Type specifies the type of filesystem to mount. Type string // HostPath used to store host side bind mount path HostPath string // GuestDeviceMount represents the path within the VM that the device // is mounted. Only relevant for block devices. This is tracked in the event // runtime wants to query the agent for mount stats. GuestDeviceMount string // BlockDeviceID represents block device that is attached to the // VM in case this mount is a block device file or a directory // backed by a block device. BlockDeviceID string // Options list all the mount options of the filesystem. Options []string // ReadOnly specifies if the mount should be read only or not ReadOnly bool // FSGroup a group ID that the group ownership of the files for the mounted volume // will need to be changed when set. FSGroup *int // FSGroupChangePolicy specifies the policy that will be used when applying // group id ownership change for a volume. FSGroupChangePolicy volume.FSGroupChangePolicy} 实践操作准备操作12345678910111213141516171819# 准备两个 runtime 为 Kata Containers 的 Pod，分别为 direct 和 Virtiofs 的模式$ kubectl get podNAME READY STATUS RESTARTS AGElocal-kata-direct 1/1 Running 0 2m7slocal-kata-virt 1/1 Running 0 2m3s$ crictl pods --no-truncPOD ID CREATED STATE NAME NAMESPACE ATTEMPT3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9 11 minutes ago Ready local-kata-virt default 03763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7 11 minutes ago Ready local-kata-direct default 0$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGElocal-kata-direct Bound pvc-23dc3ecf-37fb-44ee-b8a6-7667a98aeb05 20Mi RWO local-kata-direct 20mlocal-kata-virt Bound pvc-0878818c-5d93-4ed4-b034-5bf37a657a6e 20Mi RWO local-kata-virt 20m# 分别在持久卷种写入测试数据$ kubectl exec -it local-kata-direct touch /datadir/direct-data$ kubectl exec -it local-kata-virt touch /datadir/virt-data host 端进程12345678910111213141516171819# virtiofs 类型的 Pod$ ps -ef | grep 3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9root 889 10374 0 17:23 pts/34 00:00:00 grep --color=auto 3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9root 42721 1 0 17:10 ? 00:00:00 /usr/local/bin/containerd-shim-kata-v2 -namespace k8s.io -address /run/containerd/containerd.sock -publish-binary /usr/bin/containerd -id 3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9 -debugroot 42744 42721 0 17:10 ? 00:00:00 /opt/kata/libexec/kata-qemu/virtiofsd --syslog -o cache=auto -o no_posix_lock -o source=/run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared --fd=3 -f --thread-pool-size=1 -o announce_submountsroot 42751 1 0 17:10 ? 00:00:01 /opt/kata/bin/qemu-system-x86_64 -name sandbox-3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9 -uuid 26864a9b-ad50-43ab-9d41-2b6f4c7e34c3 -machine q35,accel=kvm,kernel_irqchip=on,nvdimm=on -cpu host,pmu=off -qmp unix:/run/vc/vm/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/qmp.sock,server=on,wait=off -m 256M,slots=10,maxmem=129389M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=off,addr=2,io-reserve=4k,mem-reserve=1m,pref64-reserve=1m -device virtio-serial-pci,disable-modern=false,id=serial0 -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/console.sock,server=on,wait=off -device nvdimm,id=nv0,memdev=mem0,unarmed=on -object memory-backend-file,id=mem0,mem-path=/opt/kata/share/kata-containers/kata-containers.img,size=134217728,readonly=on -device virtio-scsi-pci,id=scsi0,disable-modern=false -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0 -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-3588796381,guest-cid=3588796381 -chardev socket,id=char-597c9c629356cf41,path=/run/vc/vm/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-597c9c629356cf41,tag=kataShared -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 -device driver=virtio-net-pci,netdev=network-0,mac=6e:ae:f6:c6:82:0a,disable-modern=false,mq=on,vectors=4 -rtc base=utc,driftfix=slew,clock=host -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic --no-reboot -daemonize -object memory-backend-file,id=dimm1,size=256M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /opt/kata/share/kata-containers/vmlinux.container -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 cryptomgr.notests net.ifnames=0 pci=lastbus=0 root=/dev/pmem0p1 rootflags=dax,data=ordered,errors=remount-ro ro rootfstype=ext4 quiet systemd.show_status=false panic=1 nr_cpus=48 systemd.unit=kata-containers.target systemd.mask=systemd-networkd.service systemd.mask=systemd-networkd.socket scsi_mod.scan=none agent.debug_console agent.debug_console_vport=1026 -pidfile /run/vc/vm/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/pid -smp 1,cores=1,threads=1,sockets=48,maxcpus=48root 42758 42744 0 17:10 ? 00:00:00 /opt/kata/libexec/kata-qemu/virtiofsd --syslog -o cache=auto -o no_posix_lock -o source=/run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared --fd=3 -f --thread-pool-size=1 -o announce_submounts# direct 类型的 Pod$ ps -ef | grep 3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7root 25155 10374 0 17:26 pts/34 00:00:00 grep --color=auto 3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7root 38975 1 0 17:10 ? 00:00:00 /usr/local/bin/containerd-shim-kata-v2 -namespace k8s.io -address /run/containerd/containerd.sock -publish-binary /usr/bin/containerd -id 3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7 -debugroot 39008 38975 0 17:10 ? 00:00:00 /opt/kata/libexec/kata-qemu/virtiofsd --syslog -o cache=auto -o no_posix_lock -o source=/run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared --fd=3 -f --thread-pool-size=1 -o announce_submountsroot 39312 1 0 17:10 ? 00:00:01 /opt/kata/bin/qemu-system-x86_64 -name sandbox-3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7 -uuid c3fccd7b-f344-4819-a894-80f5e1c5606d -machine q35,accel=kvm,kernel_irqchip=on,nvdimm=on -cpu host,pmu=off -qmp unix:/run/vc/vm/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/qmp.sock,server=on,wait=off -m 256M,slots=10,maxmem=129389M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=off,addr=2,io-reserve=4k,mem-reserve=1m,pref64-reserve=1m -device virtio-serial-pci,disable-modern=false,id=serial0 -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/console.sock,server=on,wait=off -device nvdimm,id=nv0,memdev=mem0,unarmed=on -object memory-backend-file,id=mem0,mem-path=/opt/kata/share/kata-containers/kata-containers.img,size=134217728,readonly=on -device virtio-scsi-pci,id=scsi0,disable-modern=false -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0 -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-2091051760,guest-cid=2091051760 -chardev socket,id=char-2c14db67d9a8d23c,path=/run/vc/vm/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-2c14db67d9a8d23c,tag=kataShared -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 -device driver=virtio-net-pci,netdev=network-0,mac=86:3f:52:0e:93:29,disable-modern=false,mq=on,vectors=4 -rtc base=utc,driftfix=slew,clock=host -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic --no-reboot -daemonize -object memory-backend-file,id=dimm1,size=256M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /opt/kata/share/kata-containers/vmlinux.container -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 cryptomgr.notests net.ifnames=0 pci=lastbus=0 root=/dev/pmem0p1 rootflags=dax,data=ordered,errors=remount-ro ro rootfstype=ext4 quiet systemd.show_status=false panic=1 nr_cpus=48 systemd.unit=kata-containers.target systemd.mask=systemd-networkd.service systemd.mask=systemd-networkd.socket scsi_mod.scan=none agent.debug_console agent.debug_console_vport=1026 -pidfile /run/vc/vm/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/pid -smp 1,cores=1,threads=1,sockets=48,maxcpus=48root 39547 39008 0 17:10 ? 00:00:00 /opt/kata/libexec/kata-qemu/virtiofsd --syslog -o cache=auto -o no_posix_lock -o source=/run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared --fd=3 -f --thread-pool-size=1 -o announce_submountsYou have mail in /var/spool/mail/root# 可以看到无论是哪种持久卷挂载方式，host 上的进程信息均为：两个 virtiofsd 进程，一个 qemu-system 进程，一个 containerd-shim-kata-v2 进程# 之所以卷直通模式也会有 virtiofsd 进程启动是因为卷直通仅限于持久卷的部分，对于 sandbox rootfs 仍以 virtiofs 协议挂载至 guest 中 host 端卷目录结构12345678# virtiofs 类型的 Pod$ ls /var/lib/kubelet/pods/6cccbed3-45eb-4925-92af-aa2313f1e3f8/volumes/kubernetes.io~csi/pvc-0878818c-5d93-4ed4-b034-5bf37a657a6e/mountlost+found virt-data# direct 类型的 Pod$ ls /var/lib/kubelet/pods/7c974c3d-865a-4684-87ce-25d01a48d89e/volumes/kubernetes.io~csi/pvc-23dc3ecf-37fb-44ee-b8a6-7667a98aeb05/mount/# direct 类型的卷目录存在，但是没有相应的数据 host 端挂载点1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556$ lsblk...sdt 65:48 0 200G 0 disk └─mpathb 253:11 0 200G 0 mpath ├─i1666574565-lvmlock 253:13 0 10G 0 lvm ├─i1666574565-pvc--23dc3ecf--37fb--44ee--b8a6--7667a98aeb05 253:15 0 20M 0 lvm └─i1666574565-pvc--0878818c--5d93--4ed4--b034--5bf37a657a6e 253:16 0 20M 0 lvm /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-b00bec5dd6f4958e-datadir# virtiofs 类型的 Pod$ mount | grep 3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9shm on /run/containerd/io.containerd.grpc.v1.cri/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k)overlay on /run/containerd/io.containerd.runtime.v2.task/k8s.io/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/rootfs type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/1/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254839/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254839/work)tmpfs on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared type tmpfs (ro,mode=755)overlay on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/mounts/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/rootfs type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/1/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254839/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254839/work)overlay on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/rootfs type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/1/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254839/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254839/work)/dev/sda2 on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/mounts/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9-2c4e4dce04926bea-resolv.conf type xfs (ro,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9-2c4e4dce04926bea-resolv.conf type xfs (ro,relatime,attr2,inode64,noquota)overlay on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/mounts/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce/rootfs type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/251248/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254840/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254840/work)overlay on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce/rootfs type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/251248/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254840/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254840/work)/dev/mapper/i1666574565-pvc--0878818c--5d93--4ed4--b034--5bf37a657a6e on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/mounts/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-b00bec5dd6f4958e-datadir type ext4 (rw,relatime,data=ordered)/dev/mapper/i1666574565-pvc--0878818c--5d93--4ed4--b034--5bf37a657a6e on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-b00bec5dd6f4958e-datadir type ext4 (rw,relatime,data=ordered)/dev/sda2 on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/mounts/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-d10c2427b7cfd382-hosts type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-d10c2427b7cfd382-hosts type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/mounts/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-aec3c5c428ab0e89-termination-log type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-aec3c5c428ab0e89-termination-log type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/mounts/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-0a63282c5f8f4163-hostname type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-0a63282c5f8f4163-hostname type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/mounts/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-a9d3c3e132ce299b-resolv.conf type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-a9d3c3e132ce299b-resolv.conf type xfs (rw,relatime,attr2,inode64,noquota)tmpfs on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/mounts/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-30db53e3c43cb09f-serviceaccount type tmpfs (ro,relatime,size=32675592k)tmpfs on /run/kata-containers/shared/sandboxes/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/shared/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-30db53e3c43cb09f-serviceaccount type tmpfs (ro,relatime,size=32675592k)# direct 类型的 Pod$ mount | grep 3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7shm on /run/containerd/io.containerd.grpc.v1.cri/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k)overlay on /run/containerd/io.containerd.runtime.v2.task/k8s.io/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/rootfs type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/1/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254837/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254837/work)tmpfs on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared type tmpfs (ro,mode=755)overlay on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/mounts/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/rootfs type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/1/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254837/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254837/work)overlay on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/rootfs type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/1/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254837/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254837/work)/dev/sda2 on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/mounts/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7-27977320e8b95e05-resolv.conf type xfs (ro,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7-27977320e8b95e05-resolv.conf type xfs (ro,relatime,attr2,inode64,noquota)overlay on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/mounts/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379/rootfs type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/251248/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254838/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254838/work)overlay on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379/rootfs type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/251248/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254838/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/254838/work)/dev/sda2 on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/mounts/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-b03e8081dab8db44-hosts type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-b03e8081dab8db44-hosts type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/mounts/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-fc067734b16d5aec-termination-log type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-fc067734b16d5aec-termination-log type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/mounts/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-9578923c5ca9be73-hostname type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-9578923c5ca9be73-hostname type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/mounts/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-19d449c66d9d623d-resolv.conf type xfs (rw,relatime,attr2,inode64,noquota)/dev/sda2 on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-19d449c66d9d623d-resolv.conf type xfs (rw,relatime,attr2,inode64,noquota)tmpfs on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/mounts/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-fd37e76da5e2d86d-serviceaccount type tmpfs (ro,relatime,size=32675592k)tmpfs on /run/kata-containers/shared/sandboxes/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/shared/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-fd37e76da5e2d86d-serviceaccount type tmpfs (ro,relatime,size=32675592k)# 卷直通场景下，持久卷的块设备由 CSI 格式化后 attach 到节点后，不会执行 mount 到节点的操作，而是由 Kata Agent 触发 mount 操作，挂载到 VM 中，因此 host 端看不到挂载点信息，所以写在直通卷中的数据不会出现在 host 中# 两种模式下，rootfs 的挂载点一致 容器端挂载点12345678910111213141516171819202122232425262728293031323334# virtiofs 类型的 Pod$ kubectl exec -it local-kata-virt sh/ # df -ThFilesystem Type Size Used Available Use% Mounted onnone virtiofs 80.0G 65.3G 14.7G 82% /tmpfs tmpfs 64.0M 0 64.0M 0% /devtmpfs tmpfs 111.9M 0 111.9M 0% /sys/fs/cgroupnone virtiofs 18.4M 332.0K 17.6M 2% /datadirkataShared virtiofs 80.0G 65.3G 14.7G 82% /etc/hostskataShared virtiofs 80.0G 65.3G 14.7G 82% /dev/termination-logkataShared virtiofs 80.0G 65.3G 14.7G 82% /etc/hostnamekataShared virtiofs 80.0G 65.3G 14.7G 82% /etc/resolv.confshm tmpfs 111.9M 0 111.9M 0% /dev/shmnone virtiofs 31.2G 12.0K 31.2G 0% /var/run/secrets/kubernetes.io/serviceaccounttmpfs tmpfs 64.0M 0 64.0M 0% /proc/timer_list# direct 类型的 Pod$ kubectl exec -it local-kata-direct sh/ # df -ThFilesystem Type Size Used Available Use% Mounted onnone virtiofs 80.0G 65.3G 14.7G 82% /tmpfs tmpfs 64.0M 0 64.0M 0% /devtmpfs tmpfs 111.9M 0 111.9M 0% /sys/fs/cgroup/dev/sda ext4 18.4M 332.0K 17.6M 2% /datadirkataShared virtiofs 80.0G 65.3G 14.7G 82% /etc/hostskataShared virtiofs 80.0G 65.3G 14.7G 82% /dev/termination-logkataShared virtiofs 80.0G 65.3G 14.7G 82% /etc/hostnamekataShared virtiofs 80.0G 65.3G 14.7G 82% /etc/resolv.confshm tmpfs 111.9M 0 111.9M 0% /dev/shmnone virtiofs 31.2G 12.0K 31.2G 0% /var/run/secrets/kubernetes.io/serviceaccounttmpfs tmpfs 64.0M 0 64.0M 0% /proc/timer_list# virtiofs 的场景下的持久卷以 virtiofs 类型挂载到容器中# direct 的场景下的持久卷以 ext4 类型挂载到容器中，其中 ext4 和 /dev/sda 均可通过直通时指定 VM 端挂载点123456789101112131415161718192021222324252627282930313233343536373839404142434445# virtiofs 类型的 Pod$ kata-runtime exec 3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9root@clr-bab03a69f32d46fa9bfa8f735f0a4036 / $ df -ThFilesystem Type Size Used Avail Use% Mounted on/dev/root ext4 117M 95M 16M 87% /devtmpfs devtmpfs 111M 0 111M 0% /devtmpfs tmpfs 112M 0 112M 0% /dev/shmtmpfs tmpfs 45M 28K 45M 1% /runtmpfs tmpfs 4.0M 0 4.0M 0% /sys/fs/cgrouptmpfs tmpfs 112M 0 112M 0% /tmpkataShared virtiofs 63G 147M 63G 1% /run/kata-containers/shared/containersshm tmpfs 112M 0 112M 0% /run/kata-containers/sandbox/shmnone virtiofs 80G 66G 15G 83% /run/kata-containers/3f1316b887ba0cf7b0144a095fb543a262ae0c2b32a94c7658e1dbb3707c5ea9/rootfsnone virtiofs 80G 66G 15G 83% /run/kata-containers/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce/rootfsnone virtiofs 32G 12K 32G 1% /run/kata-containers/shared/containers/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-30db53e3c43cb09f-serviceaccountnone virtiofs 19M 332K 18M 2% /run/kata-containers/shared/containers/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-b00bec5dd6f4958e-datadirroot@clr-bab03a69f32d46fa9bfa8f735f0a4036 / $ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTSpmem0 259:0 0 126M 1 disk `-pmem0p1 259:1 0 124M 1 part /# direct 类型的 Pod$ kata-runtime exec 3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7root@clr-e5fe15d519854877912b7c4556e71299 / $ df -ThFilesystem Type Size Used Avail Use% Mounted on/dev/root ext4 117M 95M 16M 87% /devtmpfs devtmpfs 111M 0 111M 0% /devtmpfs tmpfs 112M 0 112M 0% /dev/shmtmpfs tmpfs 45M 28K 45M 1% /runtmpfs tmpfs 4.0M 0 4.0M 0% /sys/fs/cgrouptmpfs tmpfs 112M 0 112M 0% /tmpkataShared virtiofs 63G 147M 63G 1% /run/kata-containers/shared/containersshm tmpfs 112M 0 112M 0% /run/kata-containers/sandbox/shmnone virtiofs 80G 66G 15G 83% /run/kata-containers/3763faeb5fc0aa25265b751123b63fbbae1c8aa35bec202c42a4d569c0ef63a7/rootfs/dev/sda ext4 19M 332K 18M 2% /run/kata-containers/sandbox/storage/MDownone virtiofs 80G 66G 15G 83% /run/kata-containers/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379/rootfsnone virtiofs 32G 12K 32G 1% /run/kata-containers/shared/containers/6da9c84109d97ce2895b3e5b38631ebdf7185fc7b915cf4766572803ce4df379-fd37e76da5e2d86d-serviceaccountroot@clr-e5fe15d519854877912b7c4556e71299 / $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTSsda 8:0 0 20M 0 disk /run/kata-containers/sandbox/storage/MDowpmem0 259:0 0 126M 1 disk `-pmem0p1 259:1 0 124M 1 part /# direct 类型的 Pod 会较 virtiofs 多一个挂载点，/dev/sda -&gt; /run/kata-containers/sandbox/storage/MDow# virtiofs 类型的 Pod 会较 direct 多一个挂载点，/run/kata-containers/shared/containers/cc3782d5f7aef968f640a0c161ee027efec8054c9860cae9d7254f8fe0659cce-b00bec5dd6f4958e-datadir","link":"/2022/08/01/2022-08-01%20Kata%20Containers%20Block%20Volume%20%E7%9B%B4%E9%80%9A/"},{"title":"「 Istio 」流量管理 — VirtualService","text":"based on 1.15.0 VirtualService 定义了一组流量路由规则，以在 host 被寻址时应用。每个路由规则都为特定协议的流量定义了匹配标准。如果流量匹配，则将其发送到注册表中定义的命名目标服务（或其子集/版本） 流量来源也可以在路由规则中匹配。这允许为特定的客户端上下文定制路由。 Service 绑定到 service registry 中唯一名称的应用程序行为单元。Service 由多个 endpoints 组成，这些 endpoints 也就是运行在 Pod、容器、VM 等上的工作负载实例。 Service versions（subsets） 在持续部署场景中，一个服务可以有不同的实例子集的不同变体，这些变体不一定是不同的 API 版本，它们可能是对同一服务的迭代更改，部署在不同的环境（prod, staging, dev 等）中。常见场景包括 A/B 测试、金丝雀发布等。不同版本的选择可以根据各种标准（headers, url 等）的权重来决定。每个服务都有一个由其所有实例组成的默认版本。 Source 调用服务的下游客户端。 Host 客户端尝试连接到服务时使用的地址。 Access model 应用程序仅处理目标服务（Host），而不知道各个服务版本（subsets）。版本的实际选择由代理或者 sidecar 决定，这样应用程序代码能够将自身与依赖服务的解耦。 例如，以下示例默认将所有 HTTP 流量路由到标签为 review：v1 的 review 服务的 Pod。此外，路径以 /wpcatalog/ 或 /consumercatalog/ 开头的 HTTP 请求将被重写为 /newcatalog 并发送到标签为 version: v2 的 Pod。 12345678910111213141516171819202122232425apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: reviews-routespec: hosts: - reviews.prod.svc.cluster.local http: - name: &quot;reviews-v2-routes&quot; match: - uri: prefix: &quot;/wpcatalog&quot; - uri: prefix: &quot;/consumercatalog&quot; rewrite: uri: &quot;/newcatalog&quot; route: - destination: host: reviews.prod.svc.cluster.local subset: v2 - name: &quot;reviews-v1-route&quot; route: - destination: host: reviews.prod.svc.cluster.local subset: v1 路由目的地的子集或者版本通过对必须在相应 DestinationRule 中声明的命名服务子集的引用来标识。 12345678910111213apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: reviews-destinationspec: host: reviews.prod.svc.cluster.local subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 VirtualService Field Description hosts 向其发送流量的目标主机（只有访问客户端的 Host 字段为 hosts 配置的地址才能路由到后端服务）。可以是带有通配符前缀的 DNS 名称或 IP 地址。根据平台的不同，也可以使用短名称代替 FQDN（即名称中没有点）。在这种情况下，主机的 FQDN 将基于底层平台派生单个 VirtualService 可用于描述相应 host 的所有流量属性，包括多个 HTTP 和 TCP 端口的流量属性。或者，可以使用多个 VirtualService 定义 host 的流量属性，但有一些注意事项，参阅操作指南Kubernetes 用户注意事项：当使用短名称时（例如 reviews 而不是 reviews.default.svc.cluster.local），Istio 将根据规则的命名空间而不是服务来解析短名称。名为 reviews 的 host 在 default 命名空间中的规则将被解析为 reviews.default.svc.cluster.local，而与 reviews 服务关联的实际命名空间无关。为避免潜在的错误配置，建议始终使用完全限定域名而不是短名称hosts 字段适用于 HTTP 和 TCP 服务。网格内的服务，即服务注册表中的服务，必须始终使用它们的字母数字名称来引用。 IP 地址仅允许用于通过 Gateway 定义的服务注意：对于 delegate VirtualService 而言，必须为空。 gateways 应用这些路由规则的 Gateway 和 sidecar 的名称。 其他命名空间中的 Gateway 的引用方式为&lt;gateway namespace&gt;/&lt;gateway name&gt;；指定没有显式的指定命名空间则与 VirtualService 的命名空间相同。单个 VirtualService 用于网格内的 sidecar 以及一个或多个 Gateway。可以使用协议特定路由的匹配条件中的源字段覆盖该字段强加的选择条件。保留字 mesh 用于表述网格中的所有 sidecar。当省略该字段时，将使用默认 Gateway 即 mesh，这会将规则应用于网格中的所有 sidecar。如果提供了 Gateway 名称列表，则规则将仅适用该相关的 Gateway。要将规则应用于 Gateway 和 sidecar，请将 mesh 指定为 gateways 之一 http HTTP 流量的路由规则的有序列表。 使用匹配传入请求的第一个规则 tls An ordered list of route rule for non-terminated TLS &amp; HTTPS traffic. Routing is typically performed using the SNI value presented by the ClientHello message. TLS routes will be applied to platform service ports named ‘https-*’, ‘tls-*’, unterminated gateway ports using HTTPS/TLS protocols (i.e. with “passthrough” TLS mode) and service entry ports using HTTPS/TLS protocols. The first rule matching an incoming request is used. NOTE: Traffic ‘https-*’ or ‘tls-*’ ports without associated virtual service will be treated as opaque TCP traffic. tcp 不透明 TCP 流量的路由规则的有序列表。 TCP 路由将应用于不是 HTTP 或 TLS 端口的任何端口。使用匹配传入请求的第一个规则 exportTo 此 VirtualService 暴露到的命名空间列表。暴露 VirtualService 允许它被其他命名空间中定义的 sidecar 和 Gateway 使用。此功能为服务所有者和网格管理员提供了一种机制来控制跨命名空间边界的 VirtualService 的可见性如果未指定命名空间，则默认情况下将 VirtualService 暴露到所有命名空间 . 为保留标识代表暴露到 VirtualService 的同一命名空间中。类似地，* 代表暴露到所有命名空间 DestinationDestination 表示在处理路由规则后将请求（连接）发送到的网络可寻址服务。 destination.host 应该明确引用服务注册表中的服务。 Istio 的服务注册表由平台服务注册表中的所有服务（例如 Kubernetes 服务、Consul 服务）以及通过 ServiceEntry 资源声明的服务组成。 以下示例中默认将所有流量路由到带有标签 version：v1（即 subset v1）的 review 服务的 Pod，并将符合条件的路由到 subset v2。 123456789101112131415161718192021222324apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: reviews-route namespace: foospec: hosts: - reviews # interpreted as reviews.foo.svc.cluster.local http: - match: - uri: prefix: &quot;/wpcatalog&quot; - uri: prefix: &quot;/consumercatalog&quot; rewrite: uri: &quot;/newcatalog&quot; route: - destination: host: reviews # interpreted as reviews.foo.svc.cluster.local subset: v2 - route: - destination: host: reviews # interpreted as reviews.foo.svc.cluster.local subset: v1 以下 VirtualService 为 productpage.prod.svc.cluster.local 服务的设置了 5s 的超时时间。此规则中没有定义子集，Istio 将从服务注册表中获取 productpage.prod.svc.cluster.local 服务的所有实例，并填充 sidecar 的负载均衡池。另外，此规则设置在 istio-system 命名空间中，但使用的是 productpage 服务的完全限定域名 productpage.prod.svc.cluster.local。因此，规则的命名空间对解析 productpage 服务的名称没有影响。 12345678910111213apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: my-productpage-rule namespace: istio-systemspec: hosts: - productpage.prod.svc.cluster.local # ignores rule namespace http: - timeout: 5s route: - destination: host: productpage.prod.svc.cluster.local 为了路由到网格外服务的流量，必须首先使用 ServiceEntry 资源将外部服务添加到 Istio 的内部服务注册表中。然后可以定义 VirtualServices 来控制绑定到这些外部服务的流量。例如，以下规则为 wikipedia.org 定义了一个 Service，并为 HTTP 请求设置了 5s 的超时时间。 1234567891011121314151617181920212223242526apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: external-svc-wikipediaspec: hosts: - wikipedia.org location: MESH_EXTERNAL ports: - number: 80 name: example-http protocol: HTTP resolution: DNS---apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: my-wiki-rulespec: hosts: - wikipedia.org http: - timeout: 5s route: - destination: host: wikipedia.org Field Description host 服务注册表中的服务名称。服务名称从平台的服务注册表（例如，Kubernetes 服务、Consul 服务等）和 ServiceEntry 声明的 host 中查找。两者都找不到的流量将被丢弃。同样的，推荐使用完全限定域名 subset 服务中子集的名称。仅适用于网格内的服务。子集必须在相应的 DestinationRule 中定义 port 指定寻址目标主机上的端口。如果服务只公开一个端口，则不需要显式选择端口 HTTPRoute描述路由 HTTP/1.1、HTTP2 和 gRPC 流量的匹配条件和行为。有关使用示例，请参阅 VirtualService。 Field Description name 分配给路由的名称，用作调试。该名称将与匹配的路由结果一起记录在访问日志 match 应用规则需要满足的匹配条件。单个匹配块内的所有条件都具有 AND 语义，而匹配块列表具有 OR 语义。如果任何一个匹配块匹配成功，则应用该规则 route HTTP 规则可以重定向或转发（默认）流量。转发目标可以是服务的多个版本之一（subset）。与服务版本相关的权重决定了它接收的流量比例 redirect HTTP 规则可以重定向或转发（默认）流量。如果在规则中指定了流量直通选项，则重定向将被忽略。重定向可用于将 HTTP 301 重定向发送到不同的 URI delegate 委托是指定可用于定义委托 HTTPRoute 的特定 VirtualService。只有Route和Redirect都为空时才可以设置，并且delegate VirtualService的路由规则会与当前的路由规则合并。注意：仅支持一级委派，delegate 的 HTTPMatchRequest 必须是 root 的严格子集，否则会发生冲突，HTTPRoute 不会生效 rewrite 重写 HTTP URI 和 Authority header。 Rewrite 不能与 Redirect 原语一起使用。转发前会进行重写 timeout HTTP 请求超时，默认禁用 retries HTTP 请求的重试策略 fault 故障注入策略应用于客户端的 HTTP 流量。注意，在客户端启用故障时，将不会启用超时或重试 mirror 除了将请求转发到预期目标之外，还将 HTTP 流量镜像到另一个目标。镜像流量是在尽力而为的基础上，sidercar / gateway 在从原始目的地返回响应之前不会等待镜像集群响应。会为镜像目的地生成统计信息。 mirrorPercentage 要镜像的流量百分比。默认为所有的流量（100%）都会被镜像。最大值为 100%。 corsPolicy 跨域资源共享策略 (CORS)。有关跨源资源共享的更多详细信息，参阅 CORS headers Header 操作规则 Delegate在 Istio 1.5 中，VirtualService 资源之间是无法进行转发的，在 Istio 1.6 版本中规划了 VirtualService Chain 机制，也就是说，我们可以通过 delegate 配置项将一个 VirtualService 代理到另外一个 VirtualService 中进行规则匹配。 例如，路由规则通过名为 productpage 的委托 VirtualService 将流量转发到 /productpage，通过名为 reviews 的委托 VirtualService 将流量转发到 /reviews。 12345678910111213141516171819202122apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: bookinfospec: hosts: - &quot;bookinfo.com&quot; gateways: - mygateway http: - match: - uri: prefix: &quot;/productpage&quot; delegate: name: productpage namespace: nsA - match: - uri: prefix: &quot;/reviews&quot; delegate: name: reviews namespace: nsB 12345678910111213141516apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: productpage namespace: nsAspec: http: - match: - uri: prefix: &quot;/productpage/v1/&quot; route: - destination: host: productpage-v1.nsA.svc.cluster.local - route: - destination: host: productpage.nsA.svc.cluster.local 12345678910apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviews namespace: nsBspec: http: - route: - destination: host: reviews.nsB.svc.cluster.local Field Description name 委托 VirtualService 的名称 namespace 委托 VirtualService 所在的命名空间。默认情况下，它与根的相同 Headers当 Envoy 将请求转发到目标服务或从目标服务转发响应时，可以操作 header 信息。可以为特定路由目的地或所有目的地指定 header 操作规则。以下 VirtualService 将值为 test: true 的请求头添加到路由到任何 reviews 服务目标的请求中。同时，删除了仅来自 reviews 服务的 v1 版本的响应头中的 foo。 12345678910111213141516171819202122232425apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: reviews-routespec: hosts: - reviews.prod.svc.cluster.local http: - headers: request: set: test: &quot;true&quot; route: - destination: host: reviews.prod.svc.cluster.local subset: v2 weight: 25 - destination: host: reviews.prod.svc.cluster.local subset: v1 headers: response: remove: - foo weight: 75 Field Description request 在将请求转发到目标服务之前应用的标头操作规则 response 在向调用者返回响应之前应用的标头操作规则 TLSRoute描述路由未终止的 TLS 流量 (TLS/HTTPS) 的匹配条件和操作。 以下路由规则根据 SNI 值将到达名为 mygateway 的 Gateway 的端口 443 的未终止 TLS 流量转发到网格中的内部服务。 123456789101112131415161718192021222324apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: bookinfo-snispec: hosts: - &quot;*.bookinfo.com&quot; gateways: - mygateway tls: - match: - port: 443 sniHosts: - login.bookinfo.com route: - destination: host: login.prod.svc.cluster.local - match: - port: 443 sniHosts: - reviews.bookinfo.com route: - destination: host: reviews.prod.svc.cluster.local Field Description match 应用规则需要满足的匹配条件。单个匹配块内的所有条件都具有 AND 语义，而匹配块列表具有 OR 语义。如果任何一个匹配块匹配成功，则应用该规则 route 连接应转发到的目的地 TCPRoute描述路由 TCP 流量的匹配条件和操作。 以下路由规则将到达端口 27017 的 mongo.prod.svc.cluster.local 的流量转发到端口 5555 上的另一个 Mongo 服务器。 123456789101112131415apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: bookinfo-mongospec: hosts: - mongo.prod.svc.cluster.local tcp: - match: - port: 27017 route: - destination: host: mongo.backup.svc.cluster.local port: number: 5555 Field Description match 应用规则需要满足的匹配条件。单个匹配块内的所有条件都具有 AND 语义，而匹配块列表具有 OR 语义。如果任何一个匹配块匹配成功，则应用该规则 route 连接应转发到的目的地 HTTPMatchRequestHttpMatchRequest 指定要满足的一组标准，以便将规则应用于 HTTP 请求。 例如，以下内容将规则限制为仅匹配 URL 路径以 /ratings/v2/ 开头的请求，并且请求包含具有值 jason 的自定义最终用户标头。 123456789101112131415161718apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: ratings-routespec: hosts: - ratings.prod.svc.cluster.local http: - match: - headers: end-user: exact: jason uri: prefix: &quot;/ratings/v2/&quot; ignoreUriCase: true route: - destination: host: ratings.prod.svc.cluster.local HTTPMatchRequest 不能为空。注意：指定委托 VirtualService 时，不能设置正则表达式字符串匹配。 Field Description name 分配给匹配项的名称。匹配的名称将与父路由的名称一同记录在与该路由匹配的请求的访问日志中 uri 区分大小写，格式包括 exact（精准匹配）、prefix（前缀匹配）和 regex（正则）注意：可以通过 ignore_uri_case 标志启用不区分大小写的匹配 scheme 区分大小写，格式包括 exact（精准匹配）、prefix（前缀匹配）和 regex（正则） method 区分大小写，格式包括 exact（精准匹配）、prefix（前缀匹配）和 regex（正则） authority 区分大小写，格式包括 exact（精准匹配）、prefix（前缀匹配）和 regex（正则） headers header 的 key 必须为小写并使用连字符作为分隔符，例如 x-request-id。区分大小写，格式包括 exact（精准匹配）、prefix（前缀匹配）和 regex（正则）。如果该值为空并且仅指定了 header 的名称，则检查标头的存在注意：键 uri、scheme、method 和 authority 将被忽略 port 指定正在寻址的 host 上的端口。许多服务只公开单个端口或使用它们支持的协议标记端口，在这些情况下，不需要显式选择端口 sourceLabels 一个或多个标签，用于限制规则对具有给定标签的源（客户端）工作负载的适用性。如果 VirtualService 在顶级 gateways 字段中指定了网关列表，仅当列表中包含保留的 gateway — mesh 时，该字段才生效 gateways 待应用规则的网关的名称。 VirtualService 的顶级 gateways 字段中的网关名称（如果有）被覆盖。Gateway 匹配独立于 sourceLabels queryParams 用于匹配的查询参数，例如 exact: “true”，extact: “” 和 regex: “\\d+$”注意：目前不支持前缀匹配 ignoreUriCase 用于指定 URI 匹配是否不区分大小写的标志。注意：只有在完全和前缀 URI 匹配的情况下才会忽略大小写 withoutHeaders withoutHeaders 与 header 的语法相同，但含义相反。如果 header 与 withoutHeaders 中的匹配规则匹配，则流量变为不匹配 sourceNamespace 源命名空间限制规则对该命名空间中工作负载的适用性。如果 VirtualService 在顶级 gateways 字段中指定了网关列表，仅当列表中包含保留的 gateway — mesh 时，该字段才生效 HTTPRouteDestination每个路由规则都与一个或多个服务版本相关联。与版本相关的权重决定了它接收的流量比例。 例如，以下规则会将 reviews 服务的 25% 流量路由到 v2 版本的实例中，而剩余流量（即 75%）将路由到 v1 版本。 1234567891011121314151617apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: reviews-routespec: hosts: - reviews.prod.svc.cluster.local http: - route: - destination: host: reviews.prod.svc.cluster.local subset: v2 weight: 25 - destination: host: reviews.prod.svc.cluster.local subset: v1 weight: 75 流量也可以分成两个完全不同的服务，而无需定义新的子集（也就是内部分流）。 例如，以下规则将 25% 的流量从 reviews.com 转发到 dev.reviews.com。 123456789101112131415apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: reviews-route-two-domainsspec: hosts: - reviews.com http: - route: - destination: host: dev.reviews.com weight: 25 - destination: host: reviews.com weight: 75 Field Description destination 请求/连接应转发到的服务实例 weight 要转发到目的地的流量的相对比例（即权重/所有权重的总和）。如果规则中只有一个目的地，它将接收所有流量。如果权重为 0，则目的地将不会收到任何流量 headers header 操作规则 RouteDestination Field Description destination 请求/连接应转发到的服务实例 weight 要转发到目的地的流量的相对比例（即权重/所有权重的总和）。如果规则中只有一个目的地，它将接收所有流量。如果权重为 0，则目的地将不会收到任何流量 L4MatchAttributes Field Description destinationSubnets 带有可选子网的目标 IPv4 或 IPv6 IP 地址。例如，a.b.c.d/xx 或只是 a.b.c.d port 指定正在寻址的 host 上的端口。许多服务只公开单个端口或使用它们支持的协议标记端口，在这些情况下，不需要显式选择端口 sourceLabels 一个或多个标签，用于限制规则对具有给定标签的源（客户端）工作负载的适用性。如果 VirtualService 在顶级 gateways 字段中指定了网关列表，仅当列表中包含保留的 gateway — mesh 时，该字段才生效 gateways 待应用规则的网关的名称。 VirtualService 的顶级 gateways 字段中的网关名称（如果有）被覆盖。Gateway 匹配独立于 sourceLabels sourceNamespace 源命名空间限制规则对该命名空间中工作负载的适用性。如果 VirtualService 在顶级 gateways 字段中指定了网关列表，仅当列表中包含保留的 gateway — mesh 时，该字段才生效 TLSMatchAttributes Field Description sniHosts 要匹配的 SNI（server name indicator）。通配符前缀可用于 SNI 值，例如，*.com 将匹配 foo.example.com 以及 example.com。 SNI 值必须是相应虚拟服务主机的子集（即属于域内） destinationSubnets 带有可选子网的目标 IPv4 或 IPv6 IP 地址。例如，a.b.c.d/xx 或只是 a.b.c.d port 指定正在寻址的 host 上的端口。许多服务只公开单个端口或使用它们支持的协议标记端口，在这些情况下，不需要显式选择端口 sourceLabels 一个或多个标签，用于限制规则对具有给定标签的源（客户端）工作负载的适用性。如果 VirtualService 在顶级 gateways 字段中指定了网关列表，仅当列表中包含保留的 gateway — mesh 时，该字段才生效 gateways 待应用规则的网关的名称。 VirtualService 的顶级 gateways 字段中的网关名称（如果有）被覆盖。Gateway 匹配独立于 sourceLabels sourceNamespace 源命名空间限制规则对该命名空间中工作负载的适用性。如果 VirtualService 在顶级 gateways 字段中指定了网关列表，仅当列表中包含保留的 gateway — mesh 时，该字段才生效 HTTPRedirectHTTPRedirect 可用于向调用者发送 301 重定向响应。例如，以下规则将 review 服务上的 /v1/getProductRatings API 请求重定向到 bookratings 服务提供的 /v1/bookRatings。 123456789101112131415apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: ratings-routespec: hosts: - ratings.prod.svc.cluster.local http: - match: - uri: exact: /v1/getProductRatings redirect: uri: /v1/bookRatings authority: newratings.default.svc.cluster.local ... Field Description uri 在重定向时，使用此值覆盖 URL 的 path 部分。无论请求 URI 是否匹配为确切路径或前缀，都将替换整个路径 authority 在重定向时，使用此值覆盖 URL 的 Authority/Host 部分 port 在重定向时，使用此值覆盖 URL 的 Port 部分 derivePort 在重定向时，动态设置端口 scheme 在重定向时，使用此值覆盖 URL 的 Scheme 部分。例如，http 或 https。如果未设置，将使用原 Scheme。如果 derivePort 设置为 FROM_PROTOCOL_DEFAULT，这也会影响该端口 redirectCode 在重定向时，指定要在重定向响应中使用的 HTTP 状态代码。默认响应代码为 MOVED_PERMANENTLY (301) HTTPRewriteHTTPRewrite 可用于在将请求转发到目标之前重写 HTTP 请求的特定部分。HTTPRewrite 只能与 HTTPRouteDestination 一起使用。 以下示例演示如何在实际调用之前将 /ratings 前缀重写为 rating 服务。 1234567891011121314151617apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: ratings-routespec: hosts: - ratings.prod.svc.cluster.local http: - match: - uri: prefix: /ratings rewrite: uri: /v1/bookRatings route: - destination: host: ratings.prod.svc.cluster.local subset: v1 Field Description uri 用这个值重写 URI 的 Path 或 Prefix 部分。如果原始 URI 是根据前缀匹配的，则此字段中提供的值将替换相应匹配的前缀 authority 用这个值重写 Authority/Host header StringMatch匹配 HTTP header 中的特定字符串的策略。匹配区分大小写。 Field Description exact 精准匹配 prefix 前缀匹配 regex 正则匹配 （参阅：https://github.com/google/re2/wiki/Syntax） HTTPRetryHTTP 请求失败时的重试策略。 例如，以下规则将调用 rating 服务 v1 版本时的最大重试次数设置为 3，每次重试超时为 2 秒。如果出现 gateway-error，connect-failure 和 refused-stream 的错误将重试。 12345678910111213141516apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: ratings-routespec: hosts: - ratings.prod.svc.cluster.local http: - route: - destination: host: ratings.prod.svc.cluster.local subset: v1 retries: attempts: 3 perTryTimeout: 2s retryOn: gateway-error,connect-failure,refused-stream Field Description attempts 请求允许的重试次数。重试间隔将自动确定（25ms+）。当配置了 HTTP 路由的请求超时或 perTryTimeout 时，实际尝试的重试次数还取决于指定的请求超时和 perTryTimeout 值 perTryTimeout 给定请求的每次尝试超时，包括初始调用和任何重试。格式：1h/1m/1s/1ms。必须 &gt;=1 ms。默认值与 HTTP 路由的请求超时相同，即没有超时 retryOn 指定重试发生的条件。可以使用 , 分隔列表指定一个或多个策略。如果 retryOn 指定了一个有效的 HTTP 状态，它将被添加到 retriable_status_codes 重试策略中。有关更多详细信息，参阅重试策略和 gRPC 重试策略 retryRemoteLocalities 是否应重试到其他位置。有关更多详细信息，参阅重试插件配置 CorsPolicy描述给定服务的跨域资源共享（CORS）策略。有关跨源资源共享的更多详细信息，参阅 CORS。 例如，以下规则使用 HTTP POST/GET 将跨源请求限制为来自 example.com 域的请求，并将 Access-Control-Allow-Credentials header 设置为 false。此外，它只暴露 X-Foo-bar header 并设置 1 天的有效期。 12345678910111213141516171819202122apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: ratings-routespec: hosts: - ratings.prod.svc.cluster.local http: - route: - destination: host: ratings.prod.svc.cluster.local subset: v1 corsPolicy: allowOrigins: - exact: https://example.com allowMethods: - POST - GET allowCredentials: false allowHeaders: - X-Foo-Bar maxAge: &quot;24h&quot; Field Description allowOrigins 匹配允许的来源的字符串模式。如果任何字符串匹配器匹配，则允许来源。如果找到匹配项，则传出的 Access-Control-Allow-Origin 将设置为客户端提供的源 allowMethods 允许访问资源的 HTTP 方法列表。内容将被序列化到 Access-Control-Allow-Methods header 中 allowHeaders 请求资源时可以使用的 HTTP header 列表。序列化为 Access-Control-Allow-Headers header exposeHeaders 允许浏览器访问的 HTTP header 列表。序列化为 Access-Control-Expose-Headers header maxAge 指定预检请求的结果可以缓存多长时间。转换为 Access-Control-Max-Age header allowCredentials 是否允许调用者使用凭据发送实际请求（而不是预检）。转换为 Access-Control-Allow-Credentials header HTTPFaultInjectionHTTPFaultInjection 可用于在将 HTTP 请求转发到路由中注入故障。故障规范是 VirtualService 规则的一部分。错误包括从下游服务中止 HTTP 请求、延迟请求代理等。故障规则中至少有 delay 或者 abort。注意：delay 和 abort 故障相互独立，即使两者同时指定。 Field Description delay 在转发之前延迟请求，模拟网络问题、上游服务过载等各种故障 abort 中止 HTTP 请求尝试并将错误代码返回给下游服务，模拟上游服务故障 PortSelectorPortSelector 指定用于匹配或选择最终路由的端口号。 Field Description number 有效的端口号 Percent百分比范围 0 ~ 100。 Field Description value 百分比范围 Headers.HeaderOperationsheader 操作方式。 Field Description set 用给定的值覆盖原 header 中的 Key add 讲给定的值追加到 header 中的 Key 中 remove 移除指定的 header HTTPFaultInjection.Delay延迟类故障注入。 例如，在所有标签为 env: prod 的 Pod 中对 reviews 服务的 v1 版本的发起的每 1000 个请求中引入 1 个延迟 5 秒的故障。 1234567891011121314151617181920apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: reviews-routespec: hosts: - reviews.prod.svc.cluster.local http: - match: - sourceLabels: env: prod route: - destination: host: reviews.prod.svc.cluster.local subset: v1 fault: delay: percentage: value: 0.1 fixedDelay: 5s fixedDelay 字段用于指示延迟量（以秒为单位）。可选的百分比字段可用于仅延迟一定百分比的请求。如果未指定，所有请求都将被延迟。 Field Description fixedDelay 在转发请求之前添加一个固定的延迟。格式：1h/1m/1s/1ms。必须 &gt;= 1 ms percentage 注入延迟的请求的百分比 percent 注入延迟的请求的百分比（0-100）。不推荐使用整数百分比值。请改用 percentage 字段 HTTPFaultInjection.Abort中止类故障注入。 例如，为 ratings 服务 v1 版本的每 1000 个请求中的 1 个返回 HTTP 400 错误代码。 1234567891011121314151617apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: ratings-routespec: hosts: - ratings.prod.svc.cluster.local http: - route: - destination: host: ratings.prod.svc.cluster.local subset: v1 fault: abort: percentage: value: 0.1 httpStatus: 400 httpStatus 字段表示返回给调用者的 HTTP 状态码。可选的百分比字段只能用于中止一定百分比的请求。如果未指定，则中止所有请求。 Field Description httpStatus 用于中止 HTTP 请求的 HTTP 状态码 percentage 中止请求的百分比 HTTPRedirect.RedirectPortSelection Name Description FROM_PROTOCOL_DEFAULT 对于 HTTP 自动设置为 80，对于 HTTPS 自动设置为 443 FROM_REQUEST_PORT 自动使用请求的端口","link":"/2022/08/05/2022-08-05%20Istio%20%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86%20-%20Virtual%20Service/"},{"title":"「 Istio 」流量管理 — DestinationRule","text":"based on 1.15.0 DestinationRule 定义了在路由发生后应用于服务的流量的策略。这些规则指定负载均衡的配置、sidecar 的连接池大小以及异常值检测设置，用来检测、驱逐负载均衡池中不健康的后端。 例如，review 服务的简单负载均衡策略如下。 123456789apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: bookinfo-ratingsspec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: simple: LEAST_REQUEST 可以通过定义 subset 覆盖在服务级别指定的设置来指定版本特定策略。 以下规则对流向名为 testversion 的子集的所有流量使用 ROUND_ROBIN 负载均衡策略。 12345678910111213141516apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: bookinfo-ratingsspec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: simple: LEAST_REQUEST subsets: - name: testversion labels: version: v3 trafficPolicy: loadBalancer: simple: ROUND_ROBIN 注意：为子集指定的策略在路由规则明确向该子集发送流量之前不会生效。 流量策略也可以针对特定端口进行定制。以下规则对流向端口 80 的所有流量使用 LEAST_REQUEST 负载均衡策略，而对流向端口 9080 的流量使用 ROUND_ROBIN 负载均衡策略。 12345678910111213141516apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: bookinfo-ratings-portspec: host: ratings.prod.svc.cluster.local trafficPolicy: # Apply to all ports portLevelSettings: - port: number: 80 loadBalancer: simple: LEAST_REQUEST - port: number: 9080 loadBalancer: simple: ROUND_ROBIN 目标规则也可以针对特定的工作负载进行定制。以下示例显示了如何使用工作负载选择器将目标规则应用于特定工作负载。 1234567891011121314151617apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: configure-client-mtls-dr-with-workloadselectorspec: workloadSelector: matchLabels: app: ratings trafficPolicy: loadBalancer: simple: ROUND_ROBIN portLevelSettings: - port: number: 31443 tls: credentialName: client-credential mode: MUTUAL DestinationRule Field Description host 服务注册表中的服务名称。服务名称从平台的服务注册表（例如，Kubernetes 服务、Consul 服务等）和 ServiceEntry 声明的 host 中查找。两者都找不到的流量将被丢弃。同样的，推荐使用完全限定域名，host 字段适用于 HTTP 和 TCP 的服务 trafficPolicy 要应用的流量策略（负载均衡策略、连接池大小、异常值检测） subsets 一个或多个命名集，代表服务的各个版本。流量策略可以在子集级别被覆盖 exportTo 此 VirtualService 暴露到的命名空间列表。暴露 VirtualService 允许它被其他命名空间中定义的 sidecar 和 Gateway 使用。此功能为服务所有者和网格管理员提供了一种机制来控制跨命名空间边界的 VirtualService 的可见性如果未指定命名空间，则默认情况下将 VirtualService 暴露到所有命名空间 . 为保留标识代表暴露到 VirtualService 的同一命名空间中。类似地，* 代表暴露到所有命名空间 workloadSelector 用于选择应用此 DestinationRule 配置的特定 pod/VM 集的条件。如果指定，则 DestinationRule 配置将仅应用于与同一命名空间中的选择器标签匹配的工作负载实例。工作负载选择器不适用于跨命名空间。如果省略，DestinationRule 将回退到其默认行为。例如，如果特定的 sidecar 需要为网格外的服务设置出口 TLS 设置，而不是网格中的每个 sidecar 都需要配置（这是默认行为），则可以指定工作负载选择器 TrafficPolicy Field Description loadBalancer 负载均衡器算法的设置 connectionPool 与上游服务的连接池的设置 outlierDetection 负载均衡池中逐出不健康后端的设置 tls 与上游服务连接的 TLS 相关设置 portLevelSettings 各个端口的流量策略。注意，portLevelSettings 将覆盖 destinationLevel 设置。portLevelSettings 未设置的部分，会以 destinationLevel 级别为准 tunnel 在 DestinationRule 中配置的 host 的其他传输层或应用层上的隧道 TCP 配置。隧道设置可以应用于 TCP 或 TLS 路由，但不能应用于 HTTP 路由 SubsetService 的 endpoint 的子集。子集可用于 A/B 测试或路由到特定服务版本等场景。此外，在 VirtualService 级别定义的流量策略可以被子集级别覆盖。 通常需要一个或多个标签来标识子集目的地，但是，当相应的 DestinationRule 表示支持多个 SNI 主机的主机（例如，出口网关）时，没有标签的子集可能是有意义的。在这种情况下，可以使用带有 ClientTLSSettings 的流量策略来识别与命名子集相对应的特定 SNI 主机。 Field Description name 子集的名称。服务名称和子集名称可用于路由规则中的流量拆分 labels 服务注册表中的服务 endpoint 上应用过滤器 trafficPolicy 子集的流量策略。子集会继承在 DestinationRule 级别指定的流量策略，同时会被子集级别指定的设置覆盖 LoadBalancerSettings应用到特定目的地的负载均衡策略。有关更多详细信息，参阅 Envoy 的负载均衡文档。 例如，以下规则对流向 ratings 服务的所有流量使用 ROUND_ROBIN 负载均衡策略。 123456789apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: bookinfo-ratingsspec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: simple: ROUND_ROBIN 以下示例对于使用 User cookie 作为哈希键访问 rating 服务设置粘性会话。 123456789101112apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: bookinfo-ratingsspec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: consistentHash: httpCookie: name: user ttl: 0s Field Description simple 参考 LoadBalancerSettings.SimpleLB 说明 consistentHash 参考 LoadBalancerSettings.ConsistentHashLB 说明 localityLbSetting 本地负载均衡器设置，这将完全覆盖网格范围的设置，这意味着不会在此对象和 MeshConfig 之间执行合并 warmupDurationSecs 表示 Service 的预热持续时间。如果设置，则新创建的 service endpoint 在此窗口期间从其创建时间开始保持预热模式，并且 Istio 逐渐增加该 endpoint 的流量，而不是发送成比例的流量。应该为需要预热时间的合理延迟服务完整生产负载的服务启用此功能。目前只支持 ROUND_ROBIN 和 LEAST_CONN 负载均衡器 ConnectionPoolSettings上游主机的连接池设置。这些设置适用于上游服务中的每个单独的主机。有关更多详细信息，参阅 Envoy 的断路器。连接池设置可以应用于 TCP 级别以及 HTTP 级别。 例如，以下规则设置了 100 个连接到名为 myredissrv 的 redis 服务的限制，连接超时为 30 毫秒。 1234567891011121314apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: bookinfo-redisspec: host: myredissrv.prod.svc.cluster.local trafficPolicy: connectionPool: tcp: maxConnections: 100 connectTimeout: 30ms tcpKeepalive: time: 7200s interval: 75s Field Description tcp HTTP 和 TCP 上游连接通用的设置 http HTTP 连接池设置 OutlierDetection一种断路器实现，用于跟踪上游服务中每个单独主机的状态。 适用于 HTTP 和 TCP 服务。 对于 HTTP 服务，对于 API 调用持续返回 5xx 错误的主机将在预定义的时间段内从池中弹出 对于 TCP 服务，在测量连续错误指标时，与给定主机的连接超时或连接失败将计为错误 有关更多详细信息，参阅 Envoy 的异常值检测。 以下规则将连接池大小设置为 100 个 HTTP1 连接，其中 reviews 服务的单连接的请求最大不超过 10 个。此外，它设置了 1000 个并发 HTTP2 请求的限制，并将上游 host 配置为每 5 分钟扫描一次，任何连续 7 次失败并出现 502、503 或 504 错误代码的 host 将被弹出 15 分钟。 1234567891011121314151617apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: reviews-cb-policyspec: host: reviews.prod.svc.cluster.local trafficPolicy: connectionPool: tcp: maxConnections: 100 http: http2MaxRequests: 1000 maxRequestsPerConnection: 10 outlierDetection: consecutive5xxErrors: 7 interval: 5m baseEjectionTime: 15m Field Description splitExternalLocalOriginErrors 确定是否区分本地源故障和外部错误。如果设置为 true，则连续_local_origin_failure 被考虑用于异常值检测计算。当您希望根据本地看到的错误（例如连接失败、连接超时等）而不是上游服务重新调整的状态码来推导异常检测状态时，应该使用此选项。当上游服务为某些请求显式返回 5xx 并且您希望在确定主机的异常检测状态时忽略来自上游服务的这些响应时，这尤其有用。默认为假 consecutiveLocalOriginFailures 在弹出发生之前连续发生的本地故障数。默认为 5。参数仅在 splitExternalLocalOriginErrors 设置为 true 时生效 consecutiveGatewayErrors 主机从连接池中弹出之前的网关错误数。当通过 HTTP 访问上游主机时，502、503 或 504 返回码被视为网关错误。当通过不透明的 TCP 连接访问上游主机时，连接超时和连接错误/失败事件符合网关错误。默认为 0，即禁用此功能 consecutive5xxErrors 主机从连接池中弹出之前的 5xx 错误数。当通过不透明的 TCP 连接访问上游主机时，连接超时、连接错误/失败和请求失败事件被视为 5xx 错误。默认为 5，可以通过将值设置为 0 来禁用注意， consecutiveGatewayErrors 和 consecutive5xxErrors 可以单独使用，也可以一起使用。因为 consecutiveGatewayErrors 统计的错误也包含在 consecutive5xxErrors 中，如果 consecutiveGatewayErrors 的值大于或等于 consecutive5xxErrors 的值，则 consecutiveGatewayErrors 将不起作用 interval 弹出分析的时间间隔。格式：1h/1m/1s/1ms。必须 &gt;=1 ms。默认为 10 s baseEjectionTime 最短弹出时间。被弹出的时间等于最小弹出持续时间与主机被弹出次数的乘积。这种方式下系统将自动增加不健康的上游服务器的弹出周期。格式：1h/1m/1s/1ms。必须 &gt;=1 ms。默认为 30 s maxEjectionPercent 上游服务的负载均衡池中可以弹出的后端的最大百分比。默认为 10% minHealthPercent 只要关联的负载均衡池至少有 minHealthPercent 主机处于健康模式，就会启用异常值检测。当负载均衡池中健康主机的百分比低于此阈值时，将禁用异常值检测，并且代理将在池中的所有主机（健康和不健康）之间进行负载均衡。可以通过将阈值设置为 0% 来禁用该阈值。默认值为 0%，因为它通常不适用于每个服务的 pod 很少的 k8s 环境 ClientTLSSettings上游连接的 SSL/TLS 相关设置。有关更多详细信息，参阅 Envoy 的 TLS 上下文。对 HTTP 和 TCP 上游都是通用的。 例如，以下规则将客户端配置为使用双向 TLS（MUTUAL）连接到上游数据库集群。 123456789101112apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: db-mtlsspec: host: mydbserver.prod.svc.cluster.local trafficPolicy: tls: mode: MUTUAL clientCertificate: /etc/certs/myclientcert.pem privateKey: /etc/certs/client_private_key.pem caCertificates: /etc/certs/rootcacerts.pem 以下规则将客户端配置为在与 rating 服务通信时使用 Istio 双向 TLS。 123456789apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: ratings-istio-mtlsspec: host: ratings.prod.svc.cluster.local trafficPolicy: tls: mode: ISTIO_MUTUAL Field Description mode 是否应使用 TLS 保护与此端口的连接。该字段的值决定了 TLS 的行为方式 clientCertificate 如果 mode 为 MUTUAL 时，则为必需。表示使用的客户端 TLS 证书的文件的路径。如果模式为 ISTIO_MUTUAL，则应为空 privateKey 如果 mode 为 MUTUAL 时，则为必需。保存客户端私钥的文件的路径。如果模式为 ISTIO_MUTUAL，则应为空 caCertificates 用于验证提供的服务器证书的证书颁发机构证书的文件的路径。如果省略，代理将不会验证服务器的证书。如果模式为 ISTIO_MUTUAL，则应为空 credentialName 保存客户端 TLS 证书的密钥的名称，包括 CA 证书。 Secret 必须与使用证书的代理存在于同一命名空间中。密钥（通用类型）应包含以下键和值：key：&lt;privateKey&gt;、cert：&lt;clientCert&gt;、cacert：&lt;CACertificate&gt;。这里 CACertificate 用于验证服务器证书。还支持用于客户端证书的 tls 类型的密钥以及用于 CA 证书的 ca.crt 密钥。只能指定客户端证书和 CA 证书或 credentialName 中的一个注意：仅当 DestinationRule 指定了工作负载选择器时，此字段才适用于 sidecar。否则该字段将仅适用于网关，sidecar 将继续使用证书路径 subjectAltNames 用于验证证书中主体身份的备用名称列表。如果指定，代理将验证服务器证书的主题 alt 名称是否与指定值之一匹配。如果指定，此列表将覆盖 ServiceEntry 中的 subject_alt_names 的值。如果未指定，则将根据下游 HTTP 主机/授权标头自动验证新上游连接的上游证书，前提是 VERIFY_CERT_AT_CLIENT 和 ENABLE_AUTO_SNI 环境变量设置为 true sni 在 TLS 握手期间呈现给服务器的 SNI 字符串。如果未指定，则 SNI 将根据 SIMPLE 和 MUTUAL TLS 模式的下游 HTTP host/authority header 自动设置，前提是 ENABLE_AUTO_SNI 环境变量设置为 true insecureSkipVerify InsecureSkipVerify 指定代理是否应该跳过验证主机对应的服务器证书的 CA 签名和 SAN。仅当启用全局 CA 签名验证，VerifyCertAtClient 环境变量设置为 true，但不需要对特定主机进行验证时，才应设置此标志。无论是否启用了 VerifyCertAtClient，都将跳过 CA 签名和 SAN 的验证InsecureSkipVerify 默认为 false。在 Istio 1.9 版本中，VerifyCertAtClient 默认为 false，但在以后的版本中默认为 true LocalityLoadBalancerSetting位置加权负载均衡允许根据流量的来源和终止位置来控制到 endpoint 的流量分配。这些地区是使用任意标签指定的，这些标签以 {region}/{zone}/{sub-zone} 形式指定地区层次结构。有关更多详细信息，参阅局部权重。 以下示例显示如何在网格范围内设置局部权重：在一个网格中，其工作负载中的服务部署到了 us-west/zone1/ 和 us-west/zone2/ 中。当访问服务的流量源自 us-west/zone1/ 中时，80% 的流量将发送到 us-west/zone1/ 的 endpoint，即同一区域，其余的 20% 将进入 us-west/zone2/ 的 endpoint。此设置旨在将流量路由到同一位置的 endpoint。为源自 us-west/zone2/ 的流量指定了类似的设置。 123456789distribute: - from: us-west/zone1/* to: &quot;us-west/zone1/*&quot;: 80 &quot;us-west/zone2/*&quot;: 20 - from: us-west/zone2/* to: &quot;us-west/zone1/*&quot;: 20 &quot;us-west/zone2/*&quot;: 80 如果运营商的目标不是跨区域和区域分配负载，而是限制故障转移的区域性以满足其他运营要求，则运营商可以设置“故障转移”策略而不是“分布”策略。 以下示例为区域设置本地故障转移策略。假设服务驻留在 us-east、us-west 和 eu-west 内的区域中，此示例指定当 us-east 内的 endpoint 变得不健康时，流量应故障转移到 eu-west 内的 endpoint，类似地 us-west 应该故障转移到 us-east。 12345failover: - from: us-east to: eu-west - from: us-west to: us-east Field Description distribute 只能设置 distribute、failover 或 failoverPriority 中的其一。指定跨不同区域和地理位置的负载平衡权重。参考 Locality weighted load balancing。如果为空，则根据其中的 endpoints 数量设置 locality 权重 failover 只能设置 distribute、failover 或 failoverPriority 中的其一。当本地区域的 endpoint 变得不健康时，显式指定待转移流量的区域。应与 OutlierDetection 一起使用以检测不健康的 endpoint注意：生效的前提是指定了 OutlierDetection failoverPriority failoverPriority 是标签的有序列表，用于对 endpoint 进行排序以进行基于优先级的负载平衡。这是为了支持跨不同 endpoint 组的流量故障转移。假设总共指定了 N 个标签：- 与客户端代理匹配所有 N 个标签的 endpoint 具有优先级 P(0)，即最高优先级- 将前 N-1 个标签与客户端代理匹配的 endpoint 具有优先级 P(1)，即第二高优先级- 通过扩展此逻辑，仅与客户端代理匹配第一个标签的 endpoint 具有优先级 P(N-1)，即第二低优先级- 所有其他 endpoint 具有优先级 P(N)，即最低优先级注意：对于要考虑匹配的标签，之前的标签必须匹配，即仅当前 n-1 个标签匹配时，才会认为第 n 个标签匹配它可以是在客户端和服务器工作负载上指定的任何标签。还支持以下具有特殊语义含义的标签- topology.istio.io/network 用于匹配 endpoint 的网络元数据，可以通过 pod/namespace 标签 topology.istio.io/network、sidecar env ISTIO_META_NETWORK 或 MeshNetworks 指定- topology.istio.io/cluster 用于匹配一个 endpoint 的clusterID，可以通过 pod label topology.istio.io/cluster 或pod env ISTIO_META_CLUSTER_ID 指定- topology.kubernetes.io/region 用于匹配 endpoint 的区域元数据，映射到 Kubernetes 节点标签 topology.kubernetes.io/region 或 failure-domain.beta.kubernetes.io/region（已弃用）- topology.kubernetes.io/zone 用于匹配 endpoint 的 zone 元数据，映射到 Kubernetes 节点标签 topology.kubernetes.io/zone 或 failure-domain.beta.kubernetes.io/zone（已弃用）- topology.istio.io/subzone 用于匹配 endpoint 的子区域元数据，映射到 Istio 节点标签 topology.istio.io/subzone拓扑配置遵循以下优先级：failoverPriority- topology.istio.io/network- topology.kubernetes.io/region- topology.kubernetes.io/zone- topology.istio.io/subzone即 endpoint 和客户端代理的 [network, region, zone, subzone] label 的匹配度越高，则优先级越高只能设置 distribute、failover 或 failoverPriority 中的其一。并且应该和 OutlierDetection 一起使用来检测不健康的 endpoint，否则没有效果 enabled 是否启用局部负载平衡，为 DestinationRule 级别，将完全覆盖网格范围的设置例如 true 表示无论网格范围的设置是什么，都为此 DestinationRule 打开局部负载平衡 TrafficPolicy.PortTrafficPolicy Field Description port 指定应用此策略的目标服务上的端口号 loadBalancer 控制负载均衡器算法的设置 connectionPool 控制与上游服务的连接量的设置 outlierDetection 控制从负载均衡池中逐出不健康主机的设置 tls 与上游服务连接的 TLS 相关设置 TrafficPolicy.TunnelSettings Field Description protocol 指定用于隧道下行连接的协议。支持的协议有：connect - 使用 HTTP CONNECT； post - 使用 HTTP POST。上游请求的 HTTP 版本由为代理定义的服务协议确定 targetHost 指定下游连接通过隧道连接到的主机。目标主机必须是 FQDN 或 IP 地址 targetPort 指定下游连接通过隧道连接到的端口 LoadBalancerSettings.ConsistentHashLB一致的基于哈希的负载均衡可用于提供基于 HTTP 标头、cookie 或其他属性的软会话亲和性。当从目标服务中添加/删除一个或多个主机时，与特定目标主机的关联将被移除。 Field Description httpHeaderName 基于特定 HTTP 标头的 hash httpCookie 基于 HTTP cookie 的 hash useSourceIp 基于源 IP 地址的 hash。适用于 TCP 和 HTTP 连接 httpQueryParameterName 基于特定 HTTP query 参数的 hash minimumRingSize 用于哈希环的最小虚拟节点数。默认为 1024。较大的环尺寸会产生更精细的负载分布。如果负载均衡池中的主机数量大于环大小，则每个主机将被分配一个虚拟节点 LoadBalancerSettings.ConsistentHashLB.HTTPCookie描述将用作一致哈希负载均衡器的哈希键的 HTTP cookie。如果 cookie 不存在，则会自动生成 Field Description name cookie 的名称 path 为 cookie 设置的路径 ttl cookie 的生命周期 ConnectionPoolSettings.TCPSettingsHTTP 和 TCP 上游连接通用的设置。 Field Description maxConnections 到目标主机的最大 HTTP1 /TCP 连接数。默认 2^32-1 connectTimeout TCP 连接超时。格式：1h/1m/1s/1ms。必须 &gt;=1ms。默认为 10s tcpKeepalive 如果设置，则在套接字上设置 SO_KEEPALIVE 以启用 TCP Keepalive ConnectionPoolSettings.HTTPSettings适用于 HTTP1.1/HTTP2/GRPC 连接的设置。 Field Description http1MaxPendingRequests 目标的最大挂起 HTTP 请求数。默认 2^32-1 http2MaxRequests 后端的最大请求数。默认 2^32-1 maxRequestsPerConnection 每个连接到后端的最大请求数。将此参数设置为 1 将禁用 alive。默认为 0，表示无限制，最大为 2^29 maxRetries 在给定时间可以对集群中的所有主机进行的最大重试次数。默认为 2^32-1 idleTimeout 上游连接池连接的空闲超时。空闲超时定义为没有 alive 请求的时间段。如果未设置，则默认为 1 小时。当达到空闲超时时，连接将被关闭。如果连接是 HTTP/2 连接，则会在关闭连接之前发生耗尽序列。请注意，基于请求的超时意味着 HTTP/2 PING 不会使连接保持活动状态。适用于 HTTP1.1 和 HTTP2 连接 h2UpgradePolicy 指定是否应将关联目标的 http1.1 连接升级到 http2 useClientProtocol 如果设置为 true，则启动与后端的连接时将保留客户端协议。请注意，当设置为 true 时，h2UpgradePolicy 将无效，即客户端连接不会升级到 http2 ConnectionPoolSettings.TCPSettings.TcpKeepalive Field Description probes 在确定连接已死之前，要发送且无响应的最大保活探测数。默认是使用操作系统级别的配置（除非被覆盖，Linux 默认为 9) time 在开始发送 keep-alive 探测之前，连接需要空闲的持续时间。默认是使用操作系统级别的配置（除非被覆盖，Linux 默认为 7200s） interval 保持活动探测之间的持续时间。默认是使用操作系统级别的配置（除非被覆盖，Linux 默认为 75s） LocalityLoadBalancerSetting.Distribute描述源自 from 区域或子区域的流量如何分布在一组 to 区域上。指定区域的语法是 {region}/{zone}/{sub-zone} 并且规范的任何部分都允许使用终端通配符。例如： * 匹配所有的地区 us-west/* 匹配 us-west 区域内的所有区域和子区域 us-west/zone-1/* 匹配 us-west/zone-1 中的所有子区域 Field Description from 源位置，用 / 分隔，例如 region/zone/sub_zone to 上游地区到流量分布权重的地图。所有权重的总和应为 100。任何不存在的位置都不会收到流量 LocalityLoadBalancerSetting.Failover指定跨区域的流量故障转移策略。由于默认情况下支持区域和子区域故障转移，因此仅当运营商需要限制流量故障转移时才需要为区域指定，以便全局故障转移到任何 endpoint 的默认行为不适用。这在跨区域故障转移流量不会改善服务健康或可能需要因监管控制等其他原因而受到限制时非常有用。 Field Description from 源区域 to 当 from 区域中的 endpoint 变得不健康时，流量将故障转移到目标区域 LoadBalancerSettings.SimpleLB无需调整的标准负载均衡算法。 Name Description UNSPECIFIED 用户未指定负载均衡算法。 Istio 将选择适当的默认值 RANDOM 随机负载均衡器选择一个随机的健康主机。如果没有配置健康检查策略，随机负载均衡器的性能通常比轮询更好 PASSTHROUGH 此选项会将连接转发到调用者请求的原始 IP 地址，而不进行任何形式的负载平衡。慎重使用。它适用于高级用例。有关更多详细信息，参阅 Envoy 中的原始目标负载均衡器 ROUND_ROBIN 一个基本的循环负载均衡策略。这对于许多场景（例如，使用 endpoint 加权时）通常是不安全的，因为它可能会使 endpoint 负担过重。一般来说，倾向于使用 LEAST_REQUEST 替代 ROUND_ROBIN LEAST_REQUEST 最少请求负载均衡器将负载分散到各个 endpoint，倾向于具有最少未完成请求的 endpoint。这通常更安全，并且几乎在所有情况下都优于 ROUND_ROBIN。倾向于使用 LEAST_REQUEST 替代 ROUND_ROBIN LEAST_CONN 已弃用。改用 LEAST_REQUEST ConnectionPoolSettings.HTTPSettings.H2UpgradePolicy将 http1.1 连接升级到 http2 的策略。 Name Description DEFAULT 使用全局默认值 DO_NOT_UPGRADE 不将连接升级到 http2。会覆盖默认值 UPGRADE 将连接升级到 http2。会覆盖默认值 ClientTLSSettings.TLSmode Name Description DISABLE 不设置到上游 endpoint 的 TLS 连接 SIMPLE 发起到上游 endpoint 的 TLS 连接 MUTUAL 通过提供客户端证书进行身份验证，使用双向 TLS 保护与上游的连接 ISTIO_MUTUAL 通过提供客户端证书进行身份验证，使用双向 TLS 保护与上游的连接。与 MUTUAL 模式相比，该模式使用 Istio 自动生成的证书进行 mTLS 身份验证。使用此模式时，ClientTLSSettings 中的所有其他字段都应为空","link":"/2022/08/09/2022-08-09%20Istio%20%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86%20-%20Destination%20Rule/"},{"title":"「 Istio 」流量管理 — ServiceEntry","text":"based on 1.15.0 ServiceEntry 允许将外部的服务添加到 Istio 的内部服务注册表中，以便网格中的服务可以访问/路由到这些手动指定的服务。ServiceEntry 描述了服务的属性（DNS 名称、VIP、端口、协议、端点）。这些服务可能在网格外部（例如，Web API）或网格内部服务，它们不属于平台的服务注册表（例如，一组与 Kubernetes 中的服务通信的 VM）。此外，还可以使用 workloadSelector 字段动态选择 ServiceEntry 的 endpoint。这些 endpoint 可以是使用 WorkloadEntry 对象或 Kubernetes Pod 声明的 VM 工作负载。在单个服务下同时选择 Pod 和 VM 的能力允许将服务从 VM 迁移到 Kubernetes，而无需更改与服务关联的现有 DNS 名称。 以下示例中声明了一些由内部应用程序通过 HTTPS 访问的外部 API。 Sidecar 检查 ClientHello 消息中的 SNI 值以路由到适当的外部服务。 123456789101112131415apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: external-svc-httpsspec: hosts: - api.dropboxapi.com - www.googleapis.com - api.facebook.com location: MESH_EXTERNAL ports: - number: 443 name: https protocol: TLS resolution: DNS 以下配置将一组运行在非托管 VM 上的 MongoDB 实例添加到 Istio 的注册表中，以便可以将这些服务视为网格中的服务。关联的 DestinationRule 用于启动与数据库实例的 mTLS 连接。 123456789101112131415161718apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: external-svc-mongoclusterspec: hosts: - mymongodb.somedomain # not used addresses: - 192.192.192.192/24 # VIPs ports: - number: 27018 name: mongodb protocol: MONGO location: MESH_INTERNAL resolution: STATIC endpoints: - address: 2.2.2.2 - address: 3.3.3.3 相关联的 DestinationRule。 123456789101112apiVersion: networking.istio.io/v1beta1kind: DestinationRulemetadata: name: mtls-mongoclusterspec: host: mymongodb.somedomain trafficPolicy: tls: mode: MUTUAL clientCertificate: /etc/certs/myclientcert.pem privateKey: /etc/certs/client_private_key.pem caCertificates: /etc/certs/rootcacerts.pem 以下示例结合使用 VirtualService 中的 ServiceEntry 和 TLS 路由，根据 SNI 值将流量引导至内部出口防火墙。 1234567891011121314apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: external-svc-redirectspec: hosts: - wikipedia.org - &quot;*.wikipedia.org&quot; location: MESH_EXTERNAL ports: - number: 443 name: https protocol: TLS resolution: NONE 基于 SNI 值路由的相关联的 VirtualService。 12345678910111213141516apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: tls-routingspec: hosts: - wikipedia.org - &quot;*.wikipedia.org&quot; tls: - match: - sniHosts: - wikipedia.org - &quot;*.wikipedia.org&quot; route: - destination: host: internal-egress-firewall.ns1.svc.cluster.local 具有 TLS 匹配的 Virtual Service 用于覆盖默认的 SNI 匹配。在没有 Virtual Service 的情况下，流量将被转发到 wikipedia 。 以下示例中演示了使用专用出口网关，通过该网关转发所有外部服务流量。 “exportTo” 字段允许控制服务声明对网格中其他命名空间的可见性。默认情况下，服务会导出到所有命名空间。下面的例子限制了当前命名空间的可见性，用 “.” 表示，所以它不能被其他命名空间使用。 12345678910111213141516apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: external-svc-httpbin namespace : egressspec: hosts: - example.com exportTo: - &quot;.&quot; location: MESH_EXTERNAL ports: - number: 80 name: http protocol: HTTP resolution: DNS 定义一个 Gateway 来处理所有出口流量。 123456789101112131415apiVersion: networking.istio.io/v1beta1kind: Gatewaymetadata: name: istio-egressgateway namespace: istio-systemspec: selector: istio: egressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - &quot;*&quot; 关联的 VirtualService 从 Sidecar 路由到网关服务（istio-egressgateway.istio-system.svc.cluster.local），同样的从 Gateway 路由到外部服务。VirtualService 被导出到所有命名空间，使它们能够通过 Gateway 将流量路由到外部服务。像这样强制流量通过托管中间代理是一种常见做法。 12345678910111213141516171819202122232425262728apiVersion: networking.istio.io/v1beta1kind: VirtualServicemetadata: name: gateway-routing namespace: egressspec: hosts: - example.com exportTo: - &quot;*&quot; gateways: - mesh - istio-egressgateway http: - match: - port: 80 gateways: - mesh route: - destination: host: istio-egressgateway.istio-system.svc.cluster.local - match: - port: 80 gateways: - istio-egressgateway route: - destination: host: example.com 以下示例演示了在主机中为外部服务使用通配符。如果必须将连接路由到应用程序请求的 IP 地址（即应用程序解析 DNS 并尝试连接到特定 IP），则必须将 resolution 设置为 NONE。 12345678910111213apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: external-svc-wildcard-examplespec: hosts: - &quot;*.bar.com&quot; location: MESH_EXTERNAL ports: - number: 80 name: http protocol: HTTP resolution: NONE 以下示例演示了可通过客户端主机上的 Unix 域套接字获得的服务。resolution 必须设置为 STATIC 才能使用 Unix 地址 endpoint。 123456789101112131415apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: unix-domain-socket-examplespec: hosts: - &quot;example.unix.local&quot; location: MESH_EXTERNAL ports: - number: 80 name: http protocol: HTTP resolution: STATIC endpoints: - address: unix:///var/run/example/socket 对于基于 HTTP 的服务，可以创建一个由多个 DNS 可寻址 endpoint 支持的 VirtualService。在这种情况下，应用程序可以使用 HTTP_PROXY 环境变量透明地将 VirtualService 的 API 调用重新路由到选定的后端。 例如，以下配置创建了一个名为 foo.bar.com 的不存在的外部服务，后端：us.foo.bar.com:8080、uk.foo.bar.com:9080 和 in.foo.bar.com:7080。 1234567891011121314151617181920212223apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: external-svc-dnsspec: hosts: - foo.bar.com location: MESH_EXTERNAL ports: - number: 80 name: http protocol: HTTP resolution: DNS endpoints: - address: us.foo.bar.com ports: http: 8080 - address: uk.foo.bar.com ports: http: 9080 - address: in.foo.bar.com ports: http: 7080 使用 HTTP_PROXY=http://localhost/，从应用程序到 http://foo.bar.com 的调用将在上面指定的三个域之间进行负载平衡。换句话说，对 http://foo.bar.com/baz 的调用将被转换为 http://uk.foo.bar.com/baz。 以下示例说明了包含 subjectAltNames 的 ServiceEntry 的用法，该名称的格式符合 SPIFFE 标准。 12345678910111213141516171819apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: httpbin namespace : httpbin-nsspec: hosts: - example.com location: MESH_INTERNAL ports: - number: 80 name: http protocol: HTTP resolution: STATIC endpoints: - address: 2.2.2.2 - address: 3.3.3.3 subjectAltNames: - &quot;spiffe://cluster.local/ns/httpbin-ns/sa/httpbin-service-account&quot; 以下示例演示了使用 ServiceEntry 和 workloadSelector 来处理服务 details.bookinfo.com 从 VM 到 Kubernetes 的迁移。该服务有两个基于 VM 的实例和 Sidecar，以及一组由标准部署对象管理的 Kubernetes Pod。网格中此服务的使用者将在 VM 和 Kubernetes 之间自动进行负载平衡。用于 details.bookinfo.com 服务的 VM。此 VM 已使用 details-legacy Service Account 安装和引导 Sidecar。 Sidecar 在端口 80 上接收 HTTP 流量（包装在 istio 双向 TLS 中）并将其转发到同一端口上 localhost 上的应用程序。 123456789101112131415161718192021apiVersion: networking.istio.io/v1beta1kind: WorkloadEntrymetadata: name: details-vm-1spec: serviceAccount: details address: 2.2.2.2 labels: app: details instance-id: vm1---apiVersion: networking.istio.io/v1beta1kind: WorkloadEntrymetadata: name: details-vm-2spec: serviceAccount: details address: 3.3.3.3 labels: app: details instance-id: vm2 假设还有一个 Deployment 带有 Pod 标签 app: details 使用相同的 ServiceAccount（即 details），以下 ServiceEntry 声明了一个跨 VM 和 Kubernetes 的服务： 12345678910111213141516apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: details-svcspec: hosts: - details.bookinfo.com location: MESH_INTERNAL ports: - number: 80 name: http protocol: HTTP resolution: STATIC workloadSelector: labels: app: details ServiceEntry Field Description hosts 与 ServiceEntry 关联的主机。可以是带有通配符前缀的 DNS 名称。- hosts 字段用于在 VirtualServices 和 DestinationRules 中选择匹配的主机- 对于 HTTP 流量，HTTP Host/Authority 标头将与 hosts 字段匹配- 对于包含服务器名称指示（SNI） 的 HTTPs 或 TLS 流量，SNI 值将与 hosts 字段匹配NOTE：- 当解析设置为 DNS 类型且未指定 endpoint 时，主机字段将用作将流量路由到的 endpoint 的 DNS 名称- 如果主机名与另一个服务注册中心的服务名称匹配，例如 Kubernetes，它也提供自己的一组 endpoint，则 ServiceEntry 将被视为现有 Kubernetes 服务的装饰器。如果适用，Service Entry 中的属性将添加到 Kubernetes 服务中。目前，istiod 只会考虑以下附加属性：- subjectAltNames：除了验证与服务的 pod 关联的服务帐户的 SAN 之外，还将验证此处指定的 SAN addresses 与服务关联的虚拟 IP 地址。可能是 CIDR 前缀。对于 HTTP 流量，生成的路由配置将包括地址和主机字段值的 http 路由域，并且将根据 HTTP Host/Authority 标头识别目标。如果指定了一个或多个 IP 地址，如果目标 IP 与地址字段中指定的 IP/CIDR 匹配，则传入流量将被标识为属于此服务。如果地址字段为空，则将仅根据目标端口识别流量。在这种情况下，服务被访问的端口不能被网格中的任何其他服务共享。换句话说，Sidecar 将充当简单的 TCP 代理，将指定端口上的传入流量转发到指定的目标 endpoint IP/主机。此字段不支持 Unix 域套接字地址 ports 与外部服务关联的端口。如果 endpoint 是 Unix 域套接字地址，则必须只有一个端口 location 指定是否应将服务视为网格外部或网格的一部分 resolution 主机的服务发现模式。为没有附带 IP 地址的 TCP 端口设置解析模式为 NONE 时需要注意，在这种情况下，将允许到所述端口上的任何 IP 的流量（即 0.0.0.0:&lt;port&gt;） endpoints 与服务关联的一个或多个 endpoint。只能指定 endpoints 或 workloadSelector 之一 workloadSelector 仅适用于 MESH_INTERNAL 服务。只能指定 endpoint 或工作负载选择器之一。根据标签选择一个或多个 Kubernetes Pod 或 VM 工作负载（使用 WorkloadEntry 指定）。表示 VM 的 WorkloadEntry 对象应与 ServiceEntry 定义在相同的命名空间中 exportTo 此服务导出到的命名空间列表。导出服务允许它被其他命名空间中定义的 Sidecar、Gateway 和 VirtualService 使用。此功能为服务所有者和网格管理员提供了一种机制来控制跨命名空间边界的服务的可见性。如果未指定命名空间，则默认将服务导出到所有命名空间。 ”.” 保留语义表示导出到声明服务的同一命名空间。类似地，“*” 保留语义定义导出到所有命名空间。对于 Kubernetes Service，可以通过将注解 networking.istio.io/exportTo 设置为以逗号分隔的命名空间名称列表来实现等效效果 subjectAltNames 如果指定，代理将验证服务器证书的 subject alternate name 是否与指定值之一匹配。注意：将 workloadEntry 与 workloadSelector 一起使用时，workloadEntry 中指定的 ServiceAccount 也将用于派生应验证的其他 subject alternate name ServiceEntry.Locationlocation 指定服务是 Istio 网格的一部分还是网格之外。location 决定了几个特性的行为，例如服务到服务的 mTLS 身份验证、策略执行等。当与网格外的服务通信时，Istio 的 mTLS 身份验证被禁用，并且策略执行在客户端执行，而不是在服务端。 Name Description MESH_EXTERNAL 表示服务在网格外部。通常用于指示通过 API 使用的外部服务 MESH_INTERNAL 表示服务是网格的一部分。通常用于指示作为扩展服务网格以包括非托管基础设施的一部分而显式添加的服务（例如，添加到基于 Kubernetes 的服务网格的虚拟机） ServiceEntry.Resolutionresolution 决定代理将如何解析与服务关联的网络 endpoint 的 IP 地址，以便它可以路由到其中一个。此处指定的解析模式对应用程序如何解析与服务关联的 IP 地址没有影响。应用程序可能仍需要使用 DNS 将服务解析为 IP，以便代理可以捕获出站流量。或者，对于 HTTP 服务，应用程序可以直接与代理通信（例如，通过设置 HTTP_PROXY）来与这些服务通信。 Name Description NONE 假设传入的连接已经被解析（到特定的目标 IP 地址）。此类连接通常使用 IP 表 REDIRECT/eBPF 等机制通过代理进行路由。在执行任何与路由相关的转换后，代理会将连接转发到连接绑定的 IP 地址 STATIC 使用 endpoint 中指定的静态 IP 地址（见下文）作为与服务关联的支持实例 DNS 尝试通过异步查询环境 DNS 来解析 IP 地址。如果未指定 endpoint，则代理将解析主机字段中指定的 DNS 地址（如果未使用通配符）。如果指定了 endpoint，则将解析 endpoint 中指定的 DNS 地址以确定目标 IP 地址。 DNS 解析不能与 Unix 域套接字 endpoint 一起使用 DNS_ROUND_ROBIN 尝试通过异步查询环境 DNS 来解析 IP 地址。与 DNS 不同，DNS_ROUND_ROBIN 仅在需要启动新连接时使用返回的第一个 IP 地址，而不依赖于 DNS 解析的完整结果，即使 DNS 记录频繁更改，与主机建立的连接也将被保留，从而消除了耗尽连接池和连接循环。这最适合必须通过 DNS 访问的大型 Web 规模服务。如果不使用通配符，代理将解析主机字段中指定的 DNS 地址。 DNS 解析不能与 Unix 域套接字 endpoint 一起使用","link":"/2022/08/16/2022-08-16%20Istio%20%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86%20-%20Service%20Entry/"},{"title":"「 Istio 」流量管理 — WorkloadGroup","text":"based on 1.15.0 WorkloadGroup 描述工作负载实例的集合。提供了工作负载实例可用于引导其代理的规范，包括元数据和标识。它仅适用于虚拟机等非 Kubernetes 工作负载，旨在模仿用于 Kubernetes 工作负载的现有 Sidecar 注入和部署规范模型以引导 Istio 代理。 以下示例声明了一个 WorkloadGroup，代表了一组在 bookinfo 命名空间中 reviews 下注册的工作负载集合。标签集将在初始化过程中与每个工作负载实例相关联，端口 3550 和 8080 将与 WorkloadGroup 相关联并使用名为 default Service Account。 1234567891011121314151617181920212223242526272829apiVersion: networking.istio.io/v1alpha3kind: WorkloadGroupmetadata: name: reviews namespace: bookinfospec: metadata: labels: app.kubernetes.io/name: reviews app.kubernetes.io/version: &quot;1.3.4&quot; template: ports: grpc: 3550 http: 8080 serviceAccount: default probe: initialDelaySeconds: 5 timeoutSeconds: 3 periodSeconds: 4 successThreshold: 3 failureThreshold: 3 httpGet: path: /foo/bar host: 127.0.0.1 port: 3100 scheme: HTTPS httpHeaders: - name: Lit-Header value: Im-The-Best WorkloadGroupWorkloadGroup 可以为 bootstrap 指定单个工作负载的属性，并为 WorkloadEntry 提供模板，类似于 Deployment 通过 Pod 模板指定工作负载的属性。一个 WorkloadGroup 可以有多个 WorkloadEntry。 WorkloadGroup 与控制 ServiceEntry 等服务注册表的资源没有关系，因此不会为这些工作负载配置主机名。 Field Description metadata metadata 会作用于对应的所有 WorkloadEntry 中，WorkloadGroup 的 label 设置应位于 metadata 中，而不是 template template 用于生成属于此 WorkloadGroup 的 WorkloadEntry 资源的模板。注意，不应在模板中设置地址和标签字段，空的 serviceAccount 应默认为 default。工作负载身份（mTLS 证书）将使用指定 Service Account 的令牌进行初始化。该组中的 WorkloadEntry 将与工作负载组位于同一命名空间中，并从上述 metadata 字段继承标签和注释 probe ReadinessProbe 描述了用户对其工作负载进行健康检查提供的配置。具体参考 Kubernetes 的语法与逻辑 ReadinessProbe Field Description initialDelaySeconds 容器启动后，进行就绪探测之前的秒数 timeoutSeconds 探测超时的秒数。默认为 1 秒。最小值为 1 秒 periodSeconds 执行探测的频率（以秒为单位）。默认为 10 秒。最小值为 1 秒 successThreshold 探测失败后被视为成功的最小连续成功次数。默认为 1 failureThreshold 探测成功后被视为失败的最小连续失败次数。默认为 3 httpGet 基于 http get 的健康检查 tcpSocket 基于代理是否能够连接的健康检查 exec 基于执行的命令如何退出的健康检查 HTTPHealthCheckConfig Field Description path HTTP 服务器上的访问路径 port endpoint 所在的端口 host 要连接的主机名，默认为 Pod IP scheme HTTP 或者 HTTPS，默认为 HTTP httpHeaders 请求的 header。允许重复的请求头 HTTPHeader Field Description name header 的键 value header 的值 TCPHealthCheckConfig Field Description host 待连接的主机，默认为 localhost port 主机端口 ExecHealthCheckConfig Field Description command 待运行的命令。退出状态为 0 被视为活动/健康，非零表示不健康 WorkloadGroup.ObjectMeta Field Description labels 标签信息 annotations 注解信息","link":"/2022/09/08/2022-09-08%20Istio%20%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86%20-%20Workload%20Group/"},{"title":"「 Istio 」流量管理 — WorkloadEntry","text":"based on 1.15.0 WorkloadEntry 描述单个非 Kubernetes 工作负载的属性，例如 VM 或裸机服务器，因为它被载入到网格中。 WorkloadEntry 必须伴随着 Istio ServiceEntry，它通过适当的标签选择工作负载并为 MESH_INTERNAL 服务（主机名、端口属性等）提供服务定义。 ServiceEntry 对象可以根据服务条目中指定的标签选择器选择多个工作负载条目以及 Kubernetes pod。 当工作负载连接到 istiod 时，自定义资源中的 Status 字段将更新，表示工作负载的健康状况以及其他详细信息，类似于 Kubernetes 更新 Pod 状态的方式。 以下示例为 details.bookinfo.com 服务声明一个表示 VM 的 Workload Entry。此 VM 已使用 details-legacy Service Account 安装和引导 Sidecar。该服务在端口 80 上暴露给网格中的应用程序。到该服务的 HTTP 流量被封装在 Istio 双向 TLS 中，并发送到目标端口 8080 上 VM 上的 sidecar，然后将其转发到同一端口上 localhost 上的应用程序。 1234567891011121314apiVersion: networking.istio.io/v1beta1kind: WorkloadEntrymetadata: name: details-svcspec: # use of the service account indicates that the workload has a # sidecar proxy bootstrapped with this service account. Pods with # sidecars will automatically communicate with the workload using # istio mutual TLS. serviceAccount: details-legacy address: 2.2.2.2 labels: app: details-legacy instance-id: vm1 相关联的 Service Entry： 1234567891011121314151617apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: details-svcspec: hosts: - details.bookinfo.com location: MESH_INTERNAL ports: - number: 80 name: http protocol: HTTP targetPort: 8080 resolution: STATIC workloadSelector: labels: app: details-legacy 以下示例使用其完全限定的 DNS 名称声明相同的 VM 工作负载。服务条目的解析模式应更改为 DNS，表示客户端 Sidecar 应在运行时动态解析 DNS 名称，然后再转发请求。 1234567891011121314apiVersion: networking.istio.io/v1beta1kind: WorkloadEntrymetadata: name: details-svcspec: # use of the service account indicates that the workload has a # sidecar proxy bootstrapped with this service account. Pods with # sidecars will automatically communicate with the workload using # istio mutual TLS. serviceAccount: details-legacy address: vm1.vpc01.corp.net labels: app: details-legacy instance-id: vm1 相关联的 Service Entry： 1234567891011121314151617apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: details-svcspec: hosts: - details.bookinfo.com location: MESH_INTERNAL ports: - number: 80 name: http protocol: HTTP targetPort: 8080 resolution: DNS workloadSelector: labels: app: details-legacy WorkloadEntry Field Description address 不带端口的网络 endpoint 地址。当且仅当 resolution 设置为 DNS 时，才能使用域名，并且必须是完全限定的，没有通配符。Unix 域套接字 endpoint 使用格式：unix:///absolute/path/to/socket。 ports 与 endpoint 关联的 endpoint 集。如果指定了端口映射，则它必须是 servicePortName 到此 endpoint 端口的映射，这样到服务端口的流量将被转发到映射到服务端口名称的 endpoint 端口。如果省略，并且 targetPort 被指定为服务端口规范的一部分，则到服务端口的流量将被转发到指定 targetPort 上的 endpoint 之一。如果未指定 targetPort 和 endpoint 的端口映射，则到服务端口的流量将被转发到同一端口上的 endpoint 之一 labels 与 endpoint 关联的一个或多个标签 network 使 Istio 能够对驻留在同一 L3 域/网络中的 endpoint 进行分组。假定同一网络中的所有 endpoint 彼此可直接访问。当不同网络中的 endpoint 无法直接相互访问时，可以使用 Istio Gateway 建立连接（通常在 Gateway Server 中使用 AUTO_PASSTHROUGH 模式）。这是一种高级配置，通常用于跨越多个集群的 Istio 网格 locality 与 endpoint 关联的位置信息。位置对应于故障域（例如，国家/地区/区域）。任意故障域层次结构可以通过用 / 分隔每个封装故障域来表示。例如，endpoint 在 US、US-East-1 区域、可用区 az-1 内、数据中心机架 r11 中的位置可以表示为 us/us-east-1/az-1/r11。 Istio 将配置 sidecar 以路由到与 sidecar 相同位置的 endpoint。如果本地没有可用的 endpoint，则将选择 endpoint 父本地（但在同一网络 ID 内）。例如，如果同一网络中有两个 endpoint（networkID “n1”），例如 e1 的位置为 us/us-east-1/az-1/r11 和 e2 的位置为 us/us-east-1/az-2 /r12，来自 us/us-east-1/az-1/r11 地区的 Sidecar 将更倾向于来自同一地区的 e1 而不是来自不同地区的 e2。endpoint e2 可以是与 Gateway 关联的 IP（桥接网络 n1 和 n2），也可以是与标准服务 endpoint 关联的 IP weight 与 endpoint 关联的负载平衡权重。具有较高权重的 endpoint 将按比例获得较高的流量 serviceAccount 如果工作负载中存在 Sidecar，则与工作负载关联的 Service Account。Service Account 必须存在于与配置相同的命名空间中（WorkloadEntry 或 ServiceEntry）","link":"/2022/08/23/2022-08-23%20Istio%20%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86%20-%20Workload%20Entry/"},{"title":"「 Kata Containers 」蚂蚁金服分享","text":"Kata Containers Best Practices at Ant Group","link":"/2022/10/14/2022-10-14%20Kata%20Containers%20%E8%9A%82%E8%9A%81%E9%87%91%E6%9C%8D%E5%88%86%E4%BA%AB/"},{"title":"「 Istio 」流量管理 — ProxyConfig","text":"based on 1.15.0 ProxyConfig 暴露代理级别的配置选项。 ProxyConfig 可以基于每个工作负载、每个命名空间或网格范围进行配置。 ProxyConfig 不是必需的资源。 注意：ProxyConfig 中的字段不是动态配置的，更改配置需要重启工作负载才能生效。 对于任何命名空间，包括根配置命名空间，仅对只有一个无 workloadSelector 的 ProxyConfig 资源生效。 对于具有 workloadSelector 的资源，仅对只有一个资源选择任何给定工作负载生效。 对于网格级别配置，ProxyConfig 需部署在 Istio 安装的根配置命名空间 istio-system 中，并且无需设置 workloadSelector。 123456789apiVersion: networking.istio.io/v1beta1kind: ProxyConfigmetadata: name: my-proxyconfig namespace: istio-systemspec: concurrency: 0 image: imageType: distroless 对于命名空间级别的配置，ProxyConfig 需部署在该命名空间 user-namespace 中，并且无需设置 workloadSelector。 1234567apiVersion: networking.istio.io/v1beta1kind: ProxyConfigmetadata: name: my-ns-proxyconfig namespace: user-namespacespec: concurrency: 0 对于工作负载级别配置，在 ProxyConfig 资源上设置 selector 字段： 123456789101112apiVersion: networking.istio.io/v1beta1kind: ProxyConfigmetadata: name: per-workload-proxyconfig namespace: examplespec: selector: matchLabels: app: ratings concurrency: 0 image: imageType: debug 如果定义了与工作负载匹配的 ProxyConfig CR，它将与其 proxy.istio.io/config 注释（如果存在）合并，重复字段内容以 ProxyConfig CR 为准。同样，如果定义了网格范围的 ProxyConfig CR 并设置了 meshConfig.DefaultConfig，则两个资源将合并，重复字段内容以 ProxyConfig CR 为准。 ProxyConfig Field Description selector selector 指定待应用此 ProxyConfig 的一组 Pod/VM。如果未设置，ProxyConfig 资源将应用于其所在命名空间中的所有工作负载 concurrency 要运行的工作线程数。如果未设置，则默认为 2。如果设置为 0，这将被配置为使用机器上的所有内核使用 CPU 请求和限制来选择一个值，限制优先于请求 environmentVariables 代理的其他环境变量。以 ISTIO_META_ 开头的名称将包含在生成的引导配置中并发送到 XDS 服务器 image 指定代理的镜像 ProxyImage用于构造代理镜像 url。格式：${hub}/${image_name}/${tag}-${image_type}，例如：docker.io/istio/proxyv2:1.11.1 或 docker.io/istio/proxyv2:1.11.1-distroless。 Field Description imageType 镜像的类型。可选的有：default、debug、distroless。如果 image 类型已经（例如：centos）发布到指定的 hub，则允许使用其他值。","link":"/2022/09/15/2022-09-15%20Istio%20%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86%20-%20ProxyConfig/"},{"title":"「 Istio 」流量管理 — Sidecar","text":"based on 1.15.0 Sidecar 描述了 sidecar 代理的配置，该代理将入站和出站通信调解到它所连接的工作负载实例。默认情况下，Istio 将对网格中的所有代理进行编程，并使用必要的配置来访问网格中的每个工作负载实例，并接管与工作负载关联的所有端口上的流量。 Sidecar 配置提供了一种微调端口集的方法，代理在转发流量进出工作负载时将接受的协议。此外，可以限制代理在转发来自工作负载实例的出站流量时可以访问的服务集。 网格中的服务和配置被划分成一个或多个命名空间（例如，Kubernetes 命名空间或 CF org/space）。命名空间中的 Sidecar 配置将应用于同一命名空间中的一个或多个工作负载实例，使用 workloadSelector 字段选择。在没有 workloadSelector 的情况下，它将应用于同一命名空间中的所有工作负载实例。在确定要应用于工作负载实例的 Sidecar 配置时，将优先考虑具有选择此工作负载实例的 workloadSelector 的资源，而不是没有任何 workloadSelector 的 Sidecar 配置。 注意点 每个命名空间只能有一个没有 workloadSelector 的 Sidecar 配置，该配置为该命名空间中的所有 Pod 指定默认值。建议对命名空间范围的 sidecar 使用名称 default。如果给定命名空间中存在多个无选择器的 Sidecar 配置，则系统的行为是未定义的。如果具有 workloadSelector 的两个或多个 Sidecar 配置选择相同的工作负载实例，则系统的行为是未定义的 默认情况下，MeshConfig 根命名空间中的 Sidecar 配置将应用于所有没有 Sidecar 配置的命名空间。这个全局默认 Sidecar 配置不应该有任何 workloadSelector 下面的示例在名为 istio-config 的命名空间中声明了一个全局默认 Sidecar 配置，该配置将所有命名空间中的 Sidecar 配置为仅允许出口流量到同一命名空间中的其他工作负载以及 istio-system 命名空间中的服务： 12345678910apiVersion: networking.istio.io/v1beta1kind: Sidecarmetadata: name: default namespace: istio-configspec: egress: - hosts: - &quot;./*&quot; - &quot;istio-system/*&quot; 下面的示例在 prod-us1 命名空间中声明了一个 Sidecar 配置，它覆盖了上面定义的全局默认值，并在命名空间中配置了 Sidecar 以允许出口流量到 prod-us1、prod-apis 和 istio-system 中的命名空间。 1234567891011apiVersion: networking.istio.io/v1beta1kind: Sidecarmetadata: name: default namespace: prod-us1spec: egress: - hosts: - &quot;prod-us1/*&quot; - &quot;prod-apis/*&quot; - &quot;istio-system/*&quot; 以下示例在 prod-us1 命名空间中为所有带有标签 app: rating 的 Pod 声明了 Sidecar 配置，属于 rating.prod-us1 服务。工作负载在端口 9080 上接受入站 HTTP 流量。然后将流量转发到在 Unix 域套接字上侦听的附加工作负载实例。在出口方向，除了 istio-system 命名空间，Sidecar 仅代理 prod-us1 命名空间中服务的 9080 端口的 HTTP 流量。 123456789101112131415161718192021222324apiVersion: networking.istio.io/v1beta1kind: Sidecarmetadata: name: ratings namespace: prod-us1spec: workloadSelector: labels: app: ratings ingress: - port: number: 9080 protocol: HTTP name: somename defaultEndpoint: unix:///var/run/someuds.sock egress: - port: number: 9080 protocol: HTTP name: egresshttp hosts: - &quot;prod-us1/*&quot; - hosts: - &quot;istio-system/*&quot; 如果在没有基于 IPTables 的流量捕获的情况下部署工作负载，则 Sidecar 配置是连接到工作负载实例的代理上的端口的唯一方法。 以下示例在 prod-us1 命名空间中为所有带有 app: productpage 标签的 Pod 声明了 Sidecar 配置，属于 productpage.prod-us1 服务。假设这些 Pod 部署时没有 IPtable 规则（即 istio-init 容器）并且代理中 metadata 的 ISTIO_META_INTERCEPTION_MODE 设置为 NONE，下面的规范允许这些 Pod 在端口 9080 上接收 HTTP 流量（包裹在 Istio 双向 TLS 中）和将其转发到监听 127.0.0.1:8080 的应用程序。它还允许应用程序与 127.0.0.1:3306 上的支持 MySQL 数据库通信，然后将其代理到 mysql.foo.com:3306 上的外部托管 MySQL 服务。 12345678910111213141516171819202122232425apiVersion: networking.istio.io/v1beta1kind: Sidecarmetadata: name: no-ip-tables namespace: prod-us1spec: workloadSelector: labels: app: productpage ingress: - port: number: 9080 # binds to proxy_instance_ip:9080 (0.0.0.0:9080, if no unicast IP is available for the instance) protocol: HTTP name: somename defaultEndpoint: 127.0.0.1:8080 captureMode: NONE # not needed if metadata is set for entire proxy egress: - port: number: 3306 protocol: MYSQL name: egressmysql captureMode: NONE # not needed if metadata is set for entire proxy bind: 127.0.0.1 hosts: - &quot;*/mysql.foo.com&quot; 以及用于路由到 mysql.foo.com:3306 的关联 ServiceEntry： 1234567891011121314apiVersion: networking.istio.io/v1beta1kind: ServiceEntrymetadata: name: external-svc-mysql namespace: ns1spec: hosts: - mysql.foo.com ports: - number: 3306 name: mysql protocol: MYSQL location: MESH_EXTERNAL resolution: DNS 还可以在单个代理中混合和匹配流量捕获模式。例如，考虑内部服务位于 192.168.0.0/16 子网上的设置。因此，在 VM 上设置 IP 表以捕获 192.168.0.0/16 子网上的所有出站流量。假设 VM 在 172.16.0.0/16 子网上有一个额外的网络接口用于入站流量。以下 Sidecar 配置允许 VM 在 172.16.1.32:80（VM 的 IP）上公开一个侦听器，以接收来自 172.16.0.0/16 子网的流量。 注意：VM 中代理上的 ISTIO_META_INTERCEPTION_MODE 元数据可选值有 REDIRECT 或 TPROXY，这意味着当前是基于 IP 表的流量捕获。 12345678910111213141516171819202122232425apiVersion: networking.istio.io/v1beta1kind: Sidecarmetadata: name: partial-ip-tables namespace: prod-us1spec: workloadSelector: labels: app: productpage ingress: - bind: 172.16.1.32 port: number: 80 # binds to 172.16.1.32:80 protocol: HTTP name: somename defaultEndpoint: 127.0.0.1:8080 captureMode: NONE egress: # use the system detected defaults # sets up configuration to handle outbound traffic to services # in 192.168.0.0/16 subnet, based on information provided by the # service registry - captureMode: IPTABLES hosts: - &quot;*/*&quot; 以下示例在 prod-us1 命名空间中为所有带有标签 app: rating 的 Pod 声明了 Sidecar 配置，属于 rating.prod-us1 服务。该服务在端口 8443 上接受入站 HTTPS 流量，并且 sidecar 代理使用给定的服务器证书以一种方式终止 TLS。然后将流量转发到在 Unix 域套接字上侦听的附加工作负载实例。预计将配置 PeerAuthentication 策略，以便在特定端口上将 mTLS 模式设置为“禁用”。在此示例中，在 PORT 80 上禁用了 mTLS 模式。此功能目前是实验性的。 1234567891011121314apiVersion: security.istio.io/v1beta1kind: PeerAuthenticationmetadata: name: ratings-peer-auth namespace: prod-us1spec: selector: matchLabels: app: ratings mtls: mode: STRICT portLevelMtls: 80: mode: DISABLE Sidecar Field Description workloadSelector 用于选择应用此 Sidecar 配置的特定 Pod/VM 集的标准。如果省略，Sidecar 配置将应用于同一命名空间中的所有工作负载实例 ingress Ingress 指定 Sidecar 的配置，用于处理附加工作负载实例的入站流量。如果省略，Istio 将根据从编排平台获得的工作负载信息（例如，暴露的端口、服务等）自动配置 sidecar。如果指定，当且仅当工作负载实例与服务关联时，才会配置入站端口 egress Egress 指定 sidecar 的配置，用于处理从附加工作负载实例到网格中其他服务的出站流量。如果未指定，则从命名空间范围或全局默认 Sidecar 继承系统检测到的默认值 outboundTrafficPolicy 出方向流量策略的配置。如果应用程序使用一个或多个未知的外部服务，将策略设置为 ALLOW_ANY 将导致 sidecar 将来自应用程序的任何未知流量路由到其请求的目的地。如果未指定，则从命名空间范围或全局默认 Sidecar 继承系统检测到的默认值 IstioIngressListener Field Description port 与 listener 关联的端口 bind listener 应绑定到的 IP（IPv4 或 IPv6）。入口 listener 的绑定字段中不允许使用 Unix 域套接字地址。如果省略，Istio 将根据导入的服务和应用此配置的工作负载实例自动配置默认值 captureMode 表示如何捕获（或不捕获）到 listener 的流量 defaultEndpoint 应将流量转发到的 IP 端点或 Unix 域套接字。此配置可用于将到达 sidecar 上的绑定 IP:Port 的流量重定向到应用程序工作负载实例正在侦听连接的 localhost:port 或 Unix 域套接字。不支持任意 IP。格式应该是 127.0.0.1:PORT、[::1]:PORT（转发到 localhost）、0.0.0.0:PORT、[::]:PORT（转发到实例 IP）或 unix:/// 之一path/to/socket（转发到 Unix 域套接字） tls 一组 TLS 相关选项，将在 sidecar 上为来自网格外部的请求启用 TLS 终止。目前仅支持 SIMPLE 和 MUTUAL TLS 模式 IstioEgressListener Field Description port 与 listener 关联的端口。如果使用 Unix 域套接字，请使用 0 作为端口号，并使用有效的协议。如果指定了端口，将用作与导入主机关联的默认目标端口。如果省略端口，Istio 将根据导入的主机推断 listener 端口。请注意，当指定多个出口 listener 时，其中一个或多个侦听器具有特定端口而其他 listener 没有端口，则在 listener 端口上暴露的主机将基于具有最特定端口的 listener bind listener 应绑定到的 IP（IPv4 或 IPv6）或 Unix 域套接字。如果 bind 不为空，则必须指定端口。格式：IPv4 或 IPv6 地址格式或 unix:///path/to/uds 或 unix://@foobar（Linux 抽象命名空间）。如果省略，Istio 将根据导入的服务、应用此配置的工作负载实例和 captureMode 自动配置默认值。如果 captureMode 为 NONE，bind 将默认为 127.0.0.1 captureMode 当绑定地址是 IP 时，captureMode 选项指示如何捕获（或不捕获）到 listener 的流量。对于 Unix 域套接字绑定，captureMode 必须为 DEFAULT 或 NONE hosts listener 以 namespace/dnsName 格式暴露一个或多个服务主机。与 dnsName 匹配的指定命名空间中的服务将被暴露。相应的服务可以是服务注册表中的服务（例如，Kubernetes 或云服务）或使用 ServiceEntry 或 VirtualService 配置指定的服务。还将使用同一命名空间中的任何关联 DestinationRule。应使用 FQDN 格式指定 dnsName，可选择在最左侧的组件中包含通配符（例如 prod/*.example.com）。将 dnsName 设置为 * 以选择指定命名空间中的所有服务（例如 prod/*）。命名空间可以设置为 、. 或 ~，分别表示任何命名空间、当前命名空间或无命名空间。例如，/foo.example.com 从任何可用的命名空间中选择服务，而 ./foo.example.com 仅从 sidecar 的命名空间中选择服务。如果主机设置为 */，Istio 将配置 sidecar 以便能够访问网格中导出到 sidecar 命名空间的每个服务。值 ~/ 可用于完全修剪 Sidecar 的配置，这些 Sidecar 仅接收流量并响应，但不建立自己的出站连接。只能引用导出到 sidecar 命名空间的服务和配置工件（例如，exportTo 的 * 值）。私有配置（例如，exportTo 设置为 .） WorkloadSelectorWorkloadSelector 指定用于确定是否可以将 Gateway、Sidecar、EnvoyFilter、ServiceEntry 或 DestinationRule 配置应用于代理的标准。匹配条件包括与代理关联的元数据、工作负载实例信息（例如附加到 pod/VM 的标签）或代理在初始握手期间提供给 Istio 的任何其他信息。如果指定了多个条件，则需要匹配所有条件才能选择工作负载实例。目前，仅支持基于标签的选择机制。 Field Description labels 一个或多个标签，指示应用配置的一组特定 Pod/VM。标签搜索的范围仅限于资源所在的配置命名空间 OutboundTrafficPolicy Field Description mode 设置 sidecar 的默认行为以处理来自应用程序的出站流量。如果应用程序使用一个或多个先验未知的外部服务，将策略设置为 ALLOW_ANY 将导致边车将来自应用程序的任何未知流量路由到其请求的目的地。强烈建议用户使用 ServiceEntry 配置来显式声明任何外部依赖项，而不是使用 ALLOW_ANY，以便可以监控到这些服务的流量 OutboundTrafficPolicy.Mode Name Description REGISTRY_ONLY 出站流量将仅限于服务注册表中定义的服务以及通过 ServiceEntry 配置定义的服务 ALLOW_ANY 如果目标端口没有服务或 ServiceEntry 配置，则将允许到未知目标的出站流量 CaptureMode Name Description DEFAULT 环境定义的默认捕获模式 IPTABLES 使用 IPtables 重定向捕获流量 NONE 没有流量捕获。当在出口 listener 中使用时，应用程序应与 listener 端口或 Unix 域套接字显式通信。在入口 listener 中使用时，需要注意确保 listener 端口没有被主机上的其他进程使用","link":"/2022/09/16/2022-09-16%20Istio%20%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86%20-%20Sidecar/"},{"title":"「 Kata Containers 」中国移动分享","text":"Kata Containers Performance Exploration and Practice","link":"/2022/10/27/2022-10-27%20Kata%20Containers%20%E4%B8%AD%E5%9B%BD%E7%A7%BB%E5%8A%A8%E5%88%86%E4%BA%AB/"},{"title":"「 Kata Containers 」架构演进","text":"based on 3.0.0 概览在云原生场景中，对容器启动速度、资源消耗、稳定性和安全性的需求不断增加，目前 Kata Containers 运行时相对于其他运行时面临挑战。为了解决这一点，社区提出了一个可靠的、经过现场测试的、安全的 Rust 版本的 kata-runtime。 特性 Turn key solution with builtin Dragonball Sandbox 异步 I/O 以减少资源消耗 用于多种服务、运行时和 hypervisor 的可扩展框架 sandbox 和容器相关联资源的生命周期管理 选择 Rust 的理由之所以选择 Rust，是因为它被设计为一种注重效率的系统语言。与 Go 相比，Rust 进行了各种设计权衡以获得良好的执行性能，其创新技术与 C 或 C++ 相比，提供了针对常见内存错误（缓冲区溢出、无效指针、范围错误）的合理保护、错误检查（确保错误得到处理）、线程安全、资源所有权等。 当 Kata agent 用 Rust 重写时，这些优点得到了验证。基于 Rust 的实现显着减少了内存使用量。 设计架构 内置的 VMM当前 Kata 2.x 架构 如图所示，runtime 和 VMM 是独立的进程。runtime 进程 fork 出 VMM 进程并通过 RPC 进程间通信。通常，进程间交互比进程内交互开销更大且效率更低。同时，还要考虑资源维护成本。例如，在异常情况下进行资源回收时，任何进程的异常都必须被其他组件检测到，并触发相应的资源回收进程。如果有额外的过程，恢复变得更加困难。 如何支持内置的 VMM社区提供了 Dragonball Sandbox，通过将 VMM 的功能集成到 Rust 库中来启用内置的 VMM。可以通过使用该库来执行与 VMM 相关的功能。因为 runtime 和 VMM 在同一个进程中，所以在消息处理速度和 API 同步方面有所改善。还可以保证 runtime 和 VMM 生命周期的一致性，减少资源回收和异常处理维护的复杂度，如图所示： 支持异步为什么需要异步Async 已经为稳定版 Rust 特性 参考：Why Async 和 The State of Asynchronous Rust Async 显著降低了 CPU 和内存开销，尤其是对于具有大量 I/O 绑定类型的任务负载 Async 在 Rust 中是零成本的，这意味着只需应用层面开销，可以不用堆分配和动态调度，大大提高效率 如果使用 Sync Rust 实现 kata-runtime 可能会出现的几个问题 TTRPC 连接线程数太多（TTRPC threads: reaper thread(1) + listener thread(1) + client handler(2)） 每个容器有三个 I/O 线程 在 Sync 模式下，实现超时机制具有挑战性。比如 TTRPC API 交互中，超时机制很难和 Golang 对齐 如何支持 Async kata-runtime 由 TOKIO_RUNTIME_WORKER_THREADS 控制运行 OS 线程，默认为 2 个线程。TTRPC 和容器相关的线程统一运行在tokio 线程中，Timer，File，Netlink 等相关的依赖调用需要切换到 Async。借助 Async，可以轻松支持无阻塞 I/O 和定时器。目前，仅将 Async 用于 kata-runtime。内置的 VMM 保留了 OS 线程，因为它可以保证线程的可控性。 可扩展框架Kata 3.x runtime 设计了 service、runtime、hypervisor 的扩展，结合配置满足不同场景的需求。目前服务提供注册机制，支持多种服务。服务可以通过消息与 runtime 交互。此外，runtime handler 处理来自服务的消息。为了满足二进制支持多个 runtime 和 hypervisor 的需求，启动必须通过配置获取 runtime handler 类型和 hypervisor 类型。 资源管理器在实际使用中，会有各种各样的资源，每个资源都有几个子类型。特别是对于 virtcontainers，资源的每个子类型都有不同的操作。并且可能存在依赖关系，例如 share-fs rootfs 和 share-fs volume 会使用 share-fs 资源将文件共享到 VM。目前，network、share-fs 被视为沙盒资源，rootfs、volume、cgroup 被视为容器资源。此外，社区为每个资源抽象出一个公共接口，并使用子类操作来评估不同子类型之间的差异。 路线图 阶段 1（截至 2022.06）：提供基础特性 阶段 2（截至 2022.09）：提供常用特性 阶段 3：提供全量特性 Class Sub-Class Development Stage Status Service task service Stage 1 ✅ extend service Stage 3 🚫 image service Stage 3 🚫 Runtime handler Virt-Container Stage 1 ✅ Endpoint VETH Endpoint Stage 1 ✅ Physical Endpoint Stage 2 ✅ Tap Endpoint Stage 2 ✅ Tuntap Endpoint Stage 2 ✅ IPVlan Endpoint Stage 2 ✅ MacVlan Endpoint Stage 2 ✅ MACVTAP Endpoint Stage 3 🚫 VhostUserEndpoint Stage 3 🚫 Network Interworking Model Tc filter Stage 1 ✅ MacVtap Stage 3 🚧 Storage Virtio-fs Stage 1 ✅ nydus Stage 2 🚧 device mapper Stage 2 🚫 Cgroup V2 Stage 2 🚧 Hypervisor Dragonball Stage 1 🚧 QEMU Stage 2 🚫 ACRN Stage 3 🚫 Cloud Hypervisor Stage 3 🚫 Firecracker Stage 3 🚫 FAQ service、message dispatcher 和 runtime handler 都是 Kata 3.x runtime 二进制的一部分吗？ 是的。它们是 Kata 3.x 运行时中的组件。它们将被打包成一个二进制文件 service 是一个接口，负责处理任务服务、镜像服务等多种服务 message dispatcher 用于匹配来自服务模块的多个请求 runtime handler 用于处理对沙箱和容器的操作 Kata 3.x runtime 二进制的名称是什么？ 由于 containerd-shim-v2-kata 已经被使用了，目前在内部，社区使用 containerd-shim-v2-rund Kata 3.x 设计是否与 containerd shimv2 架构兼容？ 是的。它旨在遵循 go 版本 kata 的功能。它实现了 containerd shim v2 接口/协议 用户将如何迁移到 Kata 3.x 架构？ 迁移计划将在 Kata 3.x 合并到主分支之前提供 Dragonball 是不是仅限于自己内置的 VMM？ Dragonball 系统是否可以配置为使用外部 Dragonball VMM/hypervisor 工作？ Dragonball 可以用作外部管理程序。然而，在这种情况下，稳定性和性能具有挑战性。内置 VMM 可以优化容器开销，易于维护稳定性。runD 是 runC 的 containerd-shim-v2 对应物，可以运行 Pod/容器。 Dragonball 是一种 microvm/VMM，旨在运行容器工作负载。有时将其称为安全沙箱，而不是 microvm/VMM QEMU、Cloud Hypervisor 和 Firecracker 支持已在计划中，但如何运作。它们在不同的进程中工作吗？ 是的。它们无法像 VMM 中内置的那样工作 upcall 是什么？ upcall 用于热插拔 CPU/内存/MMIO 设备，它解决了两个问题： 避免依赖 PCI/ACPI 避免在 guest 中依赖 udevd 并获得热插拔操作的确定性结果。所以 upcall 是基于 ACPI 的 CPU/内存/设备热插拔的替代方案。如果需要，Kata 社区会与相关社区合作添加对基于 ACPI 的 CPU/内存/设备热插拔的支持 Dbs-upcall 是 VMM 和 guest 之间基于 vsock 的直接通信工具。 upcall 的服务器端是 guest 内核中的驱动程序（此功能需要内核补丁），一旦内核启动，它将开始为请求提供服务。而客户端在 VMM 中，它将是一个线程，通过 uds 与 VSOCK 通信。通过upcall 直接实现了设备的热插拔，避免了ACPI 的虚拟化，将虚拟机的开销降到最低。通过这种直接的通信手段，可能还有许多其他用途。现目前已经开源：https://github.com/openanolis/dragonball-sandbox/tree/main/crates/dbs-upcall 内核补丁适用于 4.19，但它们也适用于 5.15+ 吗？ 向前兼容应该是可以实现的，社区已经将它移植到基于 5.10 的内核 这些补丁是否特定于平台，或者它们是否适用于支持 VSOCK 的任何架构？ 它几乎与平台无关，但一些与 CPU 热插拔相关的是与平台相关的 是否可以使用 loopback VSOCK 将内核驱动程序替换为 guest 中的用户态守护程序？ 需要为热添加的 CPU/内存/设备创建设备节点，因此用户空间守护进程执行这些任务并不容易 upcall 允许 VMM 和 guest 之间进行通信的事实表明，此架构可能与 https://github.com/confidential-containers 不兼容，其中 VMM 应该不知道 VM 内部发生的事情 TDX 还不支持 CPU/内存热插拔 对于基于 ACPI 的设备热插拔，它依赖于 ACPI DSDT 表，guest 内核将在处理这些热插拔事件时执行 ASL 代码来处理。与 ACPI ASL 方法相比，审计基于 VSOCK 的通信应该更容易 单体与内置 VMM 的安全边界是什么？ 它具有虚拟化的安全边界。更多细节将在下一阶段提供","link":"/2022/11/14/2022-11-14%20Kata%20Containers%20%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/"},{"title":"「 Kata Containers 」源码走读 — kata-runtime","text":"based on 3.0.0 kata-runtime 是一个可执行程序，用于运行基于 OCI（Open Container Initiative）构建打包的应用。 src/runtime/cmd/kata-runtime/main.go kata-runtime 本身是基于 urfave/cli 库构建。kata-runtime 包括 7 个子命令：check（kata-check）、env（kata-env）、exec、metrics、factory、direct-volume 和 iptables。 beforeSubcommands source code 如果指定了 –show-default-config-paths 参数，则展示配置文件默认的路径（/etc/kata-containers/configuration.toml 和 /opt/kata/share/defaults/kata-containers/configuration.toml） 判断用户的输入是否需要展示用法（例如 kata-runtime、kata-runtime help、kata-runtime –help 和 kata-runtime -h 等），如果满足条件，则直接展示用法文本，不执行后续流程 解析 –rootless 参数并设置 如果子命令为 check（kata-check），则设置日志级别为 warn；否则，根据 –log 参数创建日志文件（默认为 /dev/null），根据 –log-format 设置日志格式（支持 text 和 json，默认为 text），日志中新增 command 字段标识子命令，提取 context 设置给 logger 将配置文件内容解析并转为 OCI runtime 配置，设置在 context 中，后续的操作中不再解析配置文件 check（kata-check）Kata Containers 的运行环境要求检查 source code 如果指定了 –verbose 参数，则设置日志级别为 Info 如果没有指定 –no-network-checks 参数并且没有声明 KATA_CHECK_NO_NETWORK 环境变量，则借助网络尝试进行 release 版本检查：如果当前用户为 root 则输出 Not running network checks as super user，否则执行 release version 检查 校验当前版本号是否符合 SemVer 版本规范 根据版本解析中的主版本号获取 release URL：如果为 0，则不合法；如果为 1 则获取 1.x 的 release URL；如果为 2 则获取 2.x 的 release URL；如果环境变量中声明了 KATA_RELEASE_URL 则以此为准 检验 release URL 的合法性，除了默认的 1.x 和 2.x 版本之外，其余的均为不合法，因此通过环境变量 KATA_RELEASE_URL 声明的 release URL 也必须为官方默认的 如果当前用户为 root 则返回 No network checks allowed running as super user 错误，否则请求 release URL，根据是否指定了 –include-all-releases 参数，解析符合要求的 release 详情信息 如果指定了 –only-list-releases 参数，则展示所有的 release 详情，不执行后续流程 获取最新的 release，并展示其详情信息 如果指定了 –check-version-only 参数或者 –only-list-releases 参数则不执行后续流程 解析获得 OCI runtime 配置信息，根据使用的 hypervisor 的类别，设置 CPU 类别，获取运行所需的 CPU flags 和内核模块 amd64 根据 /proc/cpuinfo 文件中字符串匹配 GenuineIntel 或 AuthenticAMD 获得其 CPU 类型，x86 架构下支持 Intel 和 AMD 类型 当 CPU 类型为 Intel 时： 根据 CPU flags 中是否含有 “hypervisor” 判断是否运行在 VM 环境中，如果没运行在 VM 中，则需要支持 VMX Unrestricted 模式（用于判断系统环境是否足够新，用以满足运行 Kata Containers，至少是 Westmere） 如果 hypervisor 为 QEMU、Cloud hypervisor、Firecracker 和 Dragonball 时，则要求 CPU 具有 vmx、lm 和 sse4_1 的 flag 特性以及内核模块中 kvm、kvm_intel、vhost、vhost_net 和 vhost_vsock 应启动；如果 hypervisor 为 acrn 时，则要求 CPU 具有 lm 和 sse4_1 的 flag 特性以及内核模块中 vhm_dev、vhost 和 vhost_net 应启动；如果 hypervisor 为 mock 时，则要求 CPU 具有 vmx、lm 和 sse4_1 的 flag 特性 当 CPU 类型为 AMD 时： 无论 hypervisor 的类型，要求 CPU 具有 svm、lm、sse4_1 的 flag 特性以及内核模块中 kvm、kvm_amd、vhost、vhost_net 和 vhost_vsock 应启动 记录以上依赖要求至全局变量中，后续会作为运行环境监测的依据 arm64 arm64 架构下，setCPUtype 不做任何处理，而是采取了相关全局变量硬编码方式 要求内核模块中 kvm、vhost、vhost_net 和 vhost_vsock 应启动，CPU flag 特性无特殊要求 判断当前环境是否满足运行 Kata Containers 要求，满足要求时输出 System is capable of running Kata Containers 如果当前用户为 root，则通过系统调用创建一个最小化的 VM 之后并删除，用以检测当前环境是否能够满足创建 VM 的要求 amd64 当 hypervisor 为 QEMU、Cloud Hypervisor、Firecracker 时，验证流程参考：kvmIsUsable；hypervisor 为 ACRN 时，验证流程参考：acrnIsUsable。满足要求时输出 System can currently create Kata Containers arm64 不区分 hypervisor 类型，验证流程参考：kvmIsUsable 验证是否支持 KVM Extension，验证流程参考：checkKVMExtensions env（kata-env）展示 Kata Containers 的设置信息 source code 调用上述的 setCPUtype，根据 hypervisor 的类别，获取运行所需的 CPU flags 和内核模块 生成 meta 配置项内容，其中 version 固定为 1.0.26 通过配置文件和 OCI Runtime 的信息，生成 runtime、agent、 hypervisor、image、initrd、kernel 配置项内容 通过解析 /proc/version 获取内核版本信息；通过解析 /etc/os-release 或者 /usr/lib/os-release 获取发行版名称和版本信息；通过解析 /proc/cpuinfo 获得 CPU 类别和型号；通过 /dev/vhost-vsock 的存在性，判断是否支持 vhost-sock。此外，汇合内存总量与使用量、CPU 是否满足运行要求等，生成 host 配置项内容 汇总以上配置项内容，根据是否指定 –json 参数（默认为 TOML 格式），格式化展示内容 exec借助 debug console 进入到 VM 中 source code 如果没有指定 –kata-debug-port 参数或者指定为 0，则 debug 端口设置为默认的 1026 校验指定的 sandboxID 参数是否不为空，且正则匹配满足 ^[a-zA-Z0-9][a-zA-Z0-9_.-]+$ 通过 /run/vc/sbs/&lt;sandboxID&gt;/shim-monitor.sock 发送 HTTP GET 请求至 shim server 的 http://shim/agent-url，解析内容获得 sandbox 的 console socket，示例如下 12$ curl --unix-socket /run/vc/sbs/dd2aa45873a9c0f5e1e93fc38cc0e1fe561e79e33aa85be49487162c1ebc7f43/shim-monitor.sock http://shim/agent-urlvsock://4138340623:1024 如果 sandbox 的 console socket 协议为 vsock，则构建成类似 vsock://4138340623:1026 的格式；如果协议为 hvsock，则构建成 hvsock:///run/vc/firecracker/340b412c97bf1375cdda56bfa8f18c8a/root/kata.hvsock:1026 的格式。仅支持此两种协议，建立 grpc 请求链接，用于 VM 内外的通信交互 获取当前进程的 console，将 kata-runtime exec &lt;sandboxID&gt; 的输出流展示到当前 console 中 metrics获取 VM 中暴露的指标信息 source code 校验指定的 sandboxID 参数是否不为空，且正则匹配满足 ^[a-zA-Z0-9][a-zA-Z0-9_.-]+$ 通过 /run/vc/sbs/&lt;sandboxID&gt;/shim-monitor.sock 发送 HTTP GET 请求至 shim server 的 http://shim/metrics，展示请求返回内容 factoryinit初始化 VM factory source code 如果启用 VM cache 特性（即 [factory].vm_cache_number 大于 0），则初始化一个新的 factory（即 fetchOnly 为 false）。启动 cache server，监听 [factory].vm_cache_endpoint（默认为 /var/run/kata-containers/cache.sock） 如果启用 VM template 特性（即 [factory].enable_template 为 true），则初始化一个新的 factory（即 fetchOnly 为 false）；否则视为 VM cache 和 VM template 均未开启前提下调用 kata-runtime factory init，抛出相关报错 destory销毁 VM factory source code 如果启用 VM cache 特性（即 [factory].vm_cache_number 大于 0），则通过 [factory].vm_cache_endpoint（默认为 /var/run/kata-containers/cache.sock）gRPC 调用 cache server 的 Quit，请求关闭 cache server 如果启用 VM template 特性（即 [factory].enable_template 为 true），则获取现有的 factory （即 fetchOnly 为 true），调用 factory 的 CloseFactory，关闭 factory status查询 VM factory 的状态 source code 如果启用 VM cache 特性（即 [factory].vm_cache_number 大于 0），则通过 [factory].vm_cache_endpoint（默认为 /var/run/kata-containers/cache.sock）gRPC 调用 cache server 的 Status，展示请求返回内容 如果启用 VM template 特性（即 [factory].enable_template 为 true），则获取现有的 factory （即 fetchOnly 为 true），输出其是否存在 direct-volume12345678910111213// MountInfo contains the information needed by Kata to consume a host block device and mount it as a filesystem inside the guest VM.type MountInfo struct { // The type of the volume (ie. block) VolumeType string `json:&quot;volume-type&quot;` // The device backing the volume. Device string `json:&quot;device&quot;` // The filesystem type to be mounted on the volume. FsType string `json:&quot;fstype&quot;` // Additional metadata to pass to the agent regarding this volume. Metadata map[string]string `json:&quot;metadata,omitempty&quot;` // Additional mount options. Options []string `json:&quot;options,omitempty&quot;`} 直通卷的操作都是基于 MountInfo 结构，它描述了待直通至 VM 中的卷位于 host 侧的信息详情。 add为指定的 VM 新增直通卷 source code 对指定的 –volume-path 参数进行 URLEncoding 后，拼接成 /run/kata-containers/shared/direct-volumes/&lt;volumePath (base64)&gt; 路径目录 如果该路径不存在，则创建目录层级；如果该路径存在，则判断其是否为目录 将 –mount-info 参数内容持久化到该目录下的 mountInfo.json 文件中 remove删除指定 VM 的直通卷 source code 对指定的 –volume-path 参数进行 URLEncoding 后，拼接成 /run/kata-containers/shared/direct-volumes/&lt;volumePath (base64)&gt; 路径目录 移除该目录 stats获取 VM 中直通卷的文件系统信息 source code 对指定的 –volume-path 参数进行 URLEncoding 后，拼接成 /run/kata-containers/shared/direct-volumes/&lt;volumePath (base64)&gt; 路径目录 遍历目录，获取到 sandboxID（直通卷模式下，该目录中仅有一个 sandboxID 目录与 mountInfo.json 文件，因此名称不为 mountInfo.json 即为 sandboxID） 获取并解析目录中的 mountInto.json 文件内容，得到 mountInfo.Device（即位于 host 上待直通至 VM 中的设备） 通过 /run/vc/sbs/&lt;sandboxID&gt;/shim-monitor.sock 发送 HTTP GET 请求至 shim server 的 http://shim/direct-volume/stats?path=&lt;device&gt;，展示请求返回内容 resize扩容 VM 的直通块设备的卷大小 source code 对指定的 –volume-path 参数进行 URLEncoding 后，拼接成 /run/kata-containers/shared/direct-volumes/&lt;volumePath (base64)&gt; 路径目录 遍历目录，获取到 sandboxID（直通卷模式下，该目录中仅有一个 sandboxID 目录与 mountInfo.json 文件，因此名称不为 mountInfo.json 的即为 sandboxID） 获取并解析目录中的 mountInto.json 文件内容，得到 mountInfo.Device（即位于 host 上待直通至 VM 中的设备） 通过 /run/vc/sbs/&lt;sandboxID&gt;/shim-monitor.sock 发送格式为 application/json 的 HTTP POST 请求至 shim server 的 http://shim/direct-volume/resize，其中请求体包含 mountInfo.Device 和卷扩容后的期望大小 iptablesget获取 VM 中的 iptables 规则 source code 校验指定的 sandboxID 参数是否不为空，且正则匹配满足 ^[a-zA-Z0-9][a-zA-Z0-9_.-]+$ 如果额外指定了 –v6 参数，则 url 为 /ip6tables，否则为 /iptables 通过 /run/vc/sbs/&lt;sandboxID&gt;/shim-monitor.sock 发送 HTTP GET 请求至 shim server 的 http://shim/&lt;url&gt;，展示请求返回内容 set基于指定的文件内容，设置 VM 中的 iptables 规则 source code 校验指定的 sandboxID 参数是否不为空，且正则匹配满足 ^[a-zA-Z0-9][a-zA-Z0-9_.-]+$ 校验指定的 iptables 参数对应的文件是否存在，并读取 iptables 文件内容 如果额外指定了 –v6 参数，则 url 为 /ip6tables，否则为 /iptables 通过 /run/vc/sbs/&lt;sandboxID&gt;/shim-monitor.sock 发送格式为 application/octet-stream 的 HTTP PUT 请求至 shim server 的 http://shim/&lt;url&gt;，其中请求体包含 iptables 文件内容流","link":"/2022/11/26/2022-11-26%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20kata-runtime/"},{"title":"「 Virtual Kubelet 」快速开始","text":"based on v1.7.0 简介 Kubernetes API on top, programmable back. Virtual Kubelet（VK） 是 Kubernetes 中 Kubelet 的典型特性实现，向上伪装成 Kubelet，从而模拟出 Node 对象，对接 Kubernetes 的原生资源对象。向下提供 API，可对接其他资源管理平台提供的 Provider，不同的平台通过实现 Virtual Kubelet 定义的方法，允许节点由其对应的 Provider 提供，如 ACI、AWS Fargate、IoT Edge、Tensile Kube 等。Virtual Kubelet 的主要场景是将 Kubernetes API 扩展到 Serverless 容器平台（如 ACI 和 Fargate）或者扩展到如 Docker Swarm、Openstack ZUN 等容器平台中，也可以通过 Provider 纳管其他 Kubernetes 集群，甚至是原生的 IAAS 平台（如 VMware、Openstack 等。在社区宗旨中，Virtual Kubelet 不是用来实现集群联邦的手段。 Virtual Kubelet 具有可插拔架构和直接使用 Kubernetes 原语的特点，更易于构建。 架构 与传统 Kubelet 的区别 传统的 Kubelet 实现了所在节点的 Pod 和容器的操作行为 Virtual Kubelet 以节点的形式注册，允许开发者以自定义的行为部署 Pod 和容器 当前支持的 Kubernetes 特性 创建、删除和更新 Pod 容器日志、管理和指标 获取 Pod 以及状态 节点地址、节点容量、节点守护进程端点 管理操作系统 携带私有虚拟网络 ProvidersVirtual Kubelet 专注于提供一个库，用户可以在项目中使用该库来构建自定义 Kubernetes 节点 agent。 该项目具有一个可插拔的 Provider 接口，开发者可以实现该接口来定义 Kubelet 的操作。 支持按需和与 Kubernetes 近乎即时的编排容器计算，无需管理 VM 基础设施，同时仍然利用可移植的 Kubernetes API。 每个 Provider 可能有自己的配置文件和所需的环境变量。 Provider 必须具备以下功能才能与 Virtual Kubelet 集成支持： 提供必要的后端服务，用于支持 Kubernetes 的 Pod、容器和支持资源的生命周期管理 符合 Virtual Kubelet 的 API 规范 无法访问 Kubernetes API 服务器，并且具有定义明确的回调机制来获取 Secret 或 ConfigMap 等数据 参考：virtual-kubelet godoc Admiralty Multi-Cluster SchedulerAdmiralty Multi-Cluster Scheduler 将特定的 Pod 运行在 Virtual Kubelet 节点上，作为“代理 Pod”，并在远程集群（实际运行容器）中创建相应的”委托 Pod”。通过控制循环机制，更新代理 Pod 的信息用来反映委托 Pod 的状态。 参考：Admiralty Multi-Cluster Scheduler documentation Alibaba Cloud Elastic Container Instance (ECI)阿里云 ECI（弹性容器实例）是一种无需管理服务器或集群即可运行容器的服务。阿里云 ECI Provider 是连接 K8s和 ECI 服务之间的桥梁。 参考： Alibaba Cloud ECI documentation Azure Container Instances (ACI)Azure 容器实例 Provider 允许在同一个 Kubernetes 集群中同时使用 VM 上的 Pod 和 Azure 容器实例。 参考：Azure Container Instances documentation AWS FargateAWS Fargate 是一种允许运行容器而无需管理服务器或集群的技术。 AWS Fargate 提供商允许将 Pod 部署到 AWS Fargate。在 AWS Fargate 上的 Pod 可以访问具有子网中专用 ENI 的 VPC 网络、连接到互联网的公共 IP 地址、连接到 Kubernetes 集群的私有 IP 地址、安全组、IAM 角色、CloudWatch Logs 和许多其他 AWS 服务。 Fargate 上的 Pod 可以与同一 Kubernetes 集群中常规工作节点上的 Pod 共存。 参考：AWS Fargate documentation Elotl KipKip 是一个在云实例中运行 Pod 的提供商，允许 Kubernetes 集群透明地将工作负载扩展到云中。当一个 Pod 被调度到虚拟节点上时，Kip 会为 Pod 的工作负载启动一个大小合适的云实例，并将 Pod 调度到该实例上。当 Pod 完成运行时，云实例将终止。 当工作负载在 Kip 上运行时，集群大小自然会随着集群工作负载而扩展，Pod 彼此高度隔离，并且用户无需管理工作节点并将 Pod 战略性地调度到节点上。 参考：Elotl Kip documentation Kubernetes Container Runtime Interface (CRI)CRI Provider 是基于 CRI 的容器运行时管理真实的 Pod 和容器。 CRI Provider 的目的仅用于测试和原型。不得用于任何其他目的！ Virtual Kubelet 项目的重点是为不符合标准节点模型的容器运行时提供接口。 而 Kubelet 代码库是全面的标准 CRI 节点代理，并且此 Provider 不会尝试重新创建它。 这个 Provider 实现是一个最基本的最小实现，它可以更容易地针对真实的 Pod 和容器测试 Virtual Kubelet 项目的核心功能 —— 换句话说，它比 MockProvider 更全面。 已知限制 CRI Provider 实现了 Provider 接口全部操作，主要是管理 Pod 的生命周期、返回日志和其他内容 具备创建 emptyDir、configmap 和 secret volumes 的能力，但如果当发生变化时不会更新 不支持任何类型的持久卷 会在启动时尝试运行 kube-proxy，并且可以成功运行。但是，当将 Virtual Kubelet 转换为抽象地处理 Service 和路由的模型时，此功能将被重构为测试该功能的一种方式 网络目前是非功能性的 Huawei Cloud Container Instance (CCI)华为 CCI Virtual Kubelet Provider 将 CCI 项目配置成任何 Kubernetes 集群中的节点，例如华为 CCE（云容器引擎）。CCE 支持原生 Kubernetes 应用和工具作为私有集群，便于轻松搭建容器运行环境。被调度到 Virtual Kubelet Provider 的 Pod 将运行在 CCI 中，便于更好的利用 CCI 的高性能。 参考：Huawei CCI documentation HashiCorp NomadVirtual Kubelet 的 HashiCorp Nomad Provider 通过将 Nomad 集群公开为 Kubernetes 中的一个节点，将Kubernetes 集群与 Nomad 集群连接起来。借助 Provider，在 Kubernetes 上注册的虚拟 Nomad 节点上调度的 Pod 将作为 Job 在 Nomad 客户端上运行，就像在 Kubernetes 节点上一样。 参考：HashiCorp Nomad documentation LiqoLiqo 为 Virtual Kubelet 实现了一个 Provider，旨在透明地将 Pod 和服务卸载到“对等”Kubernetes 远程集群。 Liqo 能够发现邻居集群（使用 DNS、mDNS）并与其“对等”，或者说建立关系以共享集群的部分资源。当集群建立对等连接时，会生成一个新的 Liqo Virtual Kubelet 实例，通过提供远程集群资源的抽象来无缝扩展集群的容量。该提供商与 Liqo 网络结构相结合，通过启用 Pod 到 Pod 流量和多集群东西向服务扩展集群网络，支持两个集群上的端点。 参考：Liqo documentation OpenStack ZunVirtual Kubelet 的 OpenStack Zun Provider 用于将 Kubernetes 集群与 OpenStack 集群打通，从而可以在 OpenStack 上运行 Kubernetes 的 Pod。借助子网中的 Neutron 端口，在 OpenStack 上的 Pod 可以访问 OpenStack 租户网络，每个 Pod 都有私有 IP 地址，可以连接到租户内的其他 OpenStack 资源（例如 VM），也可以借助浮动 IP 地址连接互联网，或者将 Cinder 卷绑定给 Pod 容器使用。 参考：OpenStack Zun documentation Tencent Games Tensile KubeTensile Kube Provider 由腾讯游戏提供，可将 Kubernetes 集群与其他 Kubernetes 集群连接起来。该 Provider 能够将 Kubernetes 集群规模无限扩展。底层集群以虚拟节点的形态注册到上层集群中，借助 Provider，调度在虚拟节点上的 Pod 将在其他 Kubernetes 集群的节点上运行。 架构设计 virtual-node 基于 virtual-kubelet 实现的 Kubernetes Provider。在上层集群中创建的 Pod 将同步到底层集群。如果 Pod 依赖于 ConfigMaps 或 Secret，那么依赖关系也会在集群中创建 multi-cluster scheduler 基于 K8s schedule framework 实现。它会在调度 Pod 时根据所有底层集群的容量，并调用 filter 过滤器。如果可用节点数大于或等于 1，则 Pod 可以被调度。因此，这可能会消耗更多资源，因此添加了另一个实现（descheduler） descheduler descheduler 是基于 K8s descheduler 的二次优化，改变了一些逻辑。它会通过注入一些 nodeAffinity 重新创建一些不可调度的 Pod。可以选择部署上层集群中的 multi-scheduler 和 descheduler 之一，也可以同时选择两者 大规模集群不建议使用 multi-scheduler，当节点总数超过 10000 时，descheduler 开销相对更小 当集群中的节点较少时，multi-scheduler 效果会更好，例如有 10 个集群，每个集群只有 100 个节点 webhook Webhook 是基于 K8s mutation webhook 设计的。用于转换一些可能影响上层集群中调度 Pod（不在 kube-system 中）的字段，例如 nodeSelector、nodeAffinity 和 tolerations。但是只有标签为 virtual-pod:true 的 Pod 才会被转换 强烈建议 Pod 运行在底层集群并添加标签 virtual-pod:true，除非那些 Pod 必须部署在上层集群的 kube-system 中 对于 K8s &lt; 1.16，没有 label 的 pod 不会被转换。但仍会发送请求到 webhook 对于 K8s &gt;= 1.16，可以使用 label selector 为一些指定的 pod 启用 webhook 总的来说，最初的想法是只在底层集群中运行 Pod 限制 如果要使用 Server，必须保持 Pod 间通信正常。集群 A 中的 Pod A 可以被集群 B 中的 Pod B 通过 ip 访问。default 命名空间中的服务 Kubernetes 和 kube-system 中的其他服务将同步到较底层的集群 在 repo 中开发的 multi-scheduler 可能会花费更多资源，因为它会同步所有较底层集群中调度程序需要的所有对象 descheduler 不能绝对避免资源碎片化 PV/PVC 只支持本地 PV 的 WaitForFirstConsumer，上层集群的调度器应该忽略 VolumeBindCheck 用例 参考：Tensile Kube documentation Provider 接口实现Provider 实现了 Kubernetes 节点代理 （即 Kubelet）的核心逻辑。 PodLifecylceHandler用于处理在 Kubernetes 中创建、更新或删除 Pod 的请求。 godoc#PodLifecylceHandler 12345678910111213141516171819type PodLifecycleHandler interface { // CreatePod takes a Kubernetes Pod and deploys it within the provider. CreatePod(ctx context.Context, pod *corev1.Pod) error // UpdatePod takes a Kubernetes Pod and updates it within the provider. UpdatePod(ctx context.Context, pod *corev1.Pod) error // DeletePod takes a Kubernetes Pod and deletes it from the provider. DeletePod(ctx context.Context, pod *corev1.Pod) error // GetPod retrieves a pod by name from the provider (can be cached). GetPod(ctx context.Context, namespace, name string) (*corev1.Pod, error) // GetPodStatus retrieves the status of a pod by name from the provider. GetPodStatus(ctx context.Context, namespace, name string) (*corev1.PodStatus, error) // GetPods retrieves a list of all pods running on the provider (can be cached). GetPods(context.Context) ([]*corev1.Pod, error)} 还有一个可选实现的接口 PodNotifiery，用于 Provider 异步通知 virtual-kubelet 有关 Pod 状态更改的信息。如果没有实现这个接口，virtual-kubelet 会周期性的检查所有 Pod 的状态。 强烈建议实现 PodNotifier，尤其是当运行大量 Pod 时。 godoc#PodNotifier 1234567type PodNotifier interface { // NotifyPods instructs the notifier to call the passed in function when // the pod status changes. // // NotifyPods should not block callers. NotifyPods(context.Context, func(*corev1.Pod))} PodLifecycleHandler 由 PodController 维护，PodController 是管理分配给节点 Pod 的核心逻辑。 12pc, _ := node.NewPodController(podControllerConfig) // &lt;-- instatiates the pod controllerpc.Run(ctx) // &lt;-- starts watching for pods to be scheduled on the node NodeProviderNodeProvider 负责通知 virtual-kubelet 节点状态更新。 virtual-kubelet 会定期检查节点的状态并相应地更新至 Kubernetes。 godoc#NodeProvider 123456789101112131415type NodeProvider interface { // Ping checks if the node is still active. // This is intended to be lightweight as it will be called periodically as a // heartbeat to keep the node marked as ready in Kubernetes. Ping(context.Context) error // NotifyNodeStatus is used to asynchronously monitor the node. // The passed in callback should be called any time there is a change to the // node's status. // This will generally trigger a call to the Kubernetes API server to update // the status. // // NotifyNodeStatus should not block callers. NotifyNodeStatus(ctx context.Context, cb func(*corev1.Node))} NodeProvider 由 NodeController 维护，这是 Kubernetes 管理节点对象的核心逻辑。 12nc, _ := node.NewNodeController(nodeProvider, nodeSpec) // &lt;-- instantiate a node controller from a node provider and a kubernetes node specnc.Run(ctx) // &lt;-- creates the node in kubernetes and starts up he controller godoc#NaiveNodeProvider Virtual Kubelet 提供了一个 NaiveNodeProvider，用于不打算自定义节点行为时。 API endpointsKubelet 的工作之一是接受来自 API Server 的请求，比如 kubectl logs 和 kubectl exec。 如果想在集群中使用 HPA（Horizontal Pod Autoscaler），Provider 应该实现 GetStatsSummary 函数。然后 metrics-server 将能够获取 virtual-kubelet 上的 Pod 的指标。否则，可能会在 metrics-server 上看到 No metrics for pod，这意味着不会收集 virtual-kubelet 上的 Pod 的指标。 已知问题和解决方案Service 缺少 Load Balancer IPProvider 不支持服务发现 Kubernetes 1.9 为控制平面的 Controller Manager 引入了一个新标识 ServiceNodeExclusion。在 Controller Manager 的配置文件中启用此标志允许 Kubernetes 将 Virtual Kubelet 节点排除在添加到负载均衡器池之外，创建具有外部 IP 的 Service。 解决方案 集群要求：Kubernetes 1.9或以上 Controller Manager 配置文件中新增 –feature-gates=ServiceNodeExclusion=true 参数启用 ServiceNodeExclusion 标识。 实践操作以 Tensile Kube Provider 为例。 准备工作 Tensile Kube Provider 是将其他的 Kubernetes 集群以虚拟节点的形式加入到主集群，因此需要准备两个集群。 1234567891011# 上层集群$ kubectl get nodeNAME STATUS ROLES AGE VERSIONdesktop-ca83 Ready control-plane,master 16d v1.22.9 # 底层集群$ kubectl get nodeNAME STATUS ROLES AGE VERSIONarchcnstcm67370 Ready master 2d5h v1.18.15archcnstcm67371 Ready master 2d5h v1.18.15archcnstcm67372 Ready master 2d5h v1.18.15 编译 virtual-node 12$ git clone https://github.com/virtual-kubelet/tensile-kube.git$ cd tensile-kube &amp;&amp; make Provider 部署 Tensile Kube Provider 运行在底层集群中，将其三节点集群以虚拟节点 vk-node 加入到上层集群中。 1234567891011121314151617181920212223242526272829# 底层集群启动 Tensile Kube Provider，其中 kubeconfig 为上层集群的配置文件，client-kubeconfig 为底层集群的配置文件$ ./virtual-node --nodename vk-node --kubeconfig ./config --client-kubeconfig /root/.kube/configI1124 16:47:19.501216 3727733 provider.go:158] Informer startedI1124 16:47:19.902257 3727733 service_controller.go:114] Starting controllerI1124 16:47:19.902298 3727733 common_controller.go:115] Starting controllerI1124 16:47:19.902331 3727733 pv_controller.go:129] Starting controllerERRO[0000] TLS certificates not provided, not setting up pod http server caPath= certPath= keyPath= node=vk-node operatingSystem=Linux provider=k8s watchedNamespace=INFO[0000] Initialized node=vk-node operatingSystem=Linux provider=k8s watchedNamespace=I1124 16:47:19.902447 3727733 node.go:98] Called NotifyNodeStatusI1124 16:47:19.902455 3727733 pod.go:321] Called NotifyPodsINFO[0000] Pod cache in-sync node=vk-node operatingSystem=Linux provider=k8s watchedNamespace=INFO[0000] starting workers node=vk-node operatingSystem=Linux provider=k8s watchedNamespace=INFO[0000] started workers node=vk-node operatingSystem=Linux provider=k8s watchedNamespace=I1124 16:47:20.607794 3727733 service_controller.go:144] enqueue service addkey istio-system/istio-ingressgatewayI1124 16:47:20.607839 3727733 service_controller.go:144] enqueue service addkey istio-system/istiodI1124 16:47:20.607855 3727733 service_controller.go:144] enqueue service addkey velero/minioI1124 16:47:20.607803 3727733 service_controller.go:176] enqueue endpoint add key velero/minioI1124 16:47:20.607878 3727733 service_controller.go:176] enqueue endpoint add key istio-system/istio-ingressgatewayI1124 16:47:20.607887 3727733 service_controller.go:176] enqueue endpoint add key istio-system/istiodI1124 16:47:20.702816 3727733 pv_controller.go:135] Sync caches from master successfullyI1124 16:47:20.702859 3727733 pv_controller.go:140] Sync caches from client successfullyI1124 16:47:20.702816 3727733 service_controller.go:120] Sync caches from master successfullyI1124 16:47:20.702903 3727733 service_controller.go:125] Sync caches from client successfully# 上层集群$ kubectl get nodeNAME STATUS ROLES AGE VERSIONdesktop-ca83 Ready control-plane,master 16d v1.22.9vk-node Ready agent 9s v1.18.15 Provider 可以部署在任意集群中，通过 –kubeconfig 和 –client-kubeconfig 指定上层底层集群即可，也可以组成网状集群或者公用虚拟节点的集群。 调度 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# 上层集群部署的负载信息apiVersion: v1kind: ConfigMapmetadata: name: myconfigmapdata: username: vk-demo---apiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: USER_NAME: YWRtaW4=---apiVersion: v1kind: PersistentVolumemetadata: name: vk-demospec: capacity: storage: 100Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: &quot;&quot; local: path: /tmp/example nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - vk-node---kind: PersistentVolumeClaimapiVersion: v1metadata: name: vk-demo labels: app: nginxspec: # Optional: # storageClassName: &lt;YOUR_STORAGE_CLASS_NAME&gt; storageClassName: &quot;&quot; accessModes: - ReadWriteOnce resources: requests: storage: 50Mi---apiVersion: v1kind: Podmetadata: name: vk-demospec: automountServiceAccountToken: false containers: - name: busybox image: busybox:1.27 imagePullPolicy: IfNotPresent command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;] volumeMounts: - name: foo mountPath: &quot;/etc/foo&quot; readOnly: true - name: bar mountPath: &quot;/etc/bar&quot; readOnly: true - mountPath: /datadir name: local nodeSelector: type: virtual-kubelet tolerations: - key: &quot;virtual-kubelet.io/provider&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; volumes: - name: foo configMap: name: myconfigmap - name: bar secret: secretName: mysecret optional: false - name: local persistentVolumeClaim: claimName: vk-demo 1234567891011121314151617181920212223# 底层集群预先准备可用的卷apiVersion: v1kind: PersistentVolumemetadata: name: vk-demospec: capacity: storage: 100Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: &quot;&quot; local: path: /tmp/example nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - archcnstcm67370 创建负载之后可以看到，原本在上层的 configmap、secret 和 pvc，透传到对应的底层集群创建了一份 12345678910111213# 底层集群$ kubectl get configmapNAME DATA AGEmyconfigmap 1 15m$ kubectl get secretNAME TYPE DATA AGEdefault-token-jxhcg kubernetes.io/service-account-token 3 2d7hmysecret Opaque 1 15m$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEvk-demo Bound vk-demo 100Gi RWO 16m 负载在上层集群和底层集群均可以查询到，但由于两个集群 Pod 网络未打通，因此无法通过上层集群 exec 或者 log 查看 1234567891011121314151617181920I1124 17:52:59.931178 2460220 helper.go:57] pod vk-demo depends on secrets [mysecret]I1124 17:52:59.931202 2460220 helper.go:70] pod vk-demo depends on configMap [myconfigmap]I1124 17:52:59.931210 2460220 helper.go:83] pod vk-demo depends on pvc [vk-demo]I1124 17:52:59.936998 2460220 pod.go:457] Create myconfigmap in default successI1124 17:52:59.937019 2460220 pod.go:79] Create configmaps [myconfigmap] of default/vk-demo successINFO[0028] Created pod in provider INFO[0028] Event(v1.ObjectReference{Kind:&quot;Pod&quot;, Namespace:&quot;default&quot;, Name:&quot;vk-demo&quot;, UID:&quot;1c52b390-29ab-40de-b892-860db9fe3418&quot;, APIVersion:&quot;v1&quot;, ResourceVersion:&quot;3395567&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'ProviderCreateSuccess' Create pod in provider successfully node=vk-node operatingSystem=Linux provider=k8s watchedNamespace=I1124 17:53:00.023488 2460220 pod.go:84] Create pvc [vk-demo] of default/vk-demo successINFO[0029] Updated pod in provider INFO[0029] Event(v1.ObjectReference{Kind:&quot;Pod&quot;, Namespace:&quot;default&quot;, Name:&quot;vk-demo&quot;, UID:&quot;1c52b390-29ab-40de-b892-860db9fe3418&quot;, APIVersion:&quot;v1&quot;, ResourceVersion:&quot;3395570&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'ProviderUpdateSuccess' Update pod in provider successfully node=vk-node operatingSystem=Linux provider=k8s watchedNamespace=INFO[0042] Updated pod in provider INFO[0042] Event(v1.ObjectReference{Kind:&quot;Pod&quot;, Namespace:&quot;default&quot;, Name:&quot;vk-demo&quot;, UID:&quot;1c52b390-29ab-40de-b892-860db9fe3418&quot;, APIVersion:&quot;v1&quot;, ResourceVersion:&quot;3395617&quot;, FieldPath:&quot;&quot;}): type: 'Normal' reason: 'ProviderUpdateSuccess' Update pod in provider successfully node=vk-node operatingSystem=Linux provider=k8s watchedNamespace=$ kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESvk-demo 1/1 Running 0 16m 10.244.0.1 vk-node &lt;none&gt; &lt;none&gt;$ kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESvk-demo 1/1 Running 0 17m 10.244.0.1 archcnstcm67370 &lt;none&gt; &lt;none&gt; 总结 Tensile Kube Provider 底层多节点集群是以一个单独的虚拟节点加入的上层集群中 在上层集群创建的负载，会通过 virtual-node 组件下发到底层集群对应的 api-server 中（可以支持 HA），会根据底层集群的情况进行实际调度 由于工作负载真实运行在底层集群中，其依赖的 Secret、Configmap 和 PVC 等资源同样的会透传给底层集群创建，其运行时的资源由底层集群分配并维护，例如 Pod 的网络、存储等","link":"/2022/11/22/2022-11-22%20Virtual%20Kubelet%20%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/"},{"title":"「 Kata Containers 」源码走读 — kata-monitor","text":"based on 3.0.0 Kata monitor 是一个守护进程，能够收集和暴露在同一 host 上运行的所有 Kata 容器工作负载相关的指标。一旦启动，它会检测 containerd-shim-kata-v2 系统中所有正在运行的 Kata Containers 运行时，并暴露一些 HTTP endpoints。主要 endpoint 是 /metrics（用于聚合来自所有 Kata 工作负载的指标）。 可用指标包括： Kata 运行时指标 Kata agent 指标 Kata guestOS 指标 hypervisor 指标 Firecracker 指标 Kata monitor 指标 Kata monitor 提供的指标均采用 Prometheus 格式。虽然 Kata monitor 可以在任何运行 Kata Containers 工作负载的主机上用作独立守护进程，并且可以用于从正在运行的 Kata 运行时检索分析数据，但它的主要预期用途是作为 DaemonSet 部署在 Kubernetes 集群上。 src/runtime/cmd/kata-monitor/main.go 1234567type KataMonitor struct { // 维护 /run/vc/sbs 目录下的 sandbox 的基础信息，包括 uid、name 和 namespace sandboxCache *sandboxCache // --runtime-endpoint 参数指定，默认为 /run/containerd/containerd.sock runtimeEndpoint string} main 函数 source code 如果指定的参数为 version 或者 –version，则展示其版本、架构、commit 等信息 解构命令行参数与初始化日志 初始化 monitor server 校验 –runtime-endpoint 参数是否不为空，默认为 /run/containerd/containerd.sock 注册指标至 Prometheus 启动 goroutine，实时处理 sandbox cache 启动 fsnotify watcher，监听 /run/vc/sbs 目录下的文件（文件名即为 sandboxID） 遍历目录内容，将 sandboxID 维护在 km.sandboxCache 根据 fsnotify watcher 监听到的创建或者删除事件，同步更新维护在 km.sandboxCache 中的 sandboxID 同时，默认每隔 5 秒，根据 –runtime-endpoint 参数构建 gRPC Client 调用 CRI 的 ListPodSandbox 获取到 CRI 中所有的 PodSandbox，将详细内容（即 sandboxCRIMetadata 对象，其中包含 UID、Name 和 Namespace 属性）更新至 km.sandboxCache 中 注册 /metrics、/sandboxes、/agent-url 和一系列 Golang pprof 的 HTTP 端点；根服务请求（/）会展示所有可用的 HTTP 端点 启动 monitor server 服务，监听地址通过 –listen-address 指定，默认为 127.0.0.1:8090 ProcessMetricsRequest处理 /metrics 请求，获取 shim、hypervisor、vm 和 agent 指标 source code 获取请求中的 sandbox 参数 如果指定了 sandbox 参数，则通过 /run/vc/sbs/&lt;sandboxID&gt;/shim-monitor.sock 发送 HTTP GET 请求至 shim server 的 http://shim/metrics，获取指定 sandbox 的指标信息并返回（等价于 kata-runtime metrics &lt;sandboxID&gt;） 如果没有指定 sandbox 参数，则通过 Prometheus 聚合所有 sandbox 的指标处理并返回 ListSandboxes处理 /sandboxes 请求，获取所有运行的 sandbox source code 获取维护的所有 sandboxID 根据实际请求，具体展示 HTML 或者 Text 格式的内容 GetAgentURL处理 /agent-url 请求，获取指定 sandboxID 的 agent 地址 source code 检验请求中的 sandbox 参数是否不为空 通过 /run/vc/sbs/&lt;sandboxID&gt;/shim-monitor.sock 发送 HTTP GET 请求至 shim server 的 http://shim/agent-url，解析内容获得 sandbox socket 地址 ExpvarHandler、PprofIndex、PprofCmdline、PprofProfile、PprofSymbol、PprofTrace处理 pprof 类请求，转发至 shim server source code 不同的 endpoint 在转发前会设置特定的请求头，例如 Content-Type 和 Content-Disposition 代理请求 检验请求中的 sandbox 参数是否不为空 通过 /run/vc/sbs/&lt;sandboxID&gt;/shim-monitor.sock 转发 HTTP GET 请求至 shim server 的 http://shim/&lt;URL&gt; 根据调用传参中的请求处理方式加工数据并返回","link":"/2023/01/29/2023-01-29%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20kata-monitor/"},{"title":"「 Kata Containers 」源码走读 — containerd-shim-kata-v2","text":"based on 3.0.0 本质上，Kata agent 负责 VM（也称 sandbox、guest 等）中的容器等进程的生命周期的管理。而 containerd-shim-kata-v2 作为 Kata agent 唯一的服务入口（本身是一个可执行程序，运行后即为 shim server），一部分实现了 Containerd shimv2 接口，暴露 shim API 用于与 Containerd 的 gRPC 通信，一部分暴露 HTTP endpoints 用于命令行工具和 Kata monitor 的服务请求处理，内部调用 virtcontainers 的诸多子模块，提供了用户和 Containerd 对 VM 以及容器进程的生命周期管理的能力。 src/runtime/vendor/github.com/containerd/containerd/runtime/v2/task/shim.pb.go 12345678910111213141516171819202122232425262728293031// service is the shim implementation of a remote shim over GRPCtype service struct { ctx context.Context rootCtx context.Context // root context for tracing rootSpan otelTrace.Span sandbox vc.VCSandbox monitor chan error ec chan exit mu sync.Mutex // 配置文件信息 config *oci.RuntimeConfig // 当前 shim service 中的容器信息 containers map[string]*container // 事件消费队列（TaskCreate、TaskStart、TaskDelete、TaskPaused、TaskResumed、TaskOOM、TaskExecAdded 和 TaskExecStarted） events chan interface{} eventSendMu sync.Mutex // 由 container engine 触发的关停函数 cancel func() // 由 container engine 传入的信息 namespace string id string // 原语义为 VM 中的容器 PID，但是在 kata 模型中 shimv2 无法获得，因此这里为 hypervisor 的 PID hpid uint32 // shimv2 的 PID pid uint32} main 函数 source code 如果指定的参数为 –version，则展示其版本、commit 等信息 通过 shim API，初始化并启动 shim server，注册名称为 containerd-shim-kata-v2，其中 NoReaper 和 NoSubreaper 均为 true 初始化 containerd-kata-shim-v2 模块的 logger，如果没有开启 debug 日志级别，则默认设置为 warn 级别，以此 logger 为基准，设置 virtcontainers 与 katautils 模块的 logger 校验启动时是否指定了 –namespce 参数（在 Kubernetes 集群中为 k8s.io，Docker 中为 moby，默认为 default） 启动 goroutine，持续处理 service 中的 exit channel，构建 TaskExit 事件发送至 events channel 中 初始化 eventForwarder，并启动 goroutine 调用 eventForwarder 的 forward 持续上报事件 返回 service，交给 Containerd 负责针对每个容器启动 shim server Serviceshim server 对外暴露的 gRPC 服务。 State获取进程的运行时状态 source code 请求体和返回体结构如下 1234type StateRequest struct { ID string ExecID string } 12345678910111213type StateResponse struct { ID string Bundle string Pid uint32 Status task.Status Stdin string Stdout string Stderr string Terminal bool ExitStatus uint32 ExitedAt time.Time ExecID string } 通过 r.ID 获取维护在 service.containers 中的容器 根据容器中的属性构建返回消息，如果 r.ExecID 不为空，则通过 r.ExecID 获取维护在 container.execs 中的 exec 中的属性为准 Create使用底层 OCI 运行时创建一个新的容器 source code 请求体和返回体结构如下 123456789101112type CreateTaskRequest struct { ID string Bundle string Rootfs []*types.Mount Terminal bool Stdin string Stdout string Stderr string Checkpoint string ParentCheckpoint string Options *types1.Any } 1234type CreateTaskResponse struct { // shimv2 无法从 VM 获取容器进程 PID，因此后续对于需要 PID 的返回值，直接返回 hypervisor 的 PID 即可 Pid uint32 } 校验 r.ID 是否不为空，且正则匹配满足 ^[a-zA-Z0-9][a-zA-Z0-9_.-]+$ 调用 create，创建容器，将返回的容器状态设为 created，并维护在 service.containers 中 发送 TaskCreate 事件至 service.events 中 create基于 service 实例和请求消息创建容器 source code 初始化空的 rootfs 对象，如果请求中 r.Rootfs 已经提供了一个，则以此为准，丰富 rootfs 对象 将 r.Bundle 下的 config.json 文件（例如 /run/containerd/io.containerd.runtime.v2.task/k8s.io/&lt;containerID&gt;/config.json，该文件即为 OCI spec，由 container manager 事先创建），解析为 OCI spec 结构 校验 r.ID 与 r.Bundle 是否不为空，并且 r.Bundle 存在且为目录结构，获取 r.Bundle 符号链接（如果是）指向的路径名 获取 r.Bundle 目录下的 config.json，读取其内容，解构成 compatOCISpec 结构（为了兼容 v1.0.0-rc4 和 v1.0.0-rc5，参考：https://github.com/opencontainers/runtime-spec/commit/37391fb） 不同版本下 spec.Process.Capabilities 字段属性不同，通过类型断言，将其以及 compatOCISpec 转换成对应的 OCI spec（实际上，容器运行后，spec 信息可以通过 crictl inspect xxx 或者 ctr c info xxx 查看到，也可以查看 config.json 文件），并返回 根据 spec 中的 annotation 信息判断其容器类型，并转换成 virtcontainers 中定义的容器类型 判断 spec.Annotations 中是否包含 io.kubernetes.cri.container-type、io.kubernetes.cri-o.ContainerType 和 io.kubernetes.docker.type 的 key（分别代表 Containerd、CRI-O 和 Dockershim 三种 CRI） 获得 key 对应的 value（value 即为容器的类型），分为两类，分别是 sandbox 和 container，区别于 CRI 的不同，具体的名称有所区别（Containerd 和 CRI-O 中称为 sandbox 和 container，而 Dockershim 中称为 podsandbox 和 container） 根据容器类型，映射出 virtcontainers 中的容器类别，例如 pod_sandbox 和 pod_container，当 spec.Annotations 中未识别到上述三种 key，则视为 single_container，即非 Pod 容器（如通过 ctr，podman 启动运行） 此外，匹配到 key，但是 value 不符合 sandbox 和 container 的，均视为 unknown_container_type 构建 runtimeConfig（runtimeConfig 聚合了运行时所有的设置信息，后续的操作中不再解析配置文件），优先级依次从 spec.Annotations、shimv2 请求传参和环境变量中获取 获取 spec.Annotations 中 key 为 io.katacontainers.config_path 的 value 作为 configPath（也就是 Kata Containers 的静态配置文件） 当 configPath 为空时，尝试从 r.Options 中获取（其中在类型断言时，优先使用 github.com/containerd/containerd/pkg/runtimeoptions/v1 中的 Options 类型，为了兼容 1.4.3、1.4.4 版本的 Containerd，退而使用 github.com/containerd/cri-containerd/pkg/api/runtimeoptions/v1 中的 Options 类型） 如果此时 configPath 仍为空，则从环境变量 KATA_CONF_FILE 中获取 如果 configPath 为空，则按序优先读取 /etc/kata-containers/configuration.toml 和 /usr/share/defaults/kata-containers/configuration.toml 配置文件内容，构建 runtimeConfig 校验部分配置项 [runtime].experimental 特性是否支持 [runtime].sandbox_bind_mounts 挂载点是否嵌套（即挂载点的 base 目录存在重复） 当 [runtime].disable_new_netns 启用时，[runtime].internetworking_model 是否设置为 none [hypervisor].default_memory 是否不为 0。当 guest 镜像采用 initrd 格式时，[hypervisor].default_memory 必须大于镜像大小（因为 initrd 会完全读到内存中）；当 guest 镜像采用 image 格式时，镜像大小大于 [hypervisor].default_memory 时，会输出警告信息（虽然 image 不会完全读到内存中，但是此情况并非正常现象） 当 [factory].enable_template 启用时，guest 镜像类型必须为 initrd 格式；当 [factory].vm_cache_number 大于 0 时，hypervisor 类型必须为 QEMU 根据不同的容器类型，基于配置文件内容创建容器 pod_sandbox 或 single_container（以下统称为 pod_sandbox） 当 service.sandbox 不为空时，意味着当前容器环境已经存在一个 sandbox（只有在完成创建容器后才会回写该字段），是无法嵌套创建 sandbox 的 基于 [runtime].jaeger_endpoint、[runtime].jaeger_user 和 [runtime].jaeger_password 创建 jaeger tracer 如果容器类型为 pod_sandbox，则基于 spec.Annotations 中的 io.kubernetes.cri.sandbox-memory、io.kubernetes.cri.sandbox-cpu-period 和 io.kubernetes.cri.sandbox-cpu-quota，计算 VM 的资源大小；如果容器类型为 single_container，则基于 spec.Linux.Resources 中的 Memory.Limit、CPU.Quota 和 CPU.Period，计算 VM 的资源大小。两者公式一致： CPU = (cpu-quota * 1000 / cpu-period + 999) / 1000 MEM = memory / 1024 /1024 检查容器的 rootfs 目录是否需要挂载 如果请求中 r.Rootfs 指定了一个，则判断其是否为块设备，并且 [hypervisor].disable_block_device_use 为 false（disable_block_device_use 禁止块设备用于容器的 rootfs。 在像 devicemapper 这样的存储驱动程序中，容器的 rootfs 由块设备支持，出于性能原因，块设备直接传递给 hypervisor。 这个标志阻止块设备被传递给 hypervisor，而使用 virtio-fs 传递 rootfs）；或者，rootfs 的类型为 fuse.nydus-overlayfs。满足条件之一，即不需要挂载，而是走后续的热插流程 创建 rootfs 目录（如果不存在），遍历 r.Rootfs，挂载至 rootfs 目录下（即 /run/containerd/io.containerd.runtime.v2.task/k8s.io/&lt;containerID&gt;/rootfs） 基于 factory 配置项，尝试获取现有 VM factory，如果获取失败且未启用 VM cache 特性时，会初始化新的 VM factory，并调用 vircontainers 的 SetFactory，透传 VM factory 基于 [hypervisor].rootless 设置 rootless（默认情况下，QEMU 以 root 身份运行。 当设置为 true 时，QEMU 将以非 root 的随机用户运行），如果启用 rootless，则额外执行以下流程 创建一个用于运行 Kata Containers 的随机用户 根据用户名获取并设置 UID 和 GID 创建用户目录，并设置环境变量 XDG_RUNTIME_DIR 将 KVM GID 添加到 hypervisor 配置项中的补充组中，使 hypervisor 进程可以访问 /dev/kvm 设备 基于上述构建的 OCI spec、runtimeConfig、rootfs、bundle、containerID 等信息创建 sandbox 将 OCI spec 和 runtimeConfig 转为 virtcontainers 所需的配置结构（即 sandboxConfig）额外注明：a. 部分参数当 OCI spec annotations 中有声明时，会以 annotations 为准，前提是 [hypervisor].enable_annotations 中声明了允许动态加载的配置项且该配置项合法；b. 当启用 [hypervisor].static_sandbox_resource_mgmt，VM 规格会被静态配置，而非启动后热插拔，因此 CPU 资源规格为 [hypervisor].default_vcpus + &lt;workloadCPUs&gt;，内存同理 当 host 启用 FIPS 时（即 /proc/sys/crypto/fips_enabled 为 1），sandboxConfig 中额外追加 kernel 参数 启动容器前优先创建 netns。当 [runtime].disable_new_netns 启用时（表示 shim 和 hypervisor 会运行在 host netns 中，而非创建新的），则直接跳过后续创建；否则，当 networkID（spec.Linux.Namespace 中 type 为 network 的 path）为空时（表示 netns 并非由 CNI 提前创建好，而是需要由 Kata Containers 创建，比如脱离 K8s 运行 Kata 的场景中），则根据具体是否为 rootless，执行对应的创建网络命名空间流程。如果为提前创建好的 netns，需要判断其是否与当前进程的 netns 不一致（当前进程可以代表 host 网络，而 Kata Containers 是不支持采用 host 网络作为容器网络的） 执行 spec.Hooks.Prestart（Prestart 是在执行容器进程之前要运行的 hook 列表，现已废弃） 中定义的动作 执行 spec.Hooks.CreateRuntime（CreateRuntime 是在创建容器之后但在调用 pivot_root 或任何等效操作之前要运行的 hook 列表） 中定义的动作 调用 VC 的 CreateSandbox，根据 sandboxConfig 信息创建 sandbox，并启动 pod_sandbox 容器 调用 VCSandbox 的 GetAllContainers，校验 sandbox 中的容器总数量是否为 1，返回 sandbox 设置 service.sandbox 为上面返回的 sandbox，调用 VCSandbox 的 GetHypervisorPid，获取 hypervisor 的 PID，设置至 service.hpid 中 启动监听 /run/vc/sbs/&lt;sandboxID&gt;/shim-monitor.sock 地址的 shim server，注册服务有 /metrics、/agent-url、/direct-volume/stats、/direct-volume/resize、/iptables、/ip6tables 以及 Pprof，注册上报至 Prometheus 的 shim 相关指标以及 sandbox 相关指标 pod_container 校验 service.sandbox 是否不为空（因为 pod_container 的运行是要依托于 pod_sandbox） 检查容器的 rootfs 目录是否需要挂载 如果请求中 r.Rootfs 指定了一个，则判断其是否为块设备，并且 [hypervisor].disable_block_device_use 为 false（disable_block_device_use 禁止块设备用于容器的 rootfs。 在像 devicemapper 这样的存储驱动程序中，容器的 rootfs 由块设备支持，出于性能原因，块设备直接传递给 hypervisor。 这个标志阻止块设备被传递给 hypervisor，而使用 virtio-fs 传递 rootfs）；或者，rootfs 的类型为 fuse.nydus-overlayfs。满足条件之一，即不需要挂载，而是走后续的热插流程 创建 rootfs 目录（如果不存在），遍历 r.Rootfs，挂载 rootfs 目录 基于上述构建的 OCI spec、runtimeConfig、rootfs、bundle、containerID 等信息创建 container 类型容器 遍历 spec.Mounts，如果挂载源路径由 K8s 临时存储（即路径中有 kubernetes.io~empty-dir 标识，且文件类型为 tmpfs），则将 spec.Mounts 中对应挂载点的类型设置为 ephemeral；如果挂载源路径由 K8s emptydir（即路径中有 kubernetes.io~empty-dir 标识，且文件类型不为 tmpfs） 且 runtimeConfig 没有禁用 disable_guest_empty_dir（如果启用，将不会在 guest 的文件系统中创建 emptydir 挂载，而是在 host 上创建，由 virtiofs 共享，性能较差，但是可以实现 host 和 guest 共享文件），则将 spec.Mounts 中对应挂载点的类型设置为 local （对于给定的 Pod，临时卷仅在 VM 内由 tmpfs 支持时创建一次。 对于同一 Pod 的连续容器，将重复使用已经存在的卷） 将 OCI spec 和 runtimeConfig 转为 virtcontainers 所需的配置结构（即 containerConfig） 根据 spec.Annotations 中的 io.kubernetes.cri.sandbox-id、io.kubernetes.cri-SandboxID 和 io.kubernetes.sandbox.id 的 key（分别代表 Containerd、CRI-O 和 Dockershim 三种 CRI），获取到 value（value 即为 sandboxID） 调用 VCSandbox 的 CreateContainer，创建容器 进入到 sandbox 的网络命名空间中（如果有），执行 spec.Hooks.Prestart（Prestart 是在执行容器进程之前要运行的 hook 列表，现已废弃） 中定义的动作 构建并返回容器 Start启动容器 source code 请求体和返回体结构如下 1234type StartRequest struct { ID string ExecID string} 123type StartResponse struct { Pid uint32} 通过 r.ID 获取维护在 service.containers 中的容器 如果 r.ExecID 为空，则调用 startContainer，启动容器，并发送 TaskStart 事件至 service.events 中；否则，调用 startExec，启动 exec 进程，并发送 TaskExecStart 事件至 service.events 中 startContainer处理启动容器请求 source code 如果容器类型为 pod_sandbox 调用 VCSandbox 的 Start，启动 sandbox 调用 VCSandbox 的 Monitor，启动 monitor，并设置在 service.monitor 中 启动 goroutine，监听 service.monitor 中的错误和退出信号。如果监听到（视为异常），则调用 VCSandbox 的 Stop 和 Delete，关停与销毁 sandbox 并清理相关资源 移除 rootfs 挂载点（注意：此处清理的为 /run/containerd/io.containerd.runtime.v2.task/k8s.io/&lt;containerID&gt;/rootfs，而共享目录下的 rootfs 在上述步骤 3 中已经清理） 启动 goroutine，调用 VCSandbox 的 GetOOMEvent（如果是由于 Kata agent 关停，返回类似 ttrpc: closed 或者 Dead agent 的错误时，则不再监听；其他异常情况，仍重新尝试调用接口），监听 OOM 事件。当收到 OOM 事件后，如果 container manager 为 CRI-O 时，则在例如 /run/containerd/io.containerd.runtime.v2.task/k8s.io/&lt;containerID&gt; 目录下，创建名为 oom 的文件，用于通知 CRI-O；如果 container manager 为 Containerd 时，则发送 TaskOOM 事件至 service.events 中 如果容器的类型不为 pod_sandbox，则调用 VCSandbox 的 StartContainer，启动容器 进入到 sandbox 的网络命名空间中（如果有），执行 spec.Hooks.Poststart（Poststart 是在容器进程启动后要运行的 hook 列表） 中定义的动作 设置容器状态为 running 调用 VCSandbox 的 IOStream，启动 goroutine，实时处理容器中的标准输出等 IO 流 启动 goroutine，处理容器中的退出队列 ，即先等到容器 IO 退出事件后，再调用 VCSandbox 的 WaitProcess，进一步等待进程返回退出码后执行后续清理流程：如果容器类型为 pod_sandbox，则关停 monitor，调用 VCSandbox 的 Stop 和 Delete，关停与销毁 sandbox，并清理相关资源；否则，调用 VCSandbox 的 StopContainer，关停容器。设置容器状态为 stopped，记录退出时间，状态码等例如，当 Pod 启动之后，pod_sandbox 容器会退出，此时可以收到 pod_sandbox 容器的 IO 退出事件，但是在 WaitProcess 时，不会有返回，因此不会执行 sandbox 的关停与销毁流程；而当 Pod 删除时，pod_sandbox 的 WaitProcess 收到结果，伴随着其余的业务容器一起关停并删除 启动 goroutine，发送退出消息至 service.ec 中 startExec处理进入容器请求 source code 通过 r.ID 获取维护在 service.containers 中的容器 通过 r.ExecID 获取维护在 container.execs 中的 exec 调用 VCSandbox 的 EnterContainer，进入容器内部 设置 exec 状态为 running 调用 VCSandbox 的 WinsizeProcess，调整 tty 大小 调用 VCSandbox 的 IOStream，启动 goroutine，实时处理容器中的标准输出等 IO 流 启动 goroutine，处理 exec 中的退出队列 ，即先等到容器 IO 退出事件后，再调用 VCSandbox 的 WaitProcess，进一步等待进程返回退出码后执行后续流程。设置 exec 状态为 stopped，记录退出时间，状态码等 启动 goroutine，发送退出消息至 service.ec 中 Delete删除容器或者 exec 进程 source code 请求体和返回体结构如下 1234type DeleteRequest struct { ID string ExecID string} 12345type DeleteResponse struct { Pid uint32 ExitStatus uint32 ExitedAt time.Time } 通过 r.ID 获取维护在 service.containers 中的容器 如果 r.ExecID 为空，则调用 deleteContainer，删除容器，并发送 TaskDelete 事件至 service.events 中；否则，直接删除 container.execs 中 key 为 r.ExecID 的 exec deleteContainer删除指定容器 source code 如果容器的类型不是 pod_sandbox，则先调用 VCSandbox 的 StopContainer，关停容器（如果容器状态不为 stopped），并调用 VCSandbox 的 DeleteContainer，删除容器 执行 spec.Hooks.Poststop（Poststop 是在容器进程退出后要运行的 hook 列表）中定义的动作 如果容器文件系统已经挂载完成，则移除 rootfs 挂载点（例如 /run/containerd/io.containerd.runtime.v2.task/k8s.io/&lt;containerID&gt;/rootfs，其在 host 上以 overlay 挂载点形式存在） 删除 s.containers 中的 key 为 r.ID 的容器 Pids返回容器内的所有进程 ID，对于 Kata Containers 而言，无法从 VM 获取进程 PID，因此只返回 hypervisor 的 PID source code 请求体和返回体结构如下 123type PidsRequest struct { ID string} 123456type PidsResponse struct { // task.ProcessInfo{ // Pid: s.hpid, // } Processes []*task.ProcessInfo} Pause暂停容器 source code 请求体结构如下 123type PauseRequest struct { ID string} 通过 r.ID 获取维护在 service.containers 中的容器 设置容器状态为 pausing 调用 VCSandbox 的 PauseContainer，暂停容器 如果暂停成功，则设置容器状态为 paused，并发送 TaskPaused 事件至 service.events 中；否则，调用 VCSandbox 的 StatusContainer，查询容器状态（分为 ready、running、paused 和 stopped），如果查询失败，则实际状态视为 unknown，设置容器状态为查询到的实际结果 Resume恢复容器 source code 请求体结构如下 123type ResumeRequest struct { ID string} 通过 r.ID 获取维护在 service.containers 中的容器 调用 VCSandbox 的 ResumeContainer，恢复容器 如果恢复成功，则设置容器状态为 running，并发送 TaskResumed 事件至 service.events 中；否则，调用 VCSandbox 的 StatusContainer，查询容器状态（分为 ready、running、paused 和 stopped），如果查询失败，则实际状态视为 unknown，设置容器状态为查询到的实际结果 Checkpoint创建容器检查点 source code 请求体结构如下 12345type CheckpointTaskRequest struct { ID string Path string Options *types1.Any} 截至 Kata 3.0，尚未实现该接口 Kill根据指定信号杀死进程 source code 请求体结构如下 123456type KillRequest struct { ID string ExecID string Signal uint32 All bool } 通过 r.ID 获取维护在 service.containers 中的容器 以容器状态与 ID 作为待杀死进程的信息，如果 r.ExecID 不为空，则以通过 r.ExecID 获取维护在 container.execs 中的 exec 属性为准 如果信号为 SIGKILL 或者 SIGTERM，并且进程状态已经为 stopped，则不做处理，直接返回即可（根据 CRI 规范，Kubelet 在调用 RemovePodSandbox 之前至少会调用一次 StopPodSandbox ，此调用是幂等的，并且如果所有相关资源都已被回收则不得返回错误。 在调用中它会先发送一个 SIGKILL 信号来尝试停止容器，因此一旦容器终止，应该忽略这个信号并直接返回）；否则，调用 VCSandbox 的 SignalProcess，杀死进程 Exec在容器中追加一个额外的进程 source code 请求体结构如下 123456789type ExecProcessRequest struct { ID string ExecID string Terminal bool Stdin string Stdout string Stderr string Spec *types1.Any} 通过 r.ID 获取维护在 service.containers 中的容器 校验 r.ExecID 是否在 container.execs 不存在 基于 r.Stdin、r.Stdout、r.Stderr、r.Terminal 和 r.Spec 构建 exec，维护在 container.execs 中，并发送 TaskExecAdded 事件至 service.events 中 ResizePty调整进程的 pty 大小 source code 请求体结构如下 123456type ResizePtyRequest struct { ID string ExecID string Width uint32 Height uint32} 通过 r.ID 获取维护在 service.containers 中的容器 以 container.ID 作为待处理进程的 ID，如果 r.ExecID 不为空，则通过 r.ExecID 获取维护在 container.execs 中的 exec.ID 为准，并更新 r.Width 和 r.Height 至 exec 中 调用 VCSandbox 的 WinsizeProcess，调整待处理进程的 pty 大小 CloseIO关闭进程的 IO 流 source code 请求体结构如下 12345type CloseIORequest struct { ID string ExecID string Stdin bool } 通过 r.ID 获取维护在 service.containers 中的容器 以 container.stdinPipe 和 container.stdinCloser 作为待处理进程 IO 的信息，如果 r.ExecID 不为空，则以通过 r.ExecID 获取维护在 container.execs 中的 exec 信息为准 直至 stdinCloser channel 不再阻塞，调用 stdinPipe 的 Close 方法关闭 IO 流 Update更新容器 source code 请求体结构如下 12345type UpdateTaskRequest struct { ID string Resources *types1.Any Annotations map[string]string } 调用 VCSandbox 的 UpdateContainer，更新容器的资源规格 Wait等待进程退出 source code 请求体和返回体结构如下 1234type WaitRequest struct { ID string ExecID string} 1234type WaitResponse struct { ExitStatus uint32 ExitedAt time.Time} 通过 r.ID 获取维护在 service.containers 中的容器 从 container.exitCh 中获取退出状态码，并重新回填至 container.exitCh（用容器进程的退出代码重新填充 exitCh，以防此进程有其他等待），如果 r.ExecID 不为空，则以通过 r.ExecID 获取维护在 container.execs 中的 exec.exitCh 为准 Stats获取容器的统计信息 source code 请求体和返回体结构如下 123type StatsRequest struct { ID string } 123type StatsResponse struct { Stats *types1.Any} 通过 r.ID 获取维护在 service.containers 中的容器 调用 VCSandbox 的 StatsContainer，获取容器的 Hugetlb、Pids、CPU、Memory、Blkio 和 Network 统计信息 Connect返回 shim 相关信息 source code 请求体和返回体结构如下 123type ConnectRequest struct { ID string} 1234567type ConnectResponse struct { // 即 service.pid ShimPid uint32 // 即 service.hpid TaskPid uint32 Version string } Shutdown关闭 shim server source code 请求体结构如下 1234type ShutdownRequest struct { ID string Now bool} 如果 service.containers 中仍有元素，代表 shim server 仍然管理容器中，因此仅关闭 tracing，不作其他处理，直接返回 调用 service.cancel 退出 shim server（cancel 由 Containerd 服务注册声明） 向 service.hpid 发送 SIGKILL 信号（由于只是在执行 stopSandbox 时向 QEMU 发送了一个 shutdown qmp 命令，并没有等到 QEMU 进程退出，这里最好确保它在 shim server 终止时已经退出。 因此，这里要对 hypervisor 进行最后的清理） 调用 os.Exit(0)，退出程序 Cleanup清理容器相关资源 Cleanup 并未用于实现 shimv2 API，而是 Service 的功能扩展，用于在执行 containerd-shim-kata-v2 delete 操作时触发容器清理流程 source code 返回体结构如下 12345type DeleteResponse struct { Pid uint32 ExitStatus uint32 ExitedAt time.Time} 设置日志输出至 stderr 中（因此，日志信息并不会出现在 Kata 服务中） 获取当前目录下（例如 /run/containerd/io.containerd.runtime.v2.task/k8s.io/&lt;containerID&gt; 目录，流程需要在此路径下执行 ）的 config.json 文件，解析成 OCI spec 格式，判断其容器类型 如果容器类型是 pod_container，则通过 spec.Annotation 中获取到 sandboxID；如果容器类型是 pod_sandbox 或者 single_container，sandboxID 即为 service.id 调用 VC 的 CleanupContainer，清理容器 移除 rootfs 挂载点（例如 /run/containerd/io.containerd.runtime.v2.task/k8s.io/&lt;containerID&gt;/rootfs） ShimManagementshim server 对外暴露的 HTTP 服务。 agentURL处理 /agent-url 请求，返回 agent 的地址 source code 调用 VCSandbox 的 GetAgentURL，获取 agent 地址并返回 serveMetrics处理 /metrics 请求，返回 guest、shim 和 agent 相关的指标 source code 调用 VCSandbox 的 UpdateRuntimeMetrics，更新 guest 指标（更新就是重新获取指标，重新设置在 Prometheus 中） 更新当前进程（即 shimPID）的指标 获取 /proc/&lt;shimPID&gt;/fd 目录下的文件数量，上报 kata_shim_fds 指标 解析 /proc/&lt;shimPID&gt;/stat 文件内容，上报 kata_shim_proc_stat 指标 解析 /proc/&lt;shimPID&gt;/status 文件内容，上报 kata_shim_proc_status 指标 解析 /proc/&lt;shimPID&gt;/io 文件内容，上报 kata_shim_io 指标 如果使用旧版本的 agent（现阶段均为新版本），则不支持获取 agent 指标，直接返回 VM 和 shim 指标即可 调用 VCSandbox 的 GetAgentMetrics，获取 agent 指标（在这里，如果获取不到，则视为当前使用旧版本的 agent，后续则由步骤 3 直接返回） 启动 goroutine，上报 pod_overhead_cpu 和 pod_overhead_memory_in_bytes 至 Prometheus（收集 Pod overhead 指标需要 sleep 来获取 cpu/memory 资源使用的变化，所以这里只触发 collect 操作，下次从 Prometheus server 收集请求时收集数据） 调用 VCSandbox 的 Stats，获取 sandbox 的 cgroup 相关信息；调用 VCSandbox 的 GetAllContainers，获取所有容器，并逐一调用 VCSandbox 的 StatsContainer，获取容器的 cgroup 相关信息 间隔 1 秒钟，重复步骤 1，再次获取 sandbox 和容器的 cgroup 相关信息 根据两次数据信息以及总耗时，计算 overhead 指标 serveVolumeStats处理 /direct-volume/stats 请求，返回 guest 中指定卷的信息 source code 校验请求中 path 参数是否不为空 调用 VCSandbox 的 GuestVolumeStats，获取 guest 中指定卷的信息并返回 serveVolumeResize处理 /direct-volume/resize 请求，扩容 guest 中指定卷的大小 source code 读取请求体，并解构成 VCSandbox 所需的格式 调用 VCSandbox 的 ResizeGuestVolume，扩容 guest 中指定卷的大小 ipTablesHandler、ip6TablesHandler处理 /iptables 和 /ip6tables请求，操作 guest 中的 iptables 信息 source code (ipTablesHandler)source code (ip6TablesHandler) 两者本质相似，区别在于 ip6TablesHandler 的 isIPv6 参数为 true，后续调用接口时，会传递该参数 判断请求方法，目前仅支持 PUT 和 GET 两种，其余返回状态码 501。如果为 PUT 请求，则读取请求体，调用 VCSandbox 的 SetIPTables，设置 guest 中的 iptables 信息；如果为 GET 请求，调用 VCSandbox 的 GetIPTables，获取 guest 中的 iptables 信息 EventForwardersrc/runtime/pkg/containerd-shim-v2/event_forwarder.go EventForwarder 为事件上报模块，其中事件源自于 forwarder 中的 service.events。forwarder 包括两类：log 与 containerd，取决于事件最终上报的地点。每一个事件默认上报超时时间为 5 秒钟，超出会被取消上报。 123type logForwarder struct { s *service} 12345type containerdForwarder struct { s *service ctx context.Context publisher events.Publisher} 其中，publisher 由 Containerd 调用时提供。 工厂函数 source code 如果环境变量中声明了 TTRPC_ADDRESS，则初始化 containerdForwarder，否则初始化 logForwarder forward处理事件上报 logForwardersource code 监听 service.events 中的事件，断言其的类型，获得事件的 topic 在 containerd-kata-shim-v2 模块的日志中输出事件内容 containerdForwardersource code 监听 service.events 中的事件，断言其的类型，获得事件的 topic 调用 Containerd 的 publisher 模块的 Publish（Containerd 负责实现），上报事件至 Containerd forwarderTypeforwarder 的具体类型 logForwardersource code 类型为 log containerdForwardersource code 类型为 containerd","link":"/2023/01/21/2023-01-21%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20containerd-shim-kata-v2/"},{"title":"「 Kata Containers 」源码走读 — virtcontainers&#x2F;storage","text":"based on 3.0.0 PersistDriversrc/runtime/virtcontainers/persist/api/interface.go PersistDriver（也称 store）的实现有两类：fs 和 rootless，其中 rootlessfs driver 完全继承 fs driver。 12345678910111213type FS struct { // 包括两种，fs 和 rootless，区别在于当前是否为 root 用户权限 driverName string // - fs：/run/vc // - rootless：&lt;XDG_RUNTIME_DIR&gt;/run/vc（XDG_RUNTIME_DIR 默认为 /run/user/&lt;UID&gt;） // 用于保存 sandbox（sbs，其中容器信息以子目录形式保存）和 VM（vm）相关状态信息 storageRootPath string // 内存状态数据，用于内存数据和文件数据的持久化转换 sandboxState *persistapi.SandboxState containerState map[string]persistapi.ContainerState} 123type RootlessFS struct { *FS} 以下接口 fs 和 rootlessfs 实现方式完全一样。 ToDisk保存 sandbox 和容器状态信息到文件中 source code 以当前用户组信息创建 &lt;RunStoragePath&gt;/&lt;sandboxID&gt; 目录（如果不存在） 创建 &lt;RunStoragePath&gt;/&lt;sandboxID&gt;/persist.json 文件（如果不存在），写入 sandbox 状态信息 遍历所有的容器状态信息 以当前用户组信息创建 &lt;RunStoragePath&gt;/&lt;sandboxID&gt;/&lt;containerID&gt; 目录（如果不存在） 创建 &lt;RunStoragePath&gt;/&lt;sandboxID&gt;/&lt;containerID&gt;/persist.json 文件（如果不存在），写入容器状态信息 遍历 &lt;RunStoragePath&gt;/&lt;sandboxID&gt; 目录（目录下的所有子目录名称均为 containerID），由于步骤 3 中为当前全量的容器状态信息，以此为准移除不存在的容器目录 FromDisk读取写入文件中的 sandbox 和容器状态信息 source code 读取 &lt;RunStoragePath&gt;/&lt;sandboxID&gt;/persist.json 文件内容，获取 sandbox 状态信息 遍历 &lt;RunStoragePath&gt;/&lt;sandboxID&gt; 目录（目录下的所有子目录名称均为 containerID），读取 &lt;RunStoragePath&gt;/&lt;sandboxID&gt;/&lt;containerID&gt;/persist.json 文件内容，获取容器状态信息 Destroy删除 sandbox 状态信息目录 因为 sandbox 和容器状态信息目录之间为父子目录关系，删除父目录即可。 source code 删除 &lt;RunStoragePath&gt;/&lt;sandboxID&gt; 目录 Lock对 sandbox 状态信息目录上锁 因为 sandbox 和容器状态信息目录之间为父子目录关系，对父目录上锁即可。 source code 调用 syscall.Flock，对 &lt;RunStoragePath&gt;/&lt;sandboxID&gt; 目录上共享锁或排他锁（根据函数传参而定） 返回一个调用 syscall.Flock 的函数体，对 &lt;RunStoragePath&gt;/&lt;sandboxID&gt; 目录释放锁（即 syscall.LOCK_UN） GlobalWrite在状态信息目录中的文件写入内容 source code 以当前用户组信息创建 &lt;storageRootPath&gt;/&lt;relativePath&gt;（relativePath 为函数传参中的待写入内容的相对路径）所在目录（如果不存在） 创建 &lt;storageRootPath&gt;/&lt;relativePath&gt; 文件，写入数据 GlobalRead读取状态信息目录中的文件内容 source code 读取 &lt;storageRootPath&gt;/&lt;relativePath&gt; 文件内容（relativePath 为函数传参中的待写入内容的相对路径） RunStoragePath获取 sandbox 状态信息目录 source code 返回 &lt;storageRootPath&gt;/sbs 路径 RunVMStoragePath获取 vm 状态信息目录 source code 返回 &lt;storageRootPath&gt;/vm 路径 FilesystemSharersrc/runtime/virtcontainers/fs_share.go FilesystemSharer（也称 fsSharer）仅有 linux 操作系统下的实现。 1234567type FilesystemShare struct { sandbox *Sandbox sync.Mutex // 表示文件系统是否准备就绪，在调用 Prepare 后，设置为 true prepared bool} bindMount将 src 绑定挂载到 dst source code 处理 src 中的符号链接，获得绝对路径，校验其是否存在 创建 dst 的父目录（如果不存在） 根据 src 的格式（目录或文件），创建 dst 将 src 以 MS_BIND 的属性绑定挂载到 dst 下等价于 mount –bind foo bar，也就是把 foo 目录绑定挂载到 bar 目录，bar 目录为 foo 目录的镜像挂载点。绑定后的两个目录类似于硬链接，无论读写 bar 还是读写 foo，都会反应在另一方，内核在底层所操作的都是同一个物理位置。将 bar 卸载后，bar 目录回归原始空目录状态，期间所执行的修改都保留在 foo 目录下 更改 dst 目录挂载属性（即 MS_SHARED、MS_PRIVATE、MS_SLAVE 和 MS_UNBINDABLE）等价于 mount –make-slave bar，也可以和步骤 4 合并操作 mount –make-slave –bind foo bar。单向传播模式下，在 foo 下添加或移除子挂载点，会同步到 bar 挂载点，而在 bar 下添加或移除子挂载点，不会影响 foo 如果挂载为只读属性，则追加至绑定挂载属性中等价于 mount –read-only –bind foo bar Prepare准备 host/guest 的共享文件系统目录 source code 创建 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/shared 目录（用于 9p/virtiofs 在 host 和 guest 之间共享数据） 创建 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts 目录（用于维护所有 host 和 guest 之间的挂载点） 调用 bindMount，将 mounts 目录以只读和 MS_SLAVE 的属性绑定挂载到 shared 目录下（为了后面 mounts 挂载点下的子挂载也能出现在 shared 中） 处理 [runtime]. sandbox_bind_mounts 挂载点 创建 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/sandbox-mounts 目录 针对 sandbox_bind_mounts 目录中的每一个挂载点，调用 bindMount，以只读和 MS_PRIVATE 的属性绑定挂载到 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/sandbox-mounts/&lt;sandbox_bind_mounts&gt; 中，并追加 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/shared/sandbox-mounts/&lt;sandbox_bind_mounts&gt; 绑定挂载的只读属性 设置 prepared 为 true，表示共享文件系统已就绪（标识位也用于保证 Prepare 和 Cleanup 操作的幂等性） Cleanup清理 host/guest 的共享文件系统目录 source code 处理 [runtime]. sandbox_bind_mounts 挂载点 针对 sandbox_bind_mounts 目录中的每一个挂载点，移除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/sandbox-mounts/&lt;sandbox_bind_mounts&gt; 挂载点 删除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/sandbox-mounts 目录 移除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/shared 挂载点 移除 sandbox 中所有容器的挂载点。如果容器 rootfs 的类型为 fuse.nydus-overlayfs，则移除 /rafs/&lt;containerID&gt;/lowerdir virtiofs 挂载点，移除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt;/snapshotdir 挂载点并删除此目录，删除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt;/rootfs 目录（该目录和 /run/containerd/io.containerd.runtime.v2.task/k8s.io/&lt;containerID&gt;/rootfs 数据同步，借助 9pfs/virtiofs 实现容器文件系统共享，在 host 上以 overlay 挂载点形式存在）；否则，移除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt;/rootfs 挂载点并删除此目录 删除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt; 目录 ShareFile共享 host 文件至 guest 中 source code 共享文件（shareFile）名称格式为 &lt;containerID&gt;-&lt;random bytes&gt;-&lt;dst&gt;例如：&lt;containerID&gt;-47dcc9007bca8805-hostname 调用 hypervisor 的 Capabilities，判断是否支持 host 文件系统共享特性（QEMU 场景下支持） 如果不支持，则通过文件拷贝实现共享 校验 src 是否存在。如果 src 非常规文件，则不做处理（这里并未视为错误，而是作为一种局限性将其忽略） 调用 agent 的 copyFile，将 src 文件拷贝至 sandbox 的 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/containers/&lt;shareFile&gt; 文件位置 如果支持，则通过文件挂载实现共享 如果挂载为读写属性，则调用 bindMount，将 src 以读写和 MS_PRIVATE 的属性绑定挂载到 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;shareFile&gt; 否则，调用 bindMount，将 src 以只读和 MS_PRIVATE 的属性绑定挂载到 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/private/&lt;shareFile&gt;，进而调用 bindMount，将 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/private/&lt;shareFile&gt; 以读写和 MS_PRIVATE 的属性绑定挂载到 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;shareFile&gt;对于只读挂载，bindMount 重新挂载事件不会传播到挂载子树，并且它也不会出现在 virtiofsd 独立挂载命名空间中 设置挂载信息的 host 侧路径为 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;shareFile&gt; UnshareFile移除 host/guest 共享文件的挂载点 source code 移除挂载信息的 host 侧挂载点 如果挂载类型为 bind，校验 host 侧挂载点文件是否存在。如果为常规文件且为空，则直接删除；如果为目录，则移除目录 ShareRootFilesystem创建 guest 中容器 rootfs 共享挂载 source code 如果 rootfs 类型为 fuse.nydus-overlayfs 通过 virtiofsd 挂载 /rafs/&lt;containerID&gt;/lowerdir 目录至 guest 中 创建 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt;/rootfs 目录 调用 bindMount，将 rootfs 挂载参数中的 snapshotdir 目录以只读和 MS_SLAVE 的属性绑定挂载到 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt;/snapshotdir 挂载类型仍为联合文件系统的 overlay 形式 如果 rootfs 类型不是 fuse.nydus-overlayfs，并且是基于块设备的 rootfs 调用 devManager 的 GetDeviceByID，根据容器状态中的信息获取到设备信息 挂载类型视 [hypervisor].block_device_driver 而定 创建 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt;/rootfs 目录 对于传统的 rootfs（也就是非 fuse.nydus-overlayfs 类型，并且不是基于块设备的），调用 bindMount，以读写和 MS_PRIVATE 的属性将 rootfs（例如 /run/containerd/io.containerd.runtime.v2.task/k8s.io/&lt;containerID&gt;/rootfs）绑定挂载到 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt;/rootfs这个目录本身为共享目录，因此无需告知 agent 去挂载，在 host 侧挂载后会自动在 guest 中出现 无论以上哪种挂载形式，最终 guest 中容器 rootfs 的挂载点均为 /run/kata-containers/shared/containers/&lt;containerID&gt;/rootfs UnshareRootFilesystem移除 guest 中容器 rootfs 共享挂载 source code 如果 rootfs 类型为 fuse.nydus-overlayfs 通过 virtiofsd 移除 guest 中的 /rafs/&lt;containerID&gt;/lowerdir 挂载点 移除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt;/snapshotdir 挂载点，并删除目录 移除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt;/rootfs 目录 如果 rootfs 类型不是 fuse.nydus-overlayfs，则移除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt;/rootfs 挂载点，并删除目录 删除 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/mounts/&lt;containerID&gt; 目录","link":"/2023/03/05/2023-03-05%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20virtcontainers%20storage/"},{"title":"「 Kata Containers 」源码走读 — virtcontainers","text":"based on 3.0.0 virtcontainers 本质上不是独立组件，而是一个用于构建硬件虚拟化容器运行时的 Golang 库。 现有的少数部分基于 VM 的容器运行时都共享相同的硬件虚拟化语义，但是使用不同的代码库来实现，virtcontainers 的目标就是将这部分封装成一个通用的 Golang 库。 理想情况下，基于 VM 的容器运行时，会从将它们实现的运行时规范（例如 OCI spec 或 Kubernetes CRI）转换成 virtcontainers API。 virtcontainers API 大致受到 Kubernetes CRI 的启发。然而，尽管这两个项目之间的 API 相似，但 virtcontainers 的目标不是构建 CRI 实现，而是提供一个通用的、运行时规范不可知的、硬件虚拟化的容器库，其他项目可以利用它来自己实现 CRI。 VCsrc/runtime/virtcontainers/interfaces.go virtcontainers 库的入口模块，VC 初始化 VCSandbox 模块管理 sandbox，进而初始化 VCContainer 模块管理容器。 1234type VCImpl struct { // factory 的具体实现，不为空时表示为 VM factory 场景 factory Factory} VC 中声明的 SetLogger 和 SetFactory 均为参数赋值，无复杂逻辑，不作详述。 CreateSandbox创建 sandbox 与 pod_sandbox 容器 source code 创建 sandbox 校验 sandboxConfig 的 annotation 中自定义运行时配置（称为 assets ）的合法性并设置annotation 例如 io.katacontainers.hypervisor.kernel 为用户上层通过 Pod annotation 定义，CRI 会透传给底层运行时 初始化 VCSandbox，准备所需环境 如果 sandbox 中状态信息（即 sandbox.state）已经存在，则表明不是新创建的 pod_sandbox 容器，无需后续动作，仅用于状态更新维护，直接返回 sandbox 即可 调用 fsShare 的 Prepare，准备 sandbox 所需的共享文件系统目录 调用 agent 的 createSandbox，准备 sandbox 所需环境 设置 sandbox 状态为 ready 如果未启用 [runtime].disable_new_netns 并且不是 VM factory 场景（在 VM factory 场景下，网卡是在 VM 启动后热插进去的），则调用 network 的 AddEndpoints，添加 netns 中的所有网卡到 VM 中netns 要么是 Kata Containers 在运行时创建，要么是 CNI 等外部组件预先创建。总之，此时 netns 已经存在了 调用 resCtrl 的 setupResourceController，将当前进程加入 cgroup 中管理 启动 VM（在 1-2 步骤中的 VCSandbox 初始化流程中，已经创建了 VM） 如果 [hypervisor].enable_debug 启用（用于输出 hypervisor 和 kernel 产生的消息），则调用 hypervisor 的 GetVMConsole，获取 VM console 地址（/run/vc/vm/&lt;sandboxID&gt;/console.sock） 调用 network 的 Run，进入到该 netns 中，执行以下逻辑：如果为 VM factory 场景，则获取 factory 中缓存的 VM，调用 agent 的 reuseAgent，更新 agent 实例，并创建软链接 /run/vc/vm/&lt;sandboxID&gt; 指向 /run/vc/vm/&lt;vmID&gt;；否则，调用 hypervisor 的 StartVM，启动 VM 进程 如果为 VM factory 场景，则调用 network 的 AddEndpoints，热添加 netns 中的所有网卡到 VM 中 如果启用 [hypervisor].enable_debug，实时读取 VM console 地址获取其实时内容，并以 debug 级别日志形式输出 调用 agent 的 startSandbox 调用 network 的 Endpoints，获取 VM 所有的网卡设备，调用 endpoint 的 NetworkPair，关闭位于 host 侧的 vhost_net 句柄（即 /dev/vhost-net）截至 Kata 3.0，目前仅对 macvtap 类型的 endpoint 生效 调用 agent 的 getGuestDetails，获取如 seccompSupported 等 guest 信息详情，更新至 sandbox 中 创建 sandbox 中的每一个容器（其实，此时 sandbox 中仅有一个容器，就是 pod_sandbox 容器本身） 初始化 VCContainer，准备容器所需环境 根据 [hypervisor].disable_block_device_use、agent 是否具备使用块设备能力以及 hypervisor 是否允许块设备热插拔，判断是否当前支持块设备，并且容器的 rootfs 类型不是 fuse.nydus-overlayfs，也就是 rootfs 是基于块设备创建的 通过 /sys/dev/block/&lt;major&gt;-&lt;minor&gt;/dm 的存在性，判断是否为 devicemapper 块设备 如果是 devicemapper 块设备，则调用 devManager 的 NewDevice，初始化设备，并调用 devManager 的 AttachDevice，热插到 VM 中 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/containers/&lt;sandboxID&gt; 路径 针对容器中的每一个设备，调用 devManager 的 AttachDevice，attach 到 VM 中 调用 agent 的 createContainer，创建 pod_sandbox 容器 设置容器状态为 ready 调用 store 的 ToDisk，保存状态数据到文件中 更新维护在 sandbox 中的容器信息 调用 updateResources，热更新 VM 的资源规格（由于该流程中仅为创建 pod_sandbox，不涉及 pod_container，因此为配置中声明的 [hypervisor].default_vcpus 和 [hypervisor].default_memory） 调用 resCtrl 的 resourceControllerUpdate，更新 sandbox 的 cgroup 调用 store 的 ToDisk，保存状态数据到文件中 CleanupContainer关停、删除容器并销毁 sandbox 环境 source code 调用 store 的 FromDisk，读取 sandbox 状态信息 获取 sandbox 并更新其中的容器（更新的意义在于后续的删除操作以文件内容为准） 调用 VCSandbox 的 StopContainer 和 DeleteContainer，关停并删除该容器 调用 VCSandbox 的 GetAllContainers，获取 sandbox 中的所有容器，如果仍大于 0（说明当前 sandbox 仍有容器存在，需要保留 sandbox 环境），否则调用 VCSandbox 的 Stop，关停 sandbox，并调用 VCSandbox 的 Delete，删除 sandbox VCSandboxsrc/runtime/virtcontainers/interfaces.go virtcontainers 库中用于管理 sandbox 的模块，同时调用 VCContainer 模块间接管理容器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// Sandbox is composed of a set of containers and a runtime environment.// A Sandbox can be created, deleted, started, paused, stopped, listed, entered, and restored.type Sandbox struct { ctx context.Context id string sync.Mutex annotationsLock *sync.RWMutex wg *sync.WaitGroup monitor *monitor config *SandboxConfig // virtcontainers 中的各类子模块 devManager api.DeviceManager factory Factory hypervisor Hypervisor agent agent store persistapi.PersistDriver fsShare FilesystemSharer network Network // sandbox 中的 SWAP 设备 swapDevices []*config.BlockDrive // sandbox 中的 SWAP 设备总大小 swapSizeBytes int64 // sandbox 中的 SWAP 设备数量 swapDeviceNum uint // host 和 sandbox 的共享卷 volumes []types.Volume // 是否所有容器共享相同的 sandbox 级别的 PID 命名空间 sharePidNs bool // 用于监视 guest console 输出流 cw *consoleWatcher // [runtime].sandbox_cgroup_only]，默认为 false // - false：sandboxController 和 overheadController 同时存在 // - true：仅有 sandboxController，overheadController 为 nil sandboxController resCtrl.ResourceController overheadController resCtrl.ResourceController // sandbox 中的容器，Key 为 containerID containers map[string]*Container // sandbox 状态信息，包括 cgroup 路径、块设备索引记录等 state types.SandboxState // destination 为 /dev/shm，type 为 bind 的挂载点的 source 大小 shmSize uint64 // guest 特性，需要调用 agent 接口获得 // guest 是否支持 seccomp 特性 seccompSupported bool // 是否禁止 VM 关机 disableVMShutdown bool} 工厂函数 source code 校验 [runtime].experimental 是否为可支持的特性 初始化 hypervisor、agent、store、fsSharer、devManager 等 virtcontainers 子模块 初始化 resourceController（[runtime].sandbox_cgroup_only 为 true 表示 Pod 所有的线程全部由 sandboxController 管理；反之，仅 vCPU 线程由 sandboxController 管理，而其余的由 overheadController 管理） 获取到 spec.Linux.CgroupsPath（缺省为 /vc），如果 cgroup 不是由 systemd 纳管（通过 cgroupPath 格式判断），则最后一级路径新增 kata_ 前缀 获取 spec.Linux.Resources.Devices 中 /dev/null 和 /dev/urandom 设备信息（如果未声明，则构建） 调用 devManager 的 GetAllDevices，获取所有的设备；进一步调用 device 的 GetHostPath，获取设备位于 host 上的路径，将其构建成 cgroup 管理形式 调用 resCtrl 的 NewSandboxResourceController，初始化 sandboxController（cgroupPath 为步骤 1 处理后的结果） 如果 [runtime].sandbox_cgroup_only 为 false，则调用 resCtrl 的 NewResourceController，初始化 overheadController （cgroupPath 为 /kata_overhead/&lt;sandboxID&gt;） 从文件恢复 sandbox 状态信息 调用 store 的 FromDisk，尝试从文件中恢复 sandbox 状态信息当 sandbox 新创建的时候，并没有状态文件，因此必然失败，但是会忽略错误信息 调用 hypervisor 的 Load，加载 hypervisor 信息 调用 devManager 的 LoadDevices，加载设备信息 调用 agent 的 load，加载 agent 信息 调用 endpoint 的 load，加载网络 endpoint 信息 校验 sandboxConfig 中的 hypervisor 配置的合法性，其中包括 [hypervisor].kernel 是否不为空，[hypervisor].image 和 [hypervisor].initrd 有且仅有一个。并设置 [hypervisor].default_vcpus 缺省时为 1，[hypervisor].default_memory 缺省时为 2048 调用 hypervisor 的 CreateVM，创建 VM 调用 agent 的 init，准备 agent 环境 VCSandbox 中声明的 Annotations、GetNetNs、GetAllContainers、GetAnnotations、GetContainer、ID 和 SetAnnotations 均为参数获取与赋值，无复杂逻辑，不作详述。 updateResources热更新 VM 的资源规格 source code 如果 [runtime].static_sandbox_resource_mgmt 启用时（Kata 将在启动虚拟机之前确定合适的 sandbox 内存和 CPU 大小，而非动态更新。 作为 hypervisor 不支持 CPU、内存热插拔时的解决方案），则不触发更新操作，直接返回 计算 sandbox 中所有状态非 stopped 容器的 CPU 、内存和 SWAP 总量（前提是 [hypervisor].enable_guest_swap 开启，并且 memory.swappiness 大于 0，才会考虑 SWAP 资源），加上基础 VM 大小，得出最后预期的 VM 大小，公式为 CPU = (C1.cpu-quota * 1000 / C1.cpu-period + 999) / 1000 + C2… + [hypervisor].default_vcpus MEM = C1.memory-limit + C1.hugepages-limit + C2… + [hypervisor].default_memory SWAP = swap_in_bytes memory_limit_in_bytes swap size set set io.katacontainers.container.resource.swap_in_bytes- memory_limit_in_bytes not set set memory_limit_in_bytes not set not set io.katacontainers.config.hypervisor.default_memory set not set cgroup doesn’t support this usage 截至 Kata 3.0，在 K8s 场景下，SWAP 功能仍存在异常：参考 https://github.com/kata-containers/kata-containers/issues/5627 如果预期 SWAP 比当前 sandbox 的 SWAP 多，则需要新增一个大小为两者差值的 SWAP 文件 创建 /run/kata-containers/shared/sandboxes/swap&lt;ID&gt; 文件（sandbox 中的 SWAP 序号从 0 递增） 调整文件的大小为差值和 10 倍内存分页中最大值（小于 10 倍内页分页的 SWAP 会被 mkswap 拒绝：mkswap: error: swap area needs to be at least 40 KiB，内存分页：4096），并额外追加一个内存分页大小（SWAP 文件需要一个内存分页大小储存元数据） 调用系统命令 mkswap，转换为 SWAP 文件，并构建 raw 格式的块设备类型 调用 hypervisor 的 HotplugAddDevice，热添加 SWAP 文件到 VM 中 调用 agent 的 addSwap，配置 SWAP 文件 调用 hypervisor 的 ResizeVCPUs，调整 VM CPU 数量 如果为新增调整，则调用 agent 的 onlineCPUMem，通知 agent 上线热添加部分的 CPU 循环调用 hypervisor 的 GetTotalMemoryMB，获取当前 VM 的内存数量，比对预期 VM 的内存数量，在不超出最大热添加内存数量限制的前提下（部分场景下，例如 ACPI 热插拔，内存的单次热添加有最大数量限制；而在 virtio-mem 下，即 [hypervisor].enable_virtio_mem 启用， 且 /proc/sys/vm/overcommit_memory 文件内容为 1，则没有最大热添加数量限制），调用 hypervisor 的 ResizeMemory，分批热添加 VM 内存 调用 agent 的 memHotplugByProbe，通知 agent 内存热插事件（如果 guest 内核支持内存热添加探测），并调用 agent 的 onlineCPUMem，通知 agent 上线热添加部分的内存 Stats获取 sandbox 的统计信息 source code 调用 sandboxController 的 Stat，获取 sandbox 全部的 cgroup 统计信息（截至当前，并未聚合 kata_overhead 的统计信息，即 overheadController 部分） 调用 hypervisor 的 GetThreadIDs，获取 hypervisor 使用的 CPU 数量 聚合以上信息并返回 Start启动 sandbox 与 pod_sandbox 容器 source code 校验 sandbox 状态是否为 ready、paused 或 stopped 设置 sandbox 状态为 running 针对 sandbox 中的每一个容器，调用 VCContainer 的 start，启动容器（其实，此时 sandbox 中仅有一个容器，就是 pod_sandbox 容器本身） 调用 store 的 ToDisk，保存状态数据到文件中 Stop关停 sandbox 与容器，并清理相关资源 source code 如果 sandbox 状态已经为 stopped，则不做任何操作 校验 sandbox 状态是否为 ready、running 或者 paused 针对 sandbox 中的每一个容器，调用 VCContainer 的 stop，关停容器 调用 agent 的 stopSandbox，关停 sandbox 调用 hypervisor 的 StopVM，关停 VM 如果 [hypervisor]. enable_debug 启用，则关闭 VM console 设置 sandbox 状态为 stopped 调用 network 的 RemoveEndpoints，移除 VM 中的所有网卡 调用 store 的 ToDisk，保存状态数据到文件中 调用 agent 的 disconnect，关闭与 agent 的连接 移除 host 上的 /run/kata-containers/shared/sandboxes/swap&lt;ID&gt; 文件（sandbox 中的 SWAP 序号从 0 递增） Delete销毁 sandbox 与容器，并清理相关资源 source code 校验 sandbox 的状态是否为 ready、paused 和 stopped 针对 sandbox 中的每一个容器，调用 VCContainer 的 delete，删除容器 如果在 root 权限下，则调用 resCtrl 的 resourceControllerDelete，删除相关的 resourceController 以及 cgroup 资源 关停 sandbox 的 monitor 调用 hypervisor 的 Cleanup，清理 hypervisor 资源 调用 fsShare 的 Cleanup，清理 sandbox 的共享文件系统 调用 store 的 Destroy，删除状态数据目录 Status获取 sandbox 与容器的详细信息 source code 针对 sandbox 中的每一个容器，获取其状态信息（例如：ID、rootfs、状态、启动时间、annotation、PID 等） 结合 sandbox 的状态信息（例如 ID、状态、hypervisor 类别、hypervisor 配置、annotation 等）返回 CreateContainer创建 pod_container 容器并热更新 sandbox 规格 source code 初始化 VCContainer，准备容器环境，挂载设备等 根据 [hypervisor].disable_block_device_use、agent 是否具备使用块设备能力以及 hypervisor 是否允许块设备热插拔，判断是否当前支持块设备，并且容器的 rootfs 类型不是 fuse.nydus-overlayfs，也就是 rootfs 是基于块设备创建的 通过 /sys/dev/block/&lt;major&gt;-&lt;minor&gt;/dm 的存在性，判断是否为 devicemapper 块设备 如果是 devicemapper 块设备，则调用 devManager NewDevice，初始化设备，并调用 devManager 的 AttachDevice，热插到 VM 中 &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/containers/&lt;sandboxID&gt; 路径 针对容器中的每一个设备，调用 devManager 的 AttachDevice，热插到 VM 中 调用 agent 的 createContainer，创建 pod_container 容器 设置容器状态为 ready 调用 store 的 ToDisk，保存状态数据到文件中 更新维护在 sandbox 中的容器信息 调用 updateResources，热更新 VM 的资源规格 调用 resCtrl 的 resourceControllerUpdate，更新 sandbox 的 cgroup 调用 store 的 ToDisk，保存状态数据到文件中 DeleteContainer删除 sandbox 中的指定容器 source code 获取 sandbox 中的指定容器，并调用 VCContainer 的 delete，删除维护的容器信息 调用 resCtrl 的 resourceControllerUpdate，更新 sandbox 的 cgroup 调用 store 的 ToDisk，保存状态数据到文件中 StartContainer启动 sandbox 中的 pod_container 容器 source code 获取 sandbox 中的指定容器，并调用 VCContainer 的 start，启动容器 调用 store 的 ToDisk，保存状态数据到文件中 调用 updateResources，热更新 VM 的资源信息 StopContainer关停 sandbox 中的指定容器 source code 获取 sandbox 中的指定容器，并调用 VCContainer 的 stop，关停容器并清理相关资源 调用 store 的 ToDisk，保存状态数据到文件中 KillContainer杀死 sandbox 中的指定容器进程 source code 获取 sandbox 中的指定容器，并调用 VCContainer 的 signalProcess，发送 kill 信号（理论上是这样，然而并未有真实调用） StatusContainer获取 sandbox 中指定容器的状态 source code 获取 sandbox 中的指定容器，获取其状态信息（例如：ID、rootfs、状态、启动时间、annotation、PID 等） StatsContainer获取 sandbox 中指定容器的统计信息 source code 获取 sandbox 中的指定容器，校验其状态是否为 running 调用 agent 的 statsContainer，获取容器状态信息 PauseContainer暂停 sandbox 中的指定容器 source code 获取 sandbox 中的指定容器，校验其状态是否为 running 调用 agent 的 pauseContainer，暂停容器 设置容器状态为 paused 调用 store 的 ToDisk，保存状态数据到文件中 ResumeContainer恢复 sandbox 中的指定容器 source code 获取 sandbox 中的指定容器，校验其状态是否为 running 调用 agent 的 resumeContainer，恢复容器 设置容器状态为 running 调用 store 的 ToDisk，保存状态数据到文件中 EnterContainer在 sandbox 中的指定容器中执行命令 source code 获取 sandbox 中的指定容器，校验其状态是否为 running 调用 agent 的 exec，进入容器执行指定命令 UpdateContainer更新 sandbox 中的指定容器资源规格 source code 获取 sandbox 中的指定容器，校验其状态是否为 running 调用 updateResources，热更新 VM 的资源规格 调用 agent 的 updateContainer，更新容器 调用 resCtrl 的 resourceControllerUpdate，更新 sandbox 的 cgroup 调用 store 的 ToDisk，保存状态数据到文件中 WaitProcess等待 sandbox 中的指定容器进程返回退出码 source code 校验 sandbox 的状态是否为 running 获取 sandbox 中的指定容器，校验其状态是否为 ready 或 running 调用 agent 的 waitProcess，等待进程返回退出码 SignalProcess向 sandbox 中的指定容器进程发送指定信号 source code 校验 sandbox 的状态是否为 running 获取 sandbox 中的指定容器，调用 VCContainer 的 signalProcess，向容器进程发送指定信号 WinsizeProcess设置 sandbox 中的指定容器 tty 大小 source code 校验 sandbox 的状态是否为 running 获取 sandbox 中的指定容器，校验其状态是否为 ready 或 running 调用 agent 的 winsizeProcess，设置进程的 tty 大小 IOStream获取 sandbox 中的指定容器 IO 流 source code 校验 sandbox 的状态是否为 running 获取 sandbox 中的指定容器，校验其状态是否为 ready 或 running 初始化 IO 流，返回其 stdin、stdout 和 stderr AddDevice向 sandbox 中添加设备 source code 调用 devManager 的 NewDevice，初始化对应类型的设备 调用 devManager 的 AttachDevice，添加该设备 AddInterface向 sandbox 中添加网卡 source code 将 rpc 请求体转换成网卡信息结构 调用 network 的 AddEndpoints，热添加该网卡 获取网卡 PCI 地址，调用 agent 的 updateInterface，更新网卡信息 调用 store 的 ToDisk，保存状态数据到文件中 RemoveInterface移除 sandbox 中的指定网卡 source code 调用 network 的 Endpoints，根据 MAC 地址匹配所有网卡中待移除的网卡 调用 network 的 RemoveEndpoints，移除该网卡 调用 store 的 ToDisk，保存状态数据到文件中 ListInterfaces获取 sandbox 的所有网卡配置 source code 调用 agent 的 listInterfaces，获取所有网卡配置 UpdateRoutes更新 sandbox 的路由表 source code 调用 agent 的 updateRoutes，更新路由表 ListRoutes获取 sandbox 的所有路由配置 source code 调用 agent 的 listRoutes，获取所有路由配置 GetOOMEvent获取 sandbox 的 OOM 事件信息 source code 调用 agent 的 getOOMEvent，获取 OOM 事件信息 GetHypervisorPid获取 sandbox 的 hypervisor PID source code 调用 hypervisor 的 GetPids，获取所有 PID 列表 返回 PID 列表首位（因为首位是 hypervisor PID，次位为 virtiofsd PID） UpdateRuntimeMetrics更新 sandbox 的 hypervisor 相关指标 source code 调用 hypervisor 的 GetPids，获取 hypervisor 的 PID 获取 /proc/&lt;hypervisorPID&gt;/fd 目录下的文件数量（进程打开的所有文件描述符，这些文件描述符是指向实际文件的一个符号链接，例如 0 表示 stdin、1 表示 stdout、2 表示 stderr 等等），上报 kata_hypervisor_fds 指标 解析 /proc/&lt;hypervisorPID&gt;/net/dev 文件内容（网络设备状态信息，例如 eth0、lo、tap0_kata 和 tunl0 接受和发送的数据包、错误和冲突的数量以及其他基本统计，参考 ifconfig 命令结果），上报 kata_hypervisor_netdev 指标 解析 /proc/&lt;hypervisorPID&gt;/stat 文件内容（进程的状态信息，参考 ps 命令结果），上报 kata_hypervisor_proc_stat 指标 解析 /proc/&lt;hypervisorPID&gt;/status 文件内容（进程的状态信息，相较于 /proc/&lt;hypervisorPID&gt;/stat 更易读），上报 kata_hypervisor_proc_status 指标 解析 /proc/&lt;hypervisorPID&gt;/io 文件内容（进程的 IO 统计信息），上报 kata_hypervisor_io 指标 调用 hypervisor 的 GetVirtioFsPid，获取 virtiofsd 的 PID 获取 /proc/&lt;virtiofsdPID&gt;/fd 目录下的文件数量，上报 kata_virtiofsd_fds 指标 解析 /proc/&lt;virtiofsdPID&gt;/stat 文件内容，上报 kata_virtiofsd_proc_stat 指标 解析 /proc/&lt;hypervisorPID&gt;/status 文件内容，上报 kata_virtiofsd_proc_status 指标 解析 /proc/&lt;hypervisorPID&gt;/io 文件内容，上报 kata_virtiofsd_io 指标 GetAgentMetrics获取 sandbox 的 agent 相关指标 source code 调用 agent 的 getAgentMetrics，获取 agent 的指标信息 GetAgentURL获取 sandbox 的 agent URI 信息 source code 调用 agent 的 getAgentURl，获取 URI 信息 GuestVolumeStats获取 sandbox 中的指定挂载卷信息 source code 校验卷路径是否存在 遍历所有容器的所有挂载点，获取挂载源为指定卷目录的 sandbox 内的挂载点 调用 agent 的 getGuestVolumeStats，获取卷信息 ResizeGuestVolume调整 sandbox 中的指定挂载卷大小 source code 校验卷路径是否存在 遍历所有容器的所有挂载点，获取挂载源为指定卷目录的 sandbox 内的挂载点 调用 agent 的 resizeGuestVolume，调整卷大小 GetIPTables获取 sandbox 的 iptables 信息 source code 调用 agent 的 getIPTables，获取 iptables 信息 SetIPTables设置 sandbox 的 iptables 信息 source code 调用 agent 的 setIPTables，设置 iptables 信息 VCContainersrc/runtime/virtcontainers/interfaces.go virtcontainers 库中用于管理容器的模块。 1234567891011121314151617181920212223242526272829303132// Container is composed of a set of containers and a runtime environment.// A Container can be created, deleted, started, stopped, listed, entered, paused and restored.type Container struct { ctx context.Context config *ContainerConfig sandbox *Sandbox id string sandboxID string // &lt;sandboxID&gt;/&lt;id&gt; containerPath string // 固定为 rootfs rootfsSuffix string // 容器挂载信息，包括 source、destination、type、options 等。从 OCI spec 解析获得 mounts []Mount // 容器设备信息，包括设备 ID、容器中的设备路径、文件模式等信息 devices []ContainerDevice // 容器状态信息，包括运行状态（ready、running、paused、stopped 以及 creating）、rootfs 的块设备 ID（DeviceMapper 场景下，rootfs 为热添加的块设备）、rootfs 的文件系统类型（rootfs 为块设备时）以及 sandbox 进程所在的 cgroup 路径 state types.ContainerState // 容器进程信息，包括 PID（本质上就是 shimID）、Token 以及启动时间等 process Process // 容器 rootfs 基本信息，包括 source、target、type、options 等 rootFs RootFs // systemMountsInfo.BindMountDev 表示是否将 host 的 /dev 目录以 bind 形式挂载到容器的 /dev 中 // systemMountsInfo.DevShmSize 表示 host 的 /dev/shm 大小 systemMountsInfo SystemMountsInfo} 工厂函数 source code 检验 containerConfig 配置是否合法（containerConfig 取自于 sandboxConfig 中的相关配置） 校验 annotation 中 SWAP 资源声明是否合法（io.katacontainers.container.resource.swappiness 必须小于 200），并透传设置（区别于 CPU 和内存等资源，SWAP 无法通过 spec.Containers.Resources 的方式声明，而需要通过 annotation 声明） 调用 store 的 FromDisk，获取 sandbox 和容器的状态信息。如果成功获取则表明不是新创建的容器，无需后续动作，仅用于状态更新维护，直接返回容器实例即可 处理容器挂载信息，即 container.mounts除了借助 virtcontainers/kata-agent 的共享目录挂载之外，块设备类型的挂载还可以通过 hypervisor 热添加到 VM。这里仅处理挂载源为块设备类型的挂载信息，常规共享目录挂载仍然由 virtcontainers/kata-agent 处理 如果未禁用 [hypervisor].disable_block_device_use，则调用 agent 的 capabilities 和 hypervisor 的 Capabilities，根据 agent 是否具备使用块设备能力以及 hypervisor 是否允许块设备热插拔判断是否当前支持块设备默认场景下，rootfs 由 virtio-fs 传递共享；在未禁用 [hypervisor].disable_block_device_use 时，rootfs 基于块设备创建，并热插到 VM 中使用，以提高性能。例如 devicemapper 场景下 rootfs 必须是基于块设备创建的 针对容器中的所有挂载信息 如果 mounts.BlockDeviceID 已经存在，则表明已经有一个设备和挂载点相关联，因此不需要创建设备，跳过即可 如果挂载类型不是 bind，跳过即可 获取 /run/kata-containers/shared/direct-volumes/&lt;base64 mounts.Source&gt;/mountInfo.json 文件，如果文件存在，表明当前挂载设备需要以直通卷的方式处理，即创建 /run/kata-containers/shared/direct-volumes/&lt;base64 mounts.Source&gt;/&lt;sandboxID&gt; 文件，并替换原本 mounts 中 Source、Type、Options、ReadOnly、FSGroup（取自 mountInfo.Metadata[“FSGroup”]）、FSGroupChangePolicy（取自 mountInfo.Metadata[“FSGroupChangePolicy”]） 等信息为 mountInfo.json 的对应字段，后续支持直通卷的 CSI 会根据此文件与信息与 Kata 交互mounts.Source 格式为 /var/lib/kubelet/pods/&lt;podUID&gt;/volumes/kubernetes.io~csi/&lt;pvName&gt;/mount；mountInfo.json 中 device 字段的格式为 /dev/sda（取决于 host 上的具体设备） 如果挂载源为块设备类型或 PMEM 设备，则调用 devManager 的 NewDevice，初始化挂载块设备信息，回写 mounts.BlockDeviceID 字段信息块设备 DeviceInfo 的 HostPath、ContainerPath 等信息均为 mounts 中信息；PMEM 设备 DeviceInfo 的 HostPath 处理方式比较特殊：如果 DevType 为 c 或者 u，则 backingFile 路径为 /sys/dev/char/&lt;major:minor&gt;/loop/backing_file；如果 DevType 为 b，则 backingFile 路径为 /sys/dev/block/&lt;major:minor&gt;/loop/backing_file。读取 backingFile 文件内容作为 HostPath。此外，判断 HostPath 签名是否合法（当使用 PMEM 设备和 DAX 技术时，需要确保文件或设备路径具有正确的 PFN 签名，以便内核可以正确地管理 PMEM 设备和启用 DAX 技术）以及通过 /proc/mounts 获取 PMEM 的挂载源的文件系统类型 fstype 准备容器设备信息，即 container.devices设备均通过 hypervisor 热添加到 VM 中 针对容器中所有的设备信息，调用 devManager 的 NewDevice，初始化设备信息，并过滤类型为 CDROM 和 floppy 的设备 VCContainer 中声明的 GetAnnotations、GetPid、GetToken、ID、Sandbox 以及 Process 均为参数获取与赋值，无复杂逻辑，不作详述。 start启动容器 source code 校验 sandbox 状态是否为 running 校验容器状态是否为 ready 或 stopped 调用 agent 的 startContainer，启动容器 如果启动失败，则调用 stop，执行回滚操作；否则，设置容器状态为 running，并调用 store 的 ToDisk，保存 sandbox 和容器的状态数据到文件中 stop关停容器，并清理相关资源 source code 如果容器状态已经为 stopped，则不做任何操作 校验容器状态是否为 ready、running 或 paused 调用 signalProcess，向 sandbox 中的容器进程发送 kill 信号 调用 agent 的 waitProcess，确保容器进程已退出 调用 agent 的 stopContainer，关停容器 针对容器中每一个挂载信息，调用 fsShare 的 UnshareFile，移除 host 侧的 sandbox 共享文件 调用 fsShare 的 UnshareRootFilesystem，移除 sandbox 中的容器 rootfs 共享挂载 调用 devManager 的 DetachDevice 和 RemoveDevice，detach 并移除 sandbox 中的所有设备（含块设备） 如果容器的 rootfs 是块设备，则调用 devManager 的 DetachDevice 和 RemoveDevice，detach 并移除容器的 rootfs 块设备 设置容器状态为 stopped，并调用 store 的 ToDisk，保存 sandbox 和容器的状态数据到文件中 delete删除容器信息 source code 校验容器状态是否为 ready 或 stopped 删除维护在 sandbox 中的容器信息 调用 store 的 ToDisk，保存 sandbox 和容器的状态数据到文件中 signalProcess向容器进程发送指定信号 source code 校验 sandbox 状态是否为 ready 或者 running 校验容器状态是否为 ready、running 或者 paused 调用 agent 的 signalProcess，向 sandbox 中的指定容器进程发送信号（由于 Containerd 和 CRIO 并不会处理 ESRCH: No such process 错误，因此 Kata runtime 在这里做了特殊操作，针对此报错仅输出 warning 日志，不作返回）","link":"/2023/01/30/2023-01-30%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20virtcontainers/"},{"title":"「 Kata Containers 」源码走读 — virtcontainers&#x2F;factory","text":"based on 3.0.0 Factorysrc/runtime/virtcontainers/factory.go Factory 继承自 FactoryBase，两者的区别在于 FactoryBase 用于创建 base VM（即为模版 VM），创建后会将其暂停，而 Factory 会在 VM 使用时将其恢复，并热更新 VM 以满足运行时的规格要求。 FactoryBase 有四种实现：direct、template、grpccache 和 cache。但是它们并不会对外暴露使用，而是在 Factory 的工厂函数中根据具体的配置细节初始化对应的实现，作为统一的 Factory 对外提供接口调用，即 factory。 123type direct struct { config vc.VMConfig} 1234type grpccache struct { conn *grpc.ClientConn config *vc.VMConfig} 123456type template struct { // [factory].template_path，默认为 /run/vc/vm/template statePath string config vc.VMConfig} 12345678910111213type cache struct { // cache factory 的初始化必须基于 template factory 或者 direct factory。 base base.FactoryBase // 用于维护创建的 VM 模板 vmm map[*vc.VM]interface{} cacheCh chan *vc.VM closed chan&lt;- int wg sync.WaitGroup closeOnce sync.Once vmmLock sync.RWMutex} 123456// VMConfig is a collection of all info that a new blackbox VM needs.type VMConfig struct { HypervisorType HypervisorType AgentConfig KataAgentConfig HypervisorConfig HypervisorConfig} VMConfig 针对 VM factory 场景下，聚合的配置文件中的相关信息。 123type factory struct { base base.FactoryBase} 工厂函数 目前来看，grpccache 初始化的条件应该不存在。此外，当启用 VM factory 时，必然是 VM template 和 VM cache 二选一，所以 direct 不会作为 factory 直接对外使用，而是进一步初始化成 cache factory。 source code 校验 VMConfig 配置的合法性，其中包括 [hypervisor].kernel 是否不为空，[hypervisor].image 和 [hypervisor].initrd 有且仅有一个。并设置 [hypervisor].default_vcpus 缺省时为 1（单位：Core），[hypervisor].default_memory 缺省时为 2048（单位：MiB） 当启用 VM template 时（即 [factory].enable_template 为 true），则初始化 template factory 如果 fetchOnly 为 true，则校验 [factory].template_path 目录下是否存在 state 和 memory 文件 如果 fetchOnly 为 false，则初始化 template factory 校验 [factory].template_path 目录下是否不存在 state 和 memory 文件 创建 [factory].template_path 目录，将 tmps 挂载到此目录下，大小为 [hypervisor].default_memory + 8 MiB（amd64 架构下为 8 MiB；arm64 架构下为 300 MiB），并在此目录下创建 memory 文件 调用 NewVM，基于 VMConfig 创建 VM（创建后则作为模版 VM） 调用 agent 的 disconnect，断开与 agent 的链接 调用 hypervisor 的 PauseVM，暂停 VM 调用 hypervisor 的 SaveVM，保存 VM 到磁盘文件 调用 hypervisor 的 StopVM，关停 VM 调用 store 的 Destroy，删除状态数据目录 当启用 VM cache 时（即 [factory].vm_cache_number 大于 0），则初始化 cache factory 初始化 direct factory（cache factory 的初始化必须依赖其他 factory） 反复调用 GetBaseVM（direct.GetBaseVM），直至创建暂停状态的 VM 数量等于 [factory].vm_cache_number 将这些事先创建好的 base VM 维护在 cache 中*后续需要时通过 GetBaseVM（cache.GetBaseVM），获取 base VM，并通过 GetVM 热更新* NewVM基于 VMConfig 创建 VM NewVM 并非 FactoryBase 定义接口，而是 virtcontainers 提供的一个基于 VMConfig 创建 VM 的工厂函数，仅用于 factory 相关流程 source code 校验 VMConfig 配置的合法性，其中包括 [hypervisor].kernel 是否不为空，[hypervisor].image 和 [hypervisor].initrd 有且仅有一个。并设置 [hypervisor].default_vcpus 缺省时为 1，[hypervisor].default_memory 缺省时为 2048 初始化 hypervisor 和 agent 调用 hypervisor 的 CreateVM，创建一个不含网络信息的 VM 调用 agent 的 configure 和 setAgentURL，配置 agent 相关信息 调用 hypervisor 的 StartVM，启动 VM 如果 VM 不是从 template 启动（因为从 template 启动的 VM，会进入 pause 状态），则调用 agent 的 check，检测服务存活性 Config获取 base factory 的配置信息 cache、direct、grpccache 和 template 实现方式相同。 返回 VMConfig GetVMStatus获取 base VM 的状态信息 direct、grpccache 和 template 实现下均不支持此接口，会触发 panic。 cachesource code 针对 cache 中缓存的每一个 VM，获取其 CPU 和内存大小（即配置中声明的默认大小），并调用 hypervisor 的 GetPids，获取相关的 PID GetBaseVM获取 base VM directsource code 调用 NewVM，创建 VM 调用 hypervisor 的 PauseVM，将其暂停并返回 templatesource code 调用 NewVM，创建 VM（该 VM 基于模版创建，创建后不作为模版 VM） grpccachesource code gRPC 调用 cache server 的 GetBaseVM，获取 VM 调用 hypervisor 的 fromGrpc，配置 hypervisor 信息 调用 agent 的 configureFromGrpc，配置 agent 信息 基于配置后的 hypervisor、agent 和 gRPC 返回体构建并返回 VM cachesource code 从缓存的 base VM 中返回一个 CloseFactory关闭并销毁 factory direct、grpccache 实现下此接口不做任何处理，直接返回即可。 templatesource code 移除 [factory].template_path 挂载点（默认为 /run/vc/vm/template），并删除此目录 cachesource code 调用 base factory 的 ClostFactory，关闭 factory如上所述，cache factory 也就是调用 direct factory 的 CloseFactory GetVM热更新 base VM 以满足需求 GetVM 不是 base factory 的接口，而是 factory 的接口GetVM 接受一个 VMConfig 类型的参数，该参数描述了预期的 VM 配置（下称 newConfig），而 factory 实现中的 VMConfig 是 base VM 的配置（下称 baseConfig），两者的差异点补齐便是 GetVM 操作的核心逻辑 source code 校验 newConfig 配置的合法性，其中包括 [hypervisor].kernel 是否不为空，[hypervisor].image 和 [hypervisor].initrd 有且仅有一个。并设置 [hypervisor].default_vcpus 缺省时为 1，[hypervisor].default_memory 缺省时为 2048 调用 Config，获取 baseConfig 信息，校验两个配置信息是否并不冲突 调用 GetBaseVM，获取 base VM 调用 hypervisor 的 ResumeVM，将 VM 从暂停状态恢复 借助 /dev/urandom 重新生成随机熵，调用 agent 的 reseedRNG，为 guest 内存重新生成随机数 为了补齐 VM 的暂停时间，调用 agent 的 setGuestDateTime，同步 host 时间至 guest 中 如果 base VM 中的 CPU 数量小于期望配置中的 CPU 数量，则调用 hypervisor 的 HotplugAddDevice，热添加差值 CPU；内存同理 当有 CPU 或内存的热添加动作后，调用 agent 的 onlineCPUMem，通知 agent 上线资源 Cache Serversrc/runtime/protocols/cachecache.pb.go cache server 并非默认启动的 gRPC 服务，而是在 VM Cache 特性启用时，通过 kata-runtime factory init 命令启动。 12345type cacheServer struct { rpc *grpc.Server factory vc.Factory done chan struct{}} 启动函数 source code 基于配置，初始化一个新的 factory（即 fetchOnly 为 false） 启动 gRPC 服务，监听 [factory].vm_cache_endpoint 地址（默认为 /var/run/kata-containers/cache.sock） Config获取 base factory 的配置信息 source code 返回体结构如下 123456type GrpcVMConfig struct { // VMConfig Data []byte // VMConfig.AgentConfig AgentConfig []byte} 调用 factory 的 Config，获取 base factory 的配置信息由于 base factory 在 cache server 启动后便固定规格，因此在首次调用后，会保存配置信息，之后的调用直接返回即可 GetBaseVM获取 base VM source code 返回体结构如下 1234567891011121314type GrpcVM struct { // VM.id Id string // VM.hypervisor.toGrpc Hypervisor []byte ProxyPid int64 ProxyURL string // VM.cpu Cpu uint32 // VM.memory Memory uint32 // VM.cpuDelta CpuDelta uint32} 调用 factory 的 Config，获取 base factory 的配置信息 调用 factory 的 GetBaseVM，获取 base VM Status获取 base VM 的状态信息 source code 返回体结构如下 123456type GrpcStatus struct { // 当前进程的 PID Pid int64 // factory.GetVMStatus 的返回结果 Vmstatus []*GrpcVMStatus} 调用 factory 的 GetVMStatus，获取 base VM 的状态信息 Quit关闭 cache server source code 1 秒钟后，关闭 cache gRPC server","link":"/2023/03/12/2023-03-12%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20virtcontainers%20factory/"},{"title":"「 Kata Containers 」源码走读 — virtcontainers&#x2F;resource controller","text":"based on 3.0.0 ResourceControllersrc/runtime/pkg/resourcecontrol/controller.go ResourceController 在 Linux 上的实现为 LinuxCgroup，而 LinuxCgroup 具体体现为两种：sandboxController 和 overheadController： 当 [runtime].sandbox_cgroup_only 开启时，顾名思义仅有 sandboxController，用于管理 Pod 所有的线程资源 当 [runtime].sandbox_cgroup_only 未开启时，资源分为两类，其中 vCPU 线程资源会由 sandboxController 管理，其余资源由 overheadController 管理 具体执行标准参考 OCI runtime-spec：https://github.com/opencontainers/runtime-spec/blob/main/config-linux.md。 12345678910111213141516171819type LinuxCgroup struct { sync.Mutex // cgroup 实现，其中类型包括 legacy、hybrid 和 unified，用于区分 cgroups v1 和 v2 cgroup interface{} // cgroup 路径 path string // 待限制的 CPU cpusets *specs.LinuxCPU // 待限制的 sandbox 设备 // 除了创建时指定的 sandbox 设备，还会追加以下设备： // 默认设备：/dev/null、/dev/random、/dev/full、/dev/tty、/dev/zero、/dev/urandom 和 /dev/console // 虚拟化设备：/dev/kvm、/dev/vhost-net、/dev/vfio/vfio 和 /dev/vhost-vsock // wildcard 设备（通过手动指定 major、minor、access 和 type 属性构造的设备）：tuntap、/dev/pts 等 devices []specs.LinuxDeviceCgroup} LinuxCgroup 的实现方式本质上是封装了 github.com/containerd/cgroups 库，用于处理 cgroup 资源。因此，Type、ID、Parent、Delete、Stat、AddProcess、AddThread、Update、MoveTo、AddDevice、RemoveDevice 和 UpdateCpuSet 均为该库针对 cgroup v1 和 v2 不同版本下统一入口的二次封装。该库不支持针对 systemd 创建具有 v1 和 v2 cgroup 的 scope，因此这部分是直接与 systemd 交互创建 cgroup，然后使用 containerd 的 api 加载它。 添加运行时进程，无需调用 setupCgroups 此外，以下的函数声明并非 ResourceController 的接口声明，而是 VCSandbox 的扩展封装，为了便于理解，将其归类至 ResourceController 下。 setupResourceController配置 resourceController source code 调用 sandboxController 或 overheadController（取决于 sandbox 资源是否分开管理，即 [runtime].sandbox_cgroup_only 的配置）的 AddProcess，将当前进程 ID 加入到 cgroup 中管理确保运行时的任何子进程（即服务于 Kata Pod 的所有进程）都将存在于 resourceController 中，且如果有 overheadController 则由 overheadController 管理此类进程以及子进程 resourceControllerUpdate更新 resourceController 以及 cgroup 信息 source code 聚合 sandbox 中所有容器的 CPUSet 和 MEMSet 信息，调用 sandboxController 的 UpdateCpuSet，更新到 cgroup 中 如果 sandbox 的资源分开管理（即存在 overheadController），则调用 hypervisor 的 GetThreadIDs，获取 vCPU 线程，并调用 sandboxController 的 AddThread，将 vCPU 线程的加入到 cgroup 中管理因为当有 overheadController 时，意味着会产生新的 vCPU 线程会作为 hypervisor 的子线程，所以需要并入统一的 cgroup 中管理 resourceControllerDelete删除 resourceController 以及 cgroup 信息 source code 调用 LoadResourceController，根据 sandboxController 的 cgroupPath 获取 sandboxController 调用 sandboxController 的 Parent，获取父级信息；调用 sandboxController 的 MoveTo，将其管理的进程移至父级；并调用 sandboxController 的 Delete，删除 sandboxController 的 cgroup 如果 sandbox 的资源分开管理（即存在 overheadController），则执行同样的操作","link":"/2023/04/09/2023-04-09%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20virtcontainers%20resource%20controller/"},{"title":"「 Kata Containers 」源码走读 — virtcontainers&#x2F;device","text":"based on 3.0.0 DeviceReceiver 是一组相对而言较底层的接口声明，其直接调用 hypervisor 执行设备热插拔等操作；而 Device 描述了设备的实现细节，内部会调用 DeviceReceiver 的接口实现各自的热插拔功能；而 DeviceManager 则对外提供设备管理能力，其内部屏蔽了设备的具体类型，而是直接调用 Device 的接口管理设备。 DeviceReceiversrc/runtime/pkg/device/api/interface.go DeviceReceiver 的实现由 Sandbox 接口完成。 DeviceReceiver 中声明的 GetHypervisorType 为参数获取，无复杂逻辑，不作详述。 HotplugAddDevice热添加设备到 sandbox 中 source code 调用 sandboxController 的 AddDevice，将 device 的 GetHostPath 添加到 cgroup 管理中 如果设备类型为 vfio 调用 device 的 GetDeviceInfo，获取 iommu group 中所有设备 调用 hypervisor 的 HotplugAddDevice，热添加所有 vfio 设备group 是 IOMMU 能够进行 DMA 隔离的最小硬件单元，一个 group 内可能只有一个 device，也可能有多个 device，这取决于物理平台上硬件的 IOMMU 拓扑结构。 设备直通的时候一个 group 里面的设备必须都直通给一个虚拟机。 不能够让一个group 里的多个 device 分别从属于 2 个不同的 VM，也不允许部分 device 在 host 上而另一部分被分配到 guest 里， 因为就这样一个 guest 中的 device 可以利用 DMA 攻击获取另外一个 guest 里的数据，就无法做到物理上的 DMA 隔离。 如果设备类型为 block 或 vhost-user-blk-pci，直接调用 hypervisor 的 HotplugAddDevice，热添加设备 如果设备类型为 generic（即非 vfio、block 或者 vhost-user 设备），则不做操作根据注释的 TODO，猜测后续版本会有操作，截至 3.0.0 暂无逻辑 如果为其他设备类型，则不做操作 HotplugRemoveDevice热移除 sandbox 中的设备 source code 如果设备类型为 vfio 调用 device 的 GetDeviceInfo，获取 iommu group 中所有设备 调用 hypervisor 的 HotplugRemoveDevice，热移除所有 vfio 设备 如果设备类型为 block（非 PMEM 设备，因为持久内存设备无法热移除）或 vhost-user-blk-pci 调用 device 的 GetDeviceInfo，获取设备详情 调用 hypervisor 的 HotplugRemoveDevice，热移除设备 如果设备类型为 generic（即非 vfio、block 或者 vhost-user 设备），则不做操作根据注释的 TODO，猜测后续版本会有操作，截至 3.0.0 暂无逻辑 如果为其他设备类型，则不做操作 调用 sandboxController 的 RemoveDevice，将 device 的 GetHostPath 从 cgroup 管理中移除 GetAndSetSandboxBlockIndex获取并设置 virtio-block 索引，仅支持 virtio-blk 和 virtio-scsi 类型设备 用于记录分配给 sandbox 中容器的块设备索引（通过 BlockIndexMap（map[int]struct{}）） source code 获取维护在 sandbox.state.BlockIndexMap 中，从 0 到 65534 范围内没有被使用的索引 ID UnsetSandboxBlockIndex释放记录的 virtio-block 索引，仅支持 virtio-blk 和 virtio-scsi 类型设备 用于记录分配给 sandbox 中容器的块设备索引（通过 BlockIndexMap（map[int]struct{}）） source code 移除维护在 sandbox.state.BlockIndexMap（map[int]struct{}）中的索引 AppendDevice向 sandbox 中添加一个 vhost-user 类型的设备，用于向 hypervisor 传递启动参数 source code 如果设备类型为 vhost-user-scsi-pci、virtio-net-pci、vhost-user-blk-pci 和 vhost-user-fs-pci 调用 device 的 GetDeviceInfo，获取设备信息 调用 hypervisor 的 AddDevice，添加设备 如果设备类型为 vfio 调用 device 的 GetDeviceInfo，获取 vfio group 中所有设备 调用 hypervisor 的 AddDevice，添加所有 vfio 设备 其余设备类型均不支持 Devicesrc/runtime/pkg/device/api/interface.go Device 有以下实现方式：GenericDevice、VFIODevice、BlockDevice、VhostUserBlkDevice、VhostUserFSDevice、VhostUserNetDevice 和 VhostUserSCSIDevice，其中均以 GenericDevice 为基础，扩展部分方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// DeviceInfo is an embedded type that contains device data common to all types of devices.type DeviceInfo struct { // DriverOptions is specific options for each device driver // for example, for BlockDevice, we can set DriverOptions[&quot;block-driver&quot;]=&quot;virtio-blk&quot; DriverOptions map[string]string // Hostpath is device path on host HostPath string // ContainerPath is device path inside container ContainerPath string `json:&quot;-&quot;` // Type of device: c, b, u or p // c , u - character(unbuffered) // p - FIFO // b - block(buffered) special file // More info in mknod(1). DevType string // ID for the device that is passed to the hypervisor. ID string // Major, minor numbers for device. Major int64 Minor int64 // FileMode permission bits for the device. FileMode os.FileMode // id of the device owner. UID uint32 // id of the device group. GID uint32 // Pmem enabled persistent memory. Use HostPath as backing file // for a nvdimm device in the guest. Pmem bool // If applicable, should this device be considered RO ReadOnly bool // ColdPlug specifies whether the device must be cold plugged (true) // or hot plugged (false). ColdPlug bool} DeviceInfo 描述了设备的属性信息，通常是根据 OCI spec 中获得，并根据具体的实际设备类型覆盖。 12345678// VFIODevice is a vfio device meant to be passed to the hypervisor// to be used by the Virtual Machine.type VFIODevice struct { *GenericDevice // 元素为 /sys/kernel/iommu_groups/&lt;DeviceInfo.HostPath&gt;/devices 目录下的所有子设备（IOMMU）详情 VfioDevs []*config.VFIODev} 一个 VFIO 设备也就是一组 IOMMU 设备。 12345// BlockDevice refers to a block storage device implementation.type BlockDevice struct { *GenericDevice BlockDrive *config.BlockDrive} 12345// VhostUserBlkDevice is a block vhost-user based devicetype VhostUserBlkDevice struct { *GenericDevice VhostUserDeviceAttrs *config.VhostUserDeviceAttrs} 12345// VhostUserFSDevice is a virtio-fs vhost-user devicetype VhostUserFSDevice struct { *GenericDevice config.VhostUserDeviceAttrs} 12345// VhostUserNetDevice is a network vhost-user based devicetype VhostUserNetDevice struct { *GenericDevice *config.VhostUserDeviceAttrs} 12345// VhostUserSCSIDevice is a SCSI vhost-user based devicetype VhostUserSCSIDevice struct { *GenericDevice *config.VhostUserDeviceAttrs} 12345678910// GenericDevice refers to a device that is neither a VFIO device, block device or VhostUserDevice.type GenericDevice struct { // 设备的通用属性信息 ID string DeviceInfo *config.DeviceInfo // 设备引用与 Attach 计数 RefCount uint AttachCount uint} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// VFIODev represents a VFIO drive used for hotplugging// /sys/kernel/iommu_groups/&lt;DeviceInfo.HostPath&gt;/devices 目录下的所有文件均视为一个 VFIODevtype VFIODev struct { // ID is used to identify this drive in the hypervisor options. // 格式为 vfio-&lt;DeviceInfo.ID&gt;&lt;idx&gt;，最长保留 31 位，其中 idx 为遍历文件的递增索引 ID string // Type of VFIO device // VFIO 设备进一步分为两种类型，可以通过文件名区别： // - 常规类型，例如 0000:04:00.0 // - mediated 类型，例如 f79944e4-5a3d-11e8-99ce-479cbab002e4 Type VFIODeviceType // BDF (Bus:Device.Function) of the PCI address // - 常规类型，例如 0000:04:00.0 -&gt; 04:00.0 // - mediated 类型，例如 f79944e4-5a3d-11e8-99ce-479cbab002e4 -&gt; /sys/kernel/iommu_groups/&lt;DeviceInfo.HostPath&gt;/devices/f79944e4-5a3d-11e8-99ce-479cbab002e4 -&gt; /sys/devices/pci0000:00/0000:00:02.0/f79944e4-5a3d-11e8-99ce-479cbab002e4（软链接关系）-&gt; 0000:00:02.0 -&gt; 00:02.0 BDF string // sysfsdev of VFIO mediated device // - 常规类型，例如 0000:04:00.0 -&gt; /sys/bus/pci/devices/0000:04:00.0 // - mediated 类型，例如 f79944e4-5a3d-11e8-99ce-479cbab002e4 -&gt; /sys/kernel/iommu_groups/&lt;DeviceInfo.HostPath&gt;/devices/f79944e4-5a3d-11e8-99ce-479cbab002e4 -&gt; /sys/devices/pci0000:00/0000:00:02.0/f79944e4-5a3d-11e8-99ce-479cbab002e4（软链接关系） SysfsDev string // IsPCIe specifies device is PCIe or PCI // 根据 /sys/bus/pci/devices/0000:&lt;BDF&gt;/config 文件大小判断是否为 PCI 设备 // - PCI 设备，大小为 256 // - PCIe 设备，大小为 4096 IsPCIe bool // PCI Class Code // /sys/bus/pci/devices/0000:&lt;BDF&gt;/class 文件内容 Class string // Bus of VFIO PCIe device // 如果为 PCIe 设备，则记录名称为 rp&lt;idx&gt;，其中 idx 为当前记录的 PCIe 总设备数量 Bus string // VendorID specifies vendor id VendorID string // DeviceID specifies device id DeviceID string // Guest PCI path of device GuestPciPath vcTypes.PciPath} VFIODev 描述了 VFIODevice 设备特有的属性信息，也可以理解为 IOMMU 设备的信息。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// BlockDrive represents a block storage drive which may be used in case the storage// driver has an underlying block storage device.type BlockDrive struct { // File is the path to the disk-image/device which will be used with this drive // - BlockDevice：&lt;DeviceInfo.HostPath&gt; // - SWAP：&lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/swap&lt;idx&gt;，其中 idx 为 sandbox 中 SWAP 文件递增索引 File string // Format of the drive // - BlockDevice：DeviceInfo.DriverOptions[&quot;fstype&quot;] 指定，默认为 raw // - SWAP：固定为 raw Format string // ID is used to identify this drive in the hypervisor options. // - BlockDevice：格式为 drive-&lt;DeviceInfo.ID&gt;&lt;idx&gt;，最长保留 31 位 // - SWAP：sandbox 中 SWAP 文件递增索引 ID string // MmioAddr is used to identify the slot at which the drive is attached (order?). MmioAddr string // SCSI Address of the block device, in case the device is attached using SCSI driver // SCSI address is in the format SCSI-Id:LUN // - BlockDevice：如果 DeviceInfo.DriverOptions[&quot;block-driver&quot;] 为 virtio-scsi（不指定默认也为 virtio-scsi），则根据 Index 获取 SCSI-Id（Index / 256）以及 LUN（Index % 256），最终的 SCSI 地址为 &lt;SCSI-Id&gt;:&lt;LUN&gt; SCSIAddr string // NvdimmID is the nvdimm id inside the VM NvdimmID string // VirtPath at which the device appears inside the VM, outside of the container mount namespace // - BlockDevice：如果 DeviceInfo.DriverOptions[&quot;block-driver&quot;] 不为 virtio-scsi 或者 nvdimm，则进一步判断 // -- 如果 block-driver 为 virtio-blk 或 virtio-blk-ccw，则索引为 Index // -- 如果 block-driver 为 virtio-mmio，则索引为 Index + 1 // 根据索引计算出设备路径，例如 0 -&gt; /dev/vda，25 -&gt; /dev/vdz，27 -&gt; /dev/vdab VirtPath string // DevNo identifies the css bus id for virtio-blk-ccw DevNo string // PCIPath is the PCI path used to identify the slot at which the drive is attached. PCIPath vcTypes.PciPath // Index assigned to the drive. In case of virtio-scsi, this is used as SCSI LUN index // - BlockDevice：调用 DeviceReceiver.GetAndSetSandboxBlockIndex 获取 Index int // ShareRW enables multiple qemu instances to share the File ShareRW bool // ReadOnly sets the device file readonly // - BlockDevice：&lt;DeviceInfo.ReadOnly&gt; ReadOnly bool // Pmem enables persistent memory. Use File as backing file // for a nvdimm device in the guest // - BlockDevice：&lt;DeviceInfo.Pmem&gt; Pmem bool // This block device is for swap Swap bool} BlockDrive 描述了 BlockDevice 设备特有的属性信息，除了在 BlockDevice 中使用，SWAP 和 VM 镜像也是由 BlockDrive 构建。 123456789101112131415161718192021222324252627282930// VhostUserDeviceAttrs represents data shared by most vhost-user devicestype VhostUserDeviceAttrs struct { // VhostUserBlkDevice：格式为 blk-&lt;DeviceInfo.ID&gt;&lt;idx&gt;，最长保留 31 位 DevID string // VhostUserBlkDevice：&lt;DeviceInfo.HostPath&gt; SocketPath string // MacAddress is only meaningful for vhost user net device MacAddress string // These are only meaningful for vhost user fs devices Tag string Cache string // VhostUserBlkDevice：固定为 vhost-user-blk-pci Type DeviceType // PCIPath is the PCI path used to identify the slot at which // the drive is attached. It is only meaningful for vhost // user block devices PCIPath vcTypes.PciPath // Block index of the device if assigned // 默认为 -1，如果 DeviceInfo.DriverOptions[&quot;block-driver&quot;] 为 virtio-blk、virtio-blk-ccw 或 virtio-mmio，调用 DeviceReceiver.GetAndSetSandboxBlockIndex 获取 Index int CacheSize uint32} VhostUserDeviceAttrs 描述了 VhostUserBlkDevice、VhostUserFSDevice、VhostUserNetDevice 和 VhostUserSCSIDevice 设备特有的属性信息。 Device 中声明的 DeviceID、GetAttachCount、GetHostPath 和 GetMajorMinor 均为参数获取与赋值，无复杂逻辑，不作详述。此外，DeviceType 返回各自 Device 实现的类型（如 generic、vfio、vhost-user-blk-pci、vhost-user-fs-pci、virtio-net-pci 和 vhost-user-scsi-pci）；GetDeviceInfo 返回各自 Device 实现的属性信息；Reference 和 Dereference 用于维护设备的引用计数，未达到最多（^uint(0)，即 2 的 64 次方减一）和最少引用时，则计数加一或减一并返回；Save 和 Load 用于 Device 和 DeviceState（结构类似，用于描述状态数据）之间转换，不同的实现额外赋值其各自的属性信息。 bumpAttachCount记录设备的 attach 次数 bumpAttachCount 并非 Device 声明的接口，而是 GenericDevice 的一个常用方法，用于判断是否需要执行实际 attach 或 detach 操作，函数入参中的 bool 用于表明是否为 attach 操作，出参中的 bool 用于表明是否为单纯的计数。 source code 如果为 attach 操作 如果当前 attach 计数为 0，则计数加一，并返回 false，即需要执行实际的 attach 操作 如果当前 attach 计数为 ^uint(0)（即 2 的 64 次方减一），则返回 true 和设备 attach 次数过多的错误 除此之外，默认计数加一，并返回 true，即不需要执行实际的 attach 操作 如果为 detach 操作 如果当前 attach 计数为 0，则返回 true 和设备并未 attach 的错误 如果当前 attach 次数为 1，则计数减一，并返回 false，即需要执行实际的 detach 操作 除此之外，默认计数减一，并返回 true，即不需要执行实际的 detach 操作 Attachattach 设备 根据不同的实现，可能是冷添加或者热添加 GenericDevicesource code 调用 bumpAttachCount，维护 attach 计数，不执行实际操作 VFIODevicesource code 调用 bumpAttachCount，维护 attach 计数，判断是否执行后续实际操作 遍历 /sys/kernel/iommu_groups/&lt;device.DeviceInfo.HostPath&gt;/devices，获取 VFIO 设备的 BDF（PCIe 总线中的每一个功能都有一个唯一的标识符与之对应。这个标识符就是 BDF，即 Bus，Device，Function）、sysfsDev 和设备类型，判断是否为 PCIe 设备，获取 PCI class 等信息，如果为 PCIe 设备，生成 Bus 信息具体参考 VFIODev 结构体注释 如果设备必须冷添加，则调用 devReceiver 的 AppendDevice，添加设备；否则调用 devReceiver 的 HotplugAddDevice，热添加设备 BlockDevicesource code 调用 bumpAttachCount，维护 attach 计数，判断是否执行后续实际操作 调用 devReceiver 的 GetAndSetSandboxBlockIndex，设置并返回可用的索引 ID 根据 device.DeviceInfo.DriverOptions[“block-driver”]，回写对应的字段（SCSIAddr 和 VirtPath） 如果未指定则视为 virtio-scsi，根据索引 ID 计算出 SCSIAddr，格式为 &lt;index / 256&gt;:&lt;index % 256&gt;qemu 代码建议 scsi-id 可以取值从 0 到 255（含），而 lun 可以取值从 0 到 16383（含）。 但是超过 255 的 lun 值似乎不遵循一致的 SCSI 寻址。 因此限制为 255 如果指定不为 nvdimm，则根据索引 ID 计算出 VirtPath，例如 /dev/vda其中，索引 0 对应 vda，25 对应 vdz，27 对应 vdab，704 对应 vdaac，18277 对应 vdzzz 调用 devReceiver 的 HotplugAddDevice，热添加设备 VhostUserBlkDevicesource code 调用 bumpAttachCount，维护 attach 计数，判断是否执行后续实际操作 根据 device.DeviceInfo.DriverOptions[“block-driver”]，判断 block-driver 是否是 virtio-blk如果未指定则视为 virtio-scsi；如果指定为 virtio-blk、virtio-blk-ccw 或 virtio-mmio 则视为 virtio-blk 如果是 virtio-blk，则调用 devReceiver 的 GetAndSetSandboxBlockIndex，获取未被使用的块索引；否则，索引默认为 -1 调用 devReceiver 的 HotplugAddDevice，热添加设备 VhostUserFSDevice、VhostUserNetDevice、VhostUserSCSIDevicesource code VhostUserFSDevice、VhostUserNetDevice 和 VhostUserSCSIDevice 实现方式一致，以 GenericDevice 为例 调用 bumpAttachCount，维护 attach 计数，判断是否执行后续实际操作 调用 devReceiver 的 AppendDevice，添加设备 Detachdetach 设备 不同的实现下未必支持 detach 操作 GenericDevice、VhostUserFSDevice、VhostUserNetDevice、VhostUserSCSIDeviceGenericDevice、VhostUserFSDevice、VhostUserNetDevice 和 VhostUserSCSIDevice 实现方式一致，以 GenericDevice 为例 source code 调用 bumpAttachCount，维护 attach 计数，不执行实际操作 VFIODevicesource code 调用 bumpAttachCount，维护 attach 计数，判断是否执行后续实际操作 如果设备是冷添加的，说明没有运行后的 attach 动作，因此则无需 detach；否则，调用 devReceiver 的 HotplugRemoveDevice，热移除设备 BlockDevicesource code 调用 bumpAttachCount，维护 attach 计数，判断是否执行后续实际操作 调用 devReceiver 的 HotplugRemoveDevice，热移除设备 VhostUserBlkDevicesource code 调用 bumpAttachCount，维护 attach 计数，判断是否执行后续实际操作 调用 devReceiver 的 HotplugRemoveDevice，热移除设备 根据 device.DeviceInfo.DriverOptions[“block-driver”]，判断 block-driver 是否是 virtio-blk。如果是 virtio-blk，则调用 devReceiver 的 UnsetSandboxBlockIndex，释放记录的 virtio-block 索引如果未指定则视为 virtio-scsi；如果指定为 virtio-blk、virtio-blk-ccw 或 virtio-mmio 则视为 virtio-blk DeviceManagersrc/runtime/pkg/device/api/interface.go 1234567891011121314151617181920type deviceManager struct { sync.RWMutex // VM 中的设备 devices map[string]api.Device // [hypervisor].block_device_driver，rootfs 块设备驱动，可选有 virtio-scsi、virtio-blk 和 nvdimm blockDriver string // [hypervisor].vhost_user_store_path，默认为 /var/run/kata-containers/vhost-user // Its sub-path &quot;block&quot; is used for block devices; &quot;block/sockets&quot; is // where we expect vhost-user sockets to live; &quot;block/devices&quot; is where // simulated block device nodes for vhost-user devices to live. vhostUserStorePath string // [hypervisor].enable_vhost_user_store，默认为 false // Enabling this will result in some Linux reserved block type // major range 240-254 being chosen to represent vhost-user devices. vhostUserStoreEnabled bool} Device 中声明的 IsDeviceAttached、GetDeviceByID 和 GetAllDevices 为参数获取，无复杂逻辑，不作详述。 NewDevice初始化设备 source code 如果设备不是 pmem 类型（即 devInfo.Pmem 为 false） 如果启用了 [hypervisor].enable_vhost_user_store、devInfo.DevType 为 b 并且设备 devInfo.Major 是 242（即 vhost-user-scsi）或者 241（即 vhost-user-blk），则获取 &lt;vhostUserStorePath&gt;/block/devices 目录下，格式为 major:minor 的文件名，作为 socket 文件，返回 &lt;vhostUserStorePath&gt;/block/sockets/&lt;socket&gt; 文件路径用于获取 vhost-user 设备的主机路径。 对于 vhost-user 块设备，如 vhost-user-blk 或 vhost-user-scsi，其 socket 应位于目录 &lt;vhostUserStorePath&gt;/block/sockets/ 下，它对应的设备节点应该在目录 &lt;vhostUserStorePath&gt;/block/devices/ 下 如果 devInfo.DevType 为 c 或者 u，则 uevent 路径为 /sys/dev/char/&lt;major:minor&gt;/uevent；如果 devInfo.DevType 为 b，则 uevent 路径为 /sys/dev/block/&lt;major:minor&gt;/uevent。如果 uevent 文件不存在，则返回 devInfo.ContainerPath，否则读取文件内容（文件为 ini 格式），解析 DEVNAME 项，返回 /dev/&lt;DEVNAME &gt; 文件路径某些设备（例如 /dev/fuse、/dev/cuse）并不总是在 /sys/dev 下实现 sysfs 接口，这些设备默认由 docker 传递。 只需返回在设备配置中传递的路径，这确实意味着这些设备不支持设备重命名 设置 devInfo.HostPath 为上述返回的路径 根据 devInfo.Major 和 devInfo.Minor，判断设备是否已经存在 deviceManager 的 devices 中，存在则直接返回即可 为了避免 deviceID 冲突，重新生成 devInfo.ID 根据设备类别，初始化对应的设备 如果 devInfo.HostPath 为 /dev/vfio/xxx（排除 /dev/vfio/vfio 字符设备），则视为 vfio 设备类型 如果 devInfo.DevType 为 b，并且 devInfo.Major 为 241，则视为 vhost-user-blk 设备类型 如果 devInfo.DevType 为 b，则视为 block 设备类型（也就是 devInfo.Major 不为 241） 除此之外，均视为 generic 设备类型（也就是 vhost-user-fs、vhost-user-net 和 vhost-user-scsi 设备均为此类型） 调用 device 的 Reference，维护设备的引用计数 维护 deviceManager 中的设备信息，其中 key 为调用 device 的 DeviceID 获得，后续用于判断设备是否已经创建 RemoveDevice移除维护的设备信息 source code 校验设备是否已经创建 调用 device 的 Dereference，移除引用 如果移除后引用为 0，则并调用 device 的 GetAttachCount，校验当前设备 attach 次数是否为 0，移除维护在 deviceManager 的设备信息 AttachDeviceattach 设备 source code 校验设备是否已经创建 调用 device 的 Attach，attach 设备 DetachDevicedetach 设备 source code 校验设备是否已经创建 调用 device 的 GetAttachCount，校验当前设备 attach 次数是否不为 0 调用 device 的 Detach，detach 设备 LoadDevices加载设备信息 source code 遍历入参 []config.DeviceState 中每一个设备信息，根据其类型初始化对应的 device 对象 调用 device 的 Load，加载设备 维护 deviceManager 中的设备信息，其中 key 为调用 device 的 DeviceID 获得，后续用于判断设备是否已经创建","link":"/2023/03/18/2023-03-18%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20virtcontainers%20device/"},{"title":"「 Kata Containers 」源码走读 — virtcontainers&#x2F;network","text":"based on 3.0.0 Endpointsrc/runtime/virtcontainers/endpoint.go Endpoint 代表了某一个物理或虚拟网络设备的基础结构，具体包括：veth、ipvlan、macvlan、macvtap、physical、vhostuser、tap 和 tuntap 8 种实现方式。借助 github.com/vishvananda/netlink 将抽象 endpoint 类型转变成具体的 netlink 类型，配置后回写到 endpoint 的具体属性（例如 netPair 等）后，交由 hypervisor 创建或配置该设备信息。 12345678910111213141516171819202122// VethEndpoint gathers a network pair and its properties.type VethEndpoint struct { // 固定为 virtual EndpointType EndpointType // idx 为 VM 中 endpoint 设备的递增序号 // NetPair.TapInterface.Name 为逻辑网桥名称，固定为 br&lt;idx&gt;_kata // NetPair.TapInterface.TAPIface.Name 为 tap 设备名称，固定为 tap&lt;idx&gt;_kata // NetPair.VirtIface.Name 为 endpoint 设备名称，默认为 eth&lt;idx&gt; // NetPair.VirtIface.HardAddr 为随机生成的 MAC 地址 // NetPair.NetInterworkingModel 为 [runtime].internetworking_model，可选有 macvtap 和 tcfilter（默认） NetPair NetworkInterfacePair PCIPath vcTypes.PciPath // endpoint 设备属性信息 EndpointProperties NetworkInfo // endpoint 设备 inbound/outbound 限速标识 RxRateLimiter bool TxRateLimiter bool} 12345678910111213141516171819202122// IPVlanEndpoint represents a ipvlan endpoint that is bridged to the VMtype IPVlanEndpoint struct { // 固定为 ipvlan EndpointType EndpointType // idx 为 VM 中 endpoint 设备的递增序号 // NetPair.TapInterface.Name 为逻辑网桥名称，固定为 br&lt;idx&gt;_kata // NetPair.TapInterface.TAPIface.Name 为 tap 设备名称，固定为 tap&lt;idx&gt;_kata // NetPair.VirtIface.Name 为 endpoint 设备名称，默认为 eth&lt;idx&gt; // NetPair.VirtIface.HardAddr 为随机生成的 MAC 地址 // NetPair.NetInterworkingModel 为 tcfilter NetPair NetworkInterfacePair PCIPath vcTypes.PciPath // endpoint 设备属性信息 EndpointProperties NetworkInfo // endpoint 设备 inbound/outbound 限速标识 RxRateLimiter bool TxRateLimiter bool} 12345678910111213141516171819202122// MacvlanEndpoint represents a macvlan endpoint that is bridged to the VMtype MacvlanEndpoint struct { // 固定为 macvlan EndpointType EndpointType // idx 为 VM 中 endpoint 设备的递增序号 // NetPair.TapInterface.Name 为逻辑网桥名称，固定为 br&lt;idx&gt;_kata // NetPair.TapInterface.TAPIface.Name 为 tap 设备名称，固定为 tap&lt;idx&gt;_kata // NetPair.VirtIface.Name 为 endpoint 设备名称，默认为 eth&lt;idx&gt; // NetPair.VirtIface.HardAddr 为随机生成的 MAC 地址 // NetPair.NetInterworkingModel 为 [runtime].internetworking_model，可选有 macvtap 和 tcfilter（默认） NetPair NetworkInterfacePair PCIPath vcTypes.PciPath // endpoint 设备属性信息 EndpointProperties NetworkInfo // endpoint 设备 inbound/outbound 限速标识 RxRateLimiter bool TxRateLimiter bool} 12345678910111213141516171819// MacvtapEndpoint represents a macvtap endpointtype MacvtapEndpoint struct { // 固定为 macvtap EndpointType EndpointType // 元素数量等于 [hypervisor].default_vcpus 的 /dev/tap&lt;EndpointProperties.Iface.Index&gt; 文件句柄 VMFds []*os.File // 元素数量等于 [hypervisor].default_vcpus 的 /dev/vhost-net 文件句柄 VhostFds []*os.File PCIPath vcTypes.PciPath // endpoint 设备属性信息 EndpointProperties NetworkInfo // endpoint 设备 inbound/outbound 限速标识 RxRateLimiter bool TxRateLimiter bool} 123456789101112131415161718192021// PhysicalEndpoint gathers a physical network interface and its propertiestype PhysicalEndpoint struct { // 固定为 physical EndpointType EndpointType // 根据 IfaceName 解析获得，类比于 ethtool -i &lt;IfaceName&gt; 结果中的 bus-info BDF string // 软链接 /sys/bus/pci/devices/&lt;BDF&gt;/driver 指向实体文件路径的基础 Driver string // 由 /sys/bus/pci/devices/&lt;BDF&gt;/vendor 和 /sys/bus/pci/devices/&lt;BDF&gt;/device 文件内容拼接而成 VendorDeviceID string PCIPath vcTypes.PciPath // endpoint 设备属性信息 IfaceName string HardAddr string EndpointProperties NetworkInfo} 12345678910111213141516// VhostUserEndpoint represents a vhost-user socket based network interfacetype VhostUserEndpoint struct { // 固定为 vhost-user EndpointType EndpointType // Path to the vhost-user socket on the host system // 根据 endpoint 设备的所有 IP，获得一个存在的 /tmp/vhostuser_&lt;IP&gt;/vhu.sock 路径 SocketPath string PCIPath vcTypes.PciPath // endpoint 设备属性信息 HardAddr string IfaceName string EndpointProperties NetworkInfo} 123456789101112131415161718// TapEndpoint represents just a tap endpointtype TapEndpoint struct { // 固定为 tap EndpointType EndpointType // TapInterface.Name 为 endpoint 设备名称，默认为 eth&lt;idx&gt; // TapInterface.TAPIface.Name 为 tap 设备名称，固定为 tap&lt;idx&gt;_kata TapInterface TapInterface PCIPath vcTypes.PciPath // endpoint 设备属性信息 EndpointProperties NetworkInfo // endpoint 设备 inbound/outbound 限速标识 RxRateLimiter bool TxRateLimiter bool} 12345678910111213141516171819202122232425262728// TuntapEndpoint represents just a tap endpointtype TuntapEndpoint struct { // 固定为 tuntap EndpointType EndpointType // idx 为 VM 中设备的递增序号 // TuntapInterface.Name 为 endpoint 设备名称，默认为 eth&lt;idx&gt; // TuntapInterface.TAPIface.Name 为 tap 设备名称，固定为 tap&lt;idx&gt;_kata // TuntapInterface.TAPIface.HardAddr 为 tap 设备 MAC 地址 TuntapInterface TuntapInterface // idx 为 VM 中设备的递增序号 // NetPair.TapInterface.Name 为逻辑网桥名称，固定为 br&lt;idx&gt;_kata // NetPair.TapInterface.TAPIface.Name 为 tap 设备名称，固定为 tap&lt;idx&gt;_kata // NetPair.VirtIface.Name 为 endpoint 设备名称，默认为 eth&lt;idx&gt; // NetPair.VirtIface.HardAddr 为随机生成的 MAC 地址 // NetPair.NetInterworkingModel 为 [runtime].internetworking_model，可选有 macvtap 和 tcfilter（默认） NetPair NetworkInterfacePair PCIPath vcTypes.PciPath // endpoint 设备属性信息 EndpointProperties NetworkInfo // endpoint 设备 inbound/outbound 限速标识 RxRateLimiter bool TxRateLimiter bool} 123456789// NetworkInterfacePair defines a pair between VM and virtual network interfaces.type NetworkInterfacePair struct { // 取决于具体 endpoint 实现，内容有所不同 TapInterface VirtIface NetworkInterface // [runtime].internetworking_model，可选的有 macvtap 和 tcfilter（默认） NetInterworkingModel} NetworkInterfacePair 即 netpair（例如 br0_kata），描述了 tap 设备（TapInterface）和 veth 设备（VirtIface，即位于容器命名空间内部的 veth-pair 设备，如 eth0）的数据结构（netPair 并非真实设备，而是一个用于描述如何连通容器网络和 VM 网络的逻辑网桥）。 12345678910// NetworkInfo gathers all information related to a network interface.// It can be used to store the description of the underlying network.type NetworkInfo struct { Iface NetlinkIface DNS DNSInfo Link netlink.Link Addrs []netlink.Addr Routes []netlink.Route Neighbors []netlink.Neigh} NetworkInfo 描述 endpoint 设备的通用属性信息，通过相关 Golang 系统调用库获得。 Endpoint 中声明的 Properties、Type、PciPath、SetProperties、SetPciPath、GetRxRateLimiter、SetRxRateLimiter、GetTxRateLimiter 和 GetTxRateLimiter 均为参数获取与赋值，无复杂逻辑，不作详述。其中，Name、HardwareAddr 和 NetworkPair 视不同的 endpoint 实现，取值字段有所不同，具体为： Endpoint Name HardwareAddr NetworkPair VethEndpoint NetPair.VirtIface.Name NetPair.TAPIface.HardAddr NetPair IPVlanEndpoint NetPair.VirtIface.Name NetPair.TAPIface.HardAddr NetPair MacvlanEndpoint NetPair.VirtIface.Name NetPair.TAPIface.HardAddr NetPair MacvtapEndpoint EndpointProperties.Iface.Name EndpointProperties.Iface.HardwareAddr — PhysicalEndpoint IfaceName HardAddr — VhostUserEndpoint IfaceName HardAddr — TapEndpoint TapInterface.Name TapInterface.TAPIface.HardAddr — TuntapEndpoint TuntapInterface.Name TapInterface.TAPIface.HardAddr NetPair Attach添加 endpoint 设备到 VM 中 VethEndpoint、IPVlanEndpoint、MacvlanEndpoint、TuntapEndpointsource code 调用 network 的 xConnectVMNetwork，配置网络信息 调用 hypervisor 的 AddDevice，以 NetDev 类型添加 endpoint 设备到 VM 中 MacvtapEndpointsource code 创建 /dev/tap&lt;endpoint.EndpointProperties.Iface.Index&gt;，构建 fds（[]*os.File，元素为数量等于 [hypervisor].default_vcpus 的 /dev/tap&lt;endpoint.EndpointProperties.Iface.Index&gt; 文件句柄），回写到 endpoint.VMFds 中 如果 [hypervisor].disable_vhost_net 未开启，则创建 /dev/vhost-net，构建 fds（[]*os.File，元素为数量等于 [hypervisor].default_vcpus 的 /dev/vhost-net 文件句柄），回写到 endpoint.VhostFds 中 调用 hypervisor 的 AddDevice，以 NetDev 类型添加 endpoint 设备到 VM 中 PhysicalEndpointsource code 将 endpoint.BDF 写入 /sys/bus/pci/devices/&lt;endpoint.BDF&gt;/driver/unbind 文件中用于解除该设备在 host driver 上的绑定 将 endpoint.VendorDeviceID 写入 /sys/bus/pci/drivers/vfio-pci/new_id 文件中；并将 endpoint.BDF 写入 /sys/bus/pci/drivers/vfio-pci/bind 文件中用于将该设备绑定到 vfio-pci driver 上，后续以 vfio-passthrough 传递给 hypervisor 获取 /sys/bus/pci/devices/&lt;endpoint.BDF&gt;/iommu_group 软链接的指向路径，得到其 base 路径（即路径最后一个元素），构建 vfio 设备路径，即 /dev/vfio/&lt;base&gt; 根据 vfio 设备路径，获取设备信息，构建 DeviceInfo，并调用 devManager 的 NewDevice，初始化 vfio 类型设备 调用 devManager 的 AttachDevice，冷添加此设备到 VM 中 VhostUserEndpointsource code 调用 hypervisor 的 AddDevice，以 VhostuserDev 类型添加 virtio-net-pci 设备（socketPath、MacAddress 等信息从 endpoint 中赋值）到 VM 中 TapEndpointsource code 暂不支持添加此类设备，返回错误 Detach移除 VM 中的 endpoint 设备 VethEndpoint、IPVlanEndpoint、MacvlanEndpointsource code 如果 netns 不是由 Kata Containers 创建的，则直接跳过后续根据创建 pod_sandbox 或者 single_container 时，spec.Linux.Namespace 中的 network 是否指定判断，如果未指定，表示需要由 Kata Containers 创建，反之表示 netns 已经提前创建好 进入到该 netns 中，调用 network 的 xDisconnectVMNetwork，移除网络信息 MacvtapEndpoint、VhostUserEndpointsource code 无任何操作，直接返回 PhysicalEndpointsource code 将 endpoint.BDF 写入 /sys/bus/pci/devices/&lt;endpoint.BDF&gt;/driver/unbind 文件中用于解除该设备在 vfio-pci driver 上的绑定 将 endpoint.VendorDeviceID 写入 /sys/bus/pci/drivers/vfio-pci/remove_id 文件中；并将 endpoint.BDF 写入 /sys/bus/pci/drivers/&lt;endpoint.Driver&gt;/bind 文件中用于将该设备绑定到 host driver 上 TapEndpoint、TuntapEndpointsource code 如果 netns 不是由 Kata Containers 创建的，并且 netns 路径存在，则直接跳过后续 进入到该 netns 中，获取名为 tap0_kata（示例名称，其中 0 为递增生成的索引）的设备，关停并移除 HotAttach热添加 endpoint 设备到 VM 中 VethEndpointsource code 调用 Network 的 xConnectVMNetwork，配置网络信息 调用 hypervisor 的 HotplugAddDevice，以 NetDev 类型热添加 endpoint 设备到 VM 中 IPVlanEndpoint、MacvlanEndpoint、MacvtapEndpoint、PhysicalEndpoint、VhostUserEndpointsource code 暂不支持热添加此类设备，返回错误 TapEndpointsource code 创建名为 tap0_kata（示例名称，其中 0 为递增生成的索引）的 tuntap 设备（mode 为 tap；队列长度取自 [hypervisor].default_vcpus 最大为 1，即如果队列长度大于 1，为了避免不支持多队列，需要重置为 1，参考 tuntap 实现），并返回空的 fds，回写到 endpoint.TapInterface.VMFds 中 如果 [hypervisor].disable_vhost_net 未开启，则创建 /dev/vhost-net，构建 fds（[]*os.File，元素为队列长度数量的 /dev/vhost-net 文件句柄），回写到 endpoint.TapInterface.VhostFds 中 设置 endpoint.TapInterface.TAPIface.HardAddr 为 veth 设备的 MAC 地址将 veth MAC 地址保存到 tap 中，以便稍后用于构建 hypervisor 命令行。 此 MAC 地址必须是 VM 内部的地址，以避免任何防火墙问题。 host 上的网络插件预期流量源自这个 MAC 地址 设置 tuntap 设备的 mtu 值为 veth 设备的 mtu 值 启用 tuntap 设备 调用 hypervisor 的 HotplugAddDevice，以 NetDev 类型热添加 endpoint 设备到 VM 中 TuntapEndpointsource code 创建名为 tap0_kata（示例名称，其中 0 为递增生成的索引）的 tuntap 设备（mode 为 tap；队列长度取自 [hypervisor].default_vcpus 最大为 1，即如果队列长度大于 1，为了避免不支持多队列，需要重置为 1，参考 tuntap 实现） 设置 endpoint.TuntapInterface.TAPIface.HardAddr 为 veth 设备的 MAC 地址将 veth MAC 地址保存到 tap 中，以便稍后用于构建 hypervisor 命令行。 此 MAC 地址必须是 VM 内部的地址，以避免任何防火墙问题。 host 上的网络插件预期流量源自这个 MAC 地址 设置 tuntap 设备的 mtu 值为 veth 设备的 mtu 值 启用 tuntap 设备 调用 hypervisor 的 HotplugAddDevice，以 NetDev 类型热添加 endpoint 设备到 VM 中 HotDetach热移除 VM 中的 endpoin 设备 VethEndpointsource code 如果 netns 不是由 Kata Containers 创建的，则直接跳过后续根据创建 pod_sandbox 或者 single_container 时，spec.Linux.Namespace 中的 network 是否指定判断，如果未指定，表示需要由 Kata Containers 创建，反之表示 netns 已经提前创建好 进入到该 netns 中，调用 xDisconnectVMNetwork，移除网络信息 调用 hypervisor 的 HotplugRemoveDevice，以 NetDev 热移除 endpoint 中 VM 的设备 IPVlanEndpoint、MacvlanEndpoint、MacvtapEndpoint、PhysicalEndpoint、VhostUserEndpointsource code 暂不支持热移除此类设备，返回错误 TapEndpoint、TuntapEndpointsource code 进入到该 netns 中，获取名为 tap0_kata（示例名称，其中 0 为递增生成的索引）的设备，关停并移除 调用 hypervisor 的 HotplugRemoveDevice，以 NetDev 热移除 VM 中的 endpoint 设备 Networksrc/runtime/virtcontainers/network.go 实际操作均借助 github.com/vishvananda 实现，该库提供了等价于 ip addr、ip link、tc qdisc、tc filter 等命令行的功能。 12345678910111213141516// LinuxNetwork represents a sandbox networking setup.type LinuxNetwork struct { // OCI spec 中类型为 network 的 linux.namespace.path netNSPath string // netns 中的 endpoint 设备 eps []Endpoint // [runtime].internetworking_model，可选有 macvtap 和 tcfilter（默认） interworkingModel NetInterworkingModel // 表示当前 netns 是否为 Kata Containers 创建 // - false：netns 为事先准备好，创建 Kata 容器时，在 OCI spec 中传递该 netns（network 类型的 linux.namespace）。例如 Kubernetes 场景下，netns 由 CNI 创建 // - true：Kata Containers 发现 OCI spec 中不存在 network 类型的 linux.namespace，则会手动创建一个 netns（以 cnitest 开头）。例如 Containerd 场景下，运行 single_container netNSCreated bool} Network 中声明的 NetworkID、NetworkCreated、Endpoints 和 SetEndpoints 均为参数获取与赋值，无复杂逻辑，不作详述。其中，Run 是封装了进入 netns 中执行回调函数的流程。 xConnectVMNetwork根据不同的网络模型，打通容器和 VM 之间的网络 source code 调用 endpoint 的 NetworkPair，获取 netPair 对象的网络模型（即[runtime].internetworking_model，默认为 tcfilter） 调用 hypervisor 的 Capabilities，判断 hypervisor 是否支持多队列特性。如果支持，则队列数设为 [hypervisor].default_vcpus；否则为 0 根据网络模型，创建对应的 tap 设备，连通容器和 VM 之间的网络 无论哪种网络模式，VM 中的 eth0 都是 hypervisor 基于 tap 设备虚拟化出来，并 attach 到 VM 中建立两者的关联关系。区别在于 tap 设备和 veth 设备（即 CNI 为容器内分配的 eth0）的网络打通方式 如果网络模型为 macvtap 调用 endpoint 的 NetworkPair，获取 netPair 对象，并进一步获取 veth 设备 创建 macvtap 设备，其中名称为 tap0_kata（示例名称，其中 0 为递增生成的索引）、txQLen 属性继承自 veth 设备且 parentIndex 指向容器 eth0 设备（下称 veth 设备）目前 macvtap 场景下需要特殊处理索引（该索引后续用作命名 /dev/tap&lt;idx&gt;），是由于 Linux 内核中存在一个限制，会导致 macvtap/macvlan link 在网络 namespace 中创建时无法获得正确的 link 索引https://github.com/clearcontainers/runtime/issues/708在修复该错误之前，需要随机一个非冲突索引（即 8192 + 随机一个数字）并尝试创建一个 link。 如果失败，则继续重试，上限为 128 次。所有内核都不会检查链接 ID 是否与主机上的 link ID 冲突，因此需要偏移 link ID 以防止与主机索引发生任何重叠，内核将确保没有竞争条件 设置 netPair.TAPIface.HardAddr 为 veth 设备的 MAC 地址将 veth MAC 地址保存到 tap 中，以便稍后用于构建 hypervisor 命令行。 此 MAC 地址必须是 VM 内部的地址，以避免任何防火墙问题。 host 上的网络插件预期流量源自这个 MAC 地址 设置 macvtap 设备的 mtu 值为 veth 设备的 mtu 值 设置 veth 设备的 MAC 地址为随机生成的 MAC 地址（即 netPair.VirtIface.HardAddr，该字段初始化时为随机生成的 MAC 地址），并设置 macvtap 设备的 MAC 地址为 veth 设备的 MAC 地址 启用 macvtap 设备 获取 veth 设备的全部 IP 地址，保存至 netPair.VirtIface.Addrs，并从 veth 设备中移除这些 IP 地址清理掉 veth 设备中由 CNI 分配的 IP 地址，避免 ARP 冲突 根据步骤 2 中生成随机索引，创建 /dev/tap&lt;idx&gt;，构建 fds（[]*os.File，元素为队列长度数量的 /dev/tap&lt;idx&gt; 文件句柄），回写到 netPair.VMFds 中 如果 [hypervisor].disable_vhost_net 未开启，则创建 /dev/vhost-net，构建 fds（[]*os.File，元素为队列长度数量的 /dev/vhost-net 文件句柄），回写到 netPair.VhostFds 中 综上所述，macvtap 网络模式下，是将 veth 设备和 macvtap 设备的 mac 地址等信息互换，并将 veth 设备的网络信息转移到 VM 中 eth0 设备（实质上是清理 veth 设备网络信息，同时借助 VM dhcp 获取 CNI 分配的 IP 地址），结合 macvtap 设备的 parentIndex 指向 veth 设备，实现容器网络流量和 VM 网络流量的互通。 如果网络模型为 tcfilter 调用 endpoint 的 NetworkPair，获取 netPair 对象，并进一步获取 veth 设备 创建名为 tap0_kata（示例名称，其中 0 为递增生成的索引）的 tuntap 设备（mode 为 tap；队列长度最大为 1，即如果队列长度大于 1，为了避免不支持多队列，需要重置为 1，参考 tuntap 实现），并返回空的 fds，回写到 netPair.VMFds 中 如果 [hypervisor].disable_vhost_net 未开启，则创建 /dev/vhost-net，构建 fds（[]*os.File，元素为队列长度数量的 /dev/vhost-net 文件句柄），回写到 netPair.VhostFds 中 设置 netPair.TAPIface.HardAddr 为 veth 设备的 MAC 地址将 veth MAC 地址保存到 tap 中，以便稍后用于构建 hypervisor 命令行。 此 MAC 地址必须是 VM 内部的地址，以避免任何防火墙问题。 host 上的网络插件预期流量源自这个 MAC 地址 设置 tuntap 设备的 mtu 值为 veth 设备的 mtu 值 启用 tuntap 设备 为 tuntap 设备和 veth 设备分别创建 ingress 类型的网络队列规则与 tc 规则，将一方的入站流量重定向到另一方进行出站处理，使得所有流量在两者之间可以被重定向 综上所述，tcfilter 网络模式下，仅仅是在 veth 和 tap 设备之间配置 tc 规则，实现容器网络流量和 VM 网络流量的互通。 效果示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# 网络模型为 macvtap 时$ ip netns exec cni-97333755-9052-db96-37fe-37d4e39bf046 ethtool -i tap0_katadriver: macvlanversion: 0.1firmware-version: expansion-rom-version: bus-info: supports-statistics: nosupports-test: nosupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: no$ ip netns exec cni-fb0bd424-5621-3672-62d9-9233708dc54d ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.04: eth0@if18: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1430 qdisc noqueue state UP group default link/ether 46:ba:a7:d6:85:ec brd ff:ff:ff:ff:ff:ff link-netnsid 055446: tap0_kata@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1430 qdisc fq_codel state UP group default qlen 1500 link/ether c6:f1:06:ac:46:53 brd ff:ff:ff:ff:ff:ff inet6 fe80::c4f1:6ff:feac:4653/64 scope link valid_lft forever preferred_lft forever$ kata-runtime exec 7af17cb96ddaa59a4e370c0de584ea6df5759278ce6c203a188a3ab18b461216 root@localhost:/# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1430 qdisc fq_codel state UP group default qlen 1000 link/ether c6:f1:06:ac:46:53 brd ff:ff:ff:ff:ff:ff inet 10.244.69.173/32 brd 10.244.69.173 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::c4f1:6ff:feac:4653/64 scope link valid_lft forever preferred_lft forever# 网络模型为 tcfilter 时$ ip netns exec cni-d7e932c4-51a6-53e0-e73c-662aa84b4653 ethtool -i tap0_katadriver: tunversion: 1.6firmware-version: expansion-rom-version: bus-info: tapsupports-statistics: nosupports-test: nosupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: no$ ip netns exec cni-d7e932c4-51a6-53e0-e73c-662aa84b4653 ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.04: eth0@if17: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1430 qdisc noqueue state UP group default qlen 1000 link/ether 1e:03:66:df:ad:5e brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.69.163/32 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::1c03:66ff:fedf:ad5e/64 scope link valid_lft forever preferred_lft forever5: tap0_kata: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1430 qdisc mq state UNKNOWN group default qlen 1000 link/ether ee:b0:99:52:54:ef brd ff:ff:ff:ff:ff:ff inet6 fe80::ecb0:99ff:fe52:54ef/64 scope link valid_lft forever preferred_lft forever$ kata-runtime exec 8a390592512f2f27a35accd0fa5c2c82d29dea2f3d1eb982c6225be7856e78a6 root@localhost:/# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1430 qdisc fq_codel state UP group default qlen 1000 link/ether 1e:03:66:df:ad:5e brd ff:ff:ff:ff:ff:ff inet 10.244.69.163/32 brd 10.244.69.163 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::1c03:66ff:fedf:ad5e/64 scope link valid_lft forever preferred_lft forever xDisconnectVMNetwork根据不同的网络模型，移除容器和 VM 之间的网络配置 source code 调用 endpoint 的 NetworkPair，获取 netPair 对象的网络模型（即[runtime].internetworking_model，默认为 tcfilter） 根据网络模型，移除对应的 tap 设备 如果网络模型为 macvtap 调用 endpoint 的 NetworkPair，获取 netPair 对象，并进一步获取 macvtap 设备与 veth 设备 移除 macvtap 设备 将 veth 设备的 MAC 地址还原（在 xConnextVMNetwork 流程中保存在 netPair.TAPIface.HardAddr） 关停 veth 设备 将 veth 设备的 IP 地址还原（在 xConnextVMNetwork 流程中保存在 netPair.VirtIface.Addrs） 如果网络模型为 tcfilter 调用 endpoint 的 NetworkPair，获取 netPair 对象，并进一步获取 tuntap 设备与 veth 设备 关停 tuntap 设备，并移除 删除 veth 设备所有的 tc 规则与 ingress 类型的网络队列规则 关停 veth 设备 addSingleEndpoint添加 endpoint 设备到 VM 中 source code 根据网口的类型，初始化对应的 endpoint物理设备是根据 ethtools 获取指定网口名称的 bus 信息判断，如果 bus 格式为 0000:00:03.0（即以冒号切分后长度为 3），则表示为物理设备；vhost-user 设备是根据 /tmp/vhostuser_&lt;addr&gt;/vhu.sock（其中 addr 为网卡的每一个地址）文件是否存在，如果存在，则表示为 vhost-user 设备；tuntap 设备仅支持 tap mode 调用 endpoint 的 SetProperties，设置 endpoint 属性信息 根据是否为 hotplug，则调用 endpoint 的 HotAttach 或 Attach，热（添加）endpoint 设备到 VM 中 调用 hypervisor 的 IsRateLimiterBuiltin，判断是否内置支持限速特性。如果本身不支持限速（例如 QEMU），则需要额外配置： 网络 I/O inbound 带宽限速（即 [hypervisor].rx_rate_limiter_max_rate 大于 0） veth、ipvlan、tuntap 和 macvlan 类型的 endpoint，待限速设备为 endpoint.NetPair 的 tap 设备；macvtap 和 tap 类型的 endpoint，待限速设备为其本身，即 endpoint.Name() 调用 endpoint 的 SetRxRateLimiter，设置 inbound 限速标识 获取待限速设备的索引，使用 HTB（Hierarchical Token Bucket）qdisc traffic shaping 方案来控制网口流量，设置 class 的 rate 和 ceil 均为 [hypervisor].rx_rate_limiter_max_rateclass 1:2 是基于 class 1:1 创建，两者的 rate 和 ceil 流控指标保持一致，class 1:2 最终作为默认的 class，class 1:n 用于限制特定流量（截至 Kata 3.0，暂未实现）之所以创建了 class 1:2 作为默认的 class，是一种常规做法，一般 class 1:1 承担限制整体的最大速率，class 1:2 用于控制非特权流量。如果统一由 class 1:1 负责，可能会导致非特权流量无法得到适当的控制和优先级管理。没有专门的子类别来定义规则和限制非特权流量，可能会导致这些流量占用过多的带宽，从而影响网络的性能和服务质量；难以灵活地调整限制策略。如果需要根据具体情况对非特权流量进行不同的限制和优先级分配，使用单一的1:1类别会显得不够灵活。而有一个专门的子类别，可以根据需要定义更具体的规则和策略，更好地控制非特权流量。所以，通过设置专门的 class 1:2，可以更好地组织和管理流量，确保网络的资源分配和性能满足特定的需求和优先级 1234567891011+-----+ +---------+ +-----------+ +-----------+| | | qdisc | | class 1:1 | | class 1:2 || NIC | | htb | | rate | | rate || | --&gt; | def 1:2 | --&gt; | ceil | -+-&gt; | ceil |+-----+ +---------+ +-----------+ | +-----------+ | | +-----------+ | | class 1:n | | | rate | +-&gt; | ceil | | +-----------+ 网络 I/O outbound 带宽限速（即 [hypervisor].tx_rate_limiter_max_rate 大于 0） veth、ipvlan、tuntap 和 macvlan 类型的 endpoint 且当网络模型为 tcfilter 时，待限速设备为 endpoint.NetPair 的 veth 设备，当网络模型为 macvtap 或 none 时，待限速设备为 endpoint.NetPair 的 tap 设备；macvtap 和 tap 类型的 endpoint，待限速设备为设备本身，即 endpoint.Name() 对于 veth、ipvlan、tuntap 和 macvlan 类型的 endpoint 且当网络模型为 tcfilter 时，则获取 endpoint.NetPair 中 veth 设备的索引，同样的使用 HTB（Hierarchical Token Bucket）qdisc traffic shaping 方案来控制 veth 网口流量，设置 class 的 rate 和 ceil 均为 [hypervisor].tx_rate_limiter_max_rate对于 tcfilter，只需将 htb qdisc 应用于 veth pair。 对于其他网络模型，例如 macvtap，借助 ifb，通过将 endpoint 设备入口流量重定向到 ifb 出口，然后将 htb 应用于 ifb 出口，实现限速 其他场景时，调用 endpoint 的 SetTxRateLimiter，设置 outbound 限速标识 尝试加载 host 的 ifb 模块，创建名为 ifb0 的 ifb 设备并启用，返回 ifb 设备索引号 为待限速的设备创建 ingress 类型的网络队列规则 为待限速设备添加过滤器规则，将其入站流量重定向到 ifb 设备进行出站处理 使用 HTB（Hierarchical Token Bucket）qdisc traffic shaping 方案来控制 ifb 网口流量，设置 class 的 rate 和 ceil 均为 [hypervisor].tx_rate_limiter_max_rate 限速示例（veth endpoint） 12345678910111213141516171819202122232425262728293031323334353637# inbound 限速为 1024，outbound 限速为 2048$ cat /etc/kata-containers/configuration.toml | grep rate_limiter_max_raterx_rate_limiter_max_rate = 1024tx_rate_limiter_max_rate = 2048# 网络模型为 macvtap 时$ ip netns exec cni-593e147b-3839-2615-f57f-39dc53181ef5 tc qdisc showqdisc noqueue 0: dev lo root refcnt 2 qdisc noqueue 0: dev eth0 root refcnt 2 qdisc htb 1: dev tap0_kata root refcnt 2 r2q 10 default 2 direct_packets_stat 0 direct_qlen 1500qdisc ingress ffff: dev tap0_kata parent ffff:fff1 ---------------- qdisc htb 1: dev ifb0 root refcnt 2 r2q 10 default 2 direct_packets_stat 0 direct_qlen 32## inbound 限速作用在 tap0_kata 设备上$ ip netns exec cni-593e147b-3839-2615-f57f-39dc53181ef5 tc class show dev tap0_kataclass htb 1:1 root rate 1024bit ceil 1024bit burst 1600b cburst 1600b class htb 1:2 parent 1:1 prio 0 rate 1024bit ceil 1024bit burst 1600b cburst 1600b ## outbound 限速作用在 ifb0 设备上$ ip netns exec cni-593e147b-3839-2615-f57f-39dc53181ef5 tc class show dev eth0$ ip netns exec cni-593e147b-3839-2615-f57f-39dc53181ef5 tc class show dev ifb0class htb 1:1 root rate 2048bit ceil 2048bit burst 1600b cburst 1600b class htb 1:2 parent 1:1 prio 0 rate 2048bit ceil 2048bit burst 1600b cburst 1600b# 网络模型为 tcfilter 时$ ip netns exec cni-58d2c6b0-b9e5-797d-4c9f-291769802ac1 tc qdisc showqdisc noqueue 0: dev lo root refcnt 2 qdisc htb 1: dev eth0 root refcnt 2 r2q 10 default 2 direct_packets_stat 0 direct_qlen 1000qdisc ingress ffff: dev eth0 parent ffff:fff1 ---------------- qdisc htb 1: dev tap0_kata root refcnt 257 r2q 10 default 2 direct_packets_stat 0 direct_qlen 1000qdisc ingress ffff: dev tap0_kata parent ffff:fff1 ---------------- ## inbound 限速作用在 tap0_kata 设备上$ ip netns exec cni-58d2c6b0-b9e5-797d-4c9f-291769802ac1 tc class show dev tap0_kataclass htb 1:1 root rate 1024bit ceil 1024bit burst 1600b cburst 1600b class htb 1:2 parent 1:1 prio 0 rate 1024bit ceil 1024bit burst 1600b cburst 1600b ## outbound 限速作用在容器 veth pair 的 eth0 设备上$ ip netns exec cni-58d2c6b0-b9e5-797d-4c9f-291769802ac1 tc class show dev eth0class htb 1:1 root rate 2048bit ceil 2048bit burst 1600b cburst 1600b class htb 1:2 parent 1:1 prio 0 rate 2048bit ceil 2048bit burst 1600b cburst 1600b AddEndpoints添加 endpoint 设备到 VM 中 source code 如果未指定 endpoint，则默认添加 netns 中所有的 enpoint 设备 针对 netns 中每一个网络设备接口信息（即 NetworkInfo），获得其名称、类型、IP 地址、路由、ARP neighbor 等信息（后续会设置在 endpoint.EndpointProperties 中，用于描述 endpoint 的属性） 忽略缺少 IP 地址的网络接口，以及本地回环接口缺少 IP 地址意味着要么是没有命名空间的基本隧道设备，如 gre0、gretap0、sit0、ipip0、tunl0，要么是错误设置的接口 进入到该 netns 中，调用 addSingleEndpoint，向 VM 中添加 endpoint 设备 否则，针对每一个 endpoint，进入到该 netns 中，调用 addSingleEndpoint，向 VM 中添加 endpoint 设备 RemoveEndpoints移除 VM 中的 endpoint 设备 source code 如果未指定 endpoint，则默认为 netns 中所有的 endpoint 设备（也就是 AddEndpoints 中添加的 endpoint 设备），针对每一个待移除的 endpoint 调用 endpoint 的 GetRxRateLimiter，如果设置了 inbound 限速，则进入到该 netns 中，移除限速设备 htb 类型的网络队列规则本质上就是对 addSingleEndpoint 中 inbound 限速处理的逆操作 调用 endpoint 的 GetTxRateLimiter，如果设置了 outbound 限速，则进入到该 netns 中，移除限速设备 htb 类型的网络队列规则、删除限速设备所有的 tc 规则与 ingress 类型的网络队列规则以及关停并移除 ifb0 设备本质上就是对 addSingleEndpoint 中 outbound 限速处理的逆操作 根据是否为 hotplug，则调用 endpoint 的 HotDetach 或 Detach，（热）移除 VM 中的 endpoint 设备 如果 netns 是由 Kata Containers 创建，并且未指定 endpoint（即删除了 netns 中所有的 endpoint），则移除该 netns 的挂载点，并删除该 netns","link":"/2023/04/15/2023-04-15%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20virtcontainers%20network/"},{"title":"「 OpenFaaS 」架构与组件概述","text":"based on 0.26.3 简介 Serverless Functions Made Simple OpenFaaS 使开发人员可以轻松地将事件驱动（event-driven）的功能和微服务部署到 Kubernetes 中，而无需重复的模板代码。OpenFaaS 将代码或现有的二进制文件打包到 Docker 镜像中，使其具有自动缩放和服务指标的高度可扩展点。 OpenFaaS 亮点 支持丰富 UI 和一键安装，便于使用 借助模板库 或 Dockerfile 以任何语言编写服务和函数 构建和发布代码至 Docker 镜像或其他 OCI 兼容格式的镜像中 易于移植，借助 faas-netes 可在现有硬件或公有/私有云上运行 支持 YAML 格式的命令行工具 — faas-cli ，用于模板化和定义函数 自动缩放，支持流量高峰扩容，并在空闲时缩减直至 0 版本丰富，包含社区版、标准版和商业版 设计与架构Stack无论是本地环境、自托管集群，还是带有托管服务（如 AWS Elastic Kubernetes Service (EKS)）的平台，部署 OpenFaaS 的推荐平台都是 Kubernetes。 CI / GitOps layerOpenFaaS 既可以运行函数，也可以运行 HTTP 微服务。每个工作负载都构建到一个容器镜像中，并发布至镜像仓库。 在开发阶段，通常使用 faas-cli 手动操作完成，而在生产阶段，有几个常见的选择： 源代码控制管理（SCM）系统中内置的 CI 工具 GitHub Actions 或 GitLab pipeline 是通过在 Job 中执行 faas-cli deploy 或 faas-cli up 构建和部署函数。部署是在 Job 完成后进行的，将变更推送到集群中。如果需要访问私有 VPC 或本地的集群，可以通过使用私有且安全的入口隧道来实现 使用 ArgoCD 和 Flux 等 GitOps 控制器 GitOps 方式通常在新版本可用时立即持续部署 。部署是通过从特殊的配置库中获取预期状态来进行的 Application Layer OpenFaaS gateway 提供了一个 REST API，用于管理函数、记录指标和缩放 NATS 用于异步函数执行和排队 Prometheus 提供指标并启用 Community Edition 和 OpenFaaS Pro 的自动缩放特性 使用 OpenFaaS Pro，可以通过 HTTP、Cron、AWS SQS 或 Apache Kafka 触发函数。 构成 OpenFaaS 的项目（Prometheus、Linux、OpenFaaS、NATS 和 Kubernetes）可以称为 PLONK Stack。 PLONK Stack 能够运行事件驱动（event-driven）的功能和传统的基于 HTTP 的微服务。 这些应用程序可以通过 Helm charts 或使用 ArgoCD、Flux 等 GitOps 控制器安装。 Infrastructure Layer 函数的执行单元是 Pod，由 Containerd 或 Docker 管理 镜像仓库将每个函数作为不可变的制品保存，可以借助镜像仓库的 REST API、UI 或 CLI 将其部署到 OpenFaaS gateway Kubernetes 是允许函数跨平台，faasd 是小型安装的更简单替代方案 该 Layer 通常在探索和开发期间手动构建，在生产期间使用 Terraform 等工具构建。 工作流程 可以通过其 REST API、CLI 或 UI 访问 OpenFaas Gateway。所有服务或函数都会暴露一个默认路由，但自定义域也可以用于每个端点。 Prometheus 收集指标，这些指标可通过 OpenFaas Gateway 的 API 获得并用于自动缩放。 通过将函数的 URL 从同步的 /function/NAME 转变为异步的 /async-function/NAME，可以使用 NATS Streaming 在队列中运行调用。还可以传递一个可选的回调 URL。 faas-netes 是 OpenFaaS 最受欢迎的编排 Provider，但社区也提供了针对 Docker Swarm、Hashicorp Nomad、AWS Fargate/ECS 和 AWS Lambda 的 Provider。 Provider 使用 faas-provider SDK 构建。 Gatewayhttps://github.com/openfaas/faas/tree/master/gateway API Gateway 为函数提供外部路由，并通过 Prometheus 收集云原生指标。此外，API Gateway 内置的 UI 可用于部署用户自定义的函数或来自 OpenFaaS Function Store 的函数，并调用。 API Gateway 将通过更改 Kubernetes API 中的服务副本计数来满足需求扩展功能。API Gateway 的 /system/alert endpoint 用于接收 AlertManager 生成的自定义告警。 核心特点 内置 UI 支持从 Function Store 部署函数或部署自定义函数 通过 Prometheus 检测 通过 AlertManager 和 Prometheus 自动缩放 缩放至 0 支持 REST API Swagger 文档 以 Kubernetes 作为编排 Provider 的流程示例 WatchdogOpenFaaS watchdog 负责启动和监控 OpenFaaS 中的函数。通过使用 watchdog，任何二进制文件都可以成为一个函数。 watchdog 作为一个“初始化进程”，带有一个用 Golang 编写的嵌入式 HTTP 服务器，它可以支持并发请求、超时和健康检查。和 of-watchdog 类似，但非常适合流式的使用场景或需要在维护关键资源的情况，例如数据库连接、ML 模型或其他数据等请求之间。 官方提供的 templates repository 模板仓库内置了的通用编程语言的 watchdog 模板： Name Language Version Linux base Watchdog Link dockerfile Dockerfile N/A Alpine Linux classic Dockerfile template go Go 1.18 Alpine Linux classic Go template node12 NodeJS 12 Alpine Linux of-watchdog NodeJS template node14 NodeJS 14 Alpine Linux of-watchdog NodeJS template node16 NodeJS 16 Alpine Linux of-watchdog NodeJS template node17 NodeJS 17 Alpine Linux of-watchdog NodeJS template node18 NodeJS 18 Alpine Linux of-watchdog NodeJS template node NodeJS 12 Alpine Linux classic NodeJS template python3 Python 3 Alpine Linux classic Python 3 template python3-debian Python 3 Debian Linux classic Python 3 Debian template python Python 2.7 Alpine Linux classic Python 2.7 template java11-vert-x Java and Vert.x 11 Debian GNU/Linux of-watchdog Java LTS template java11 Java 11 Debian GNU/Linux of-watchdog Java LTS template ruby Ruby 2.7 Alpine Linux 3.11 classic Ruby template php7 PHP 7.4 Alpine Linux classic PHP 7 template php8 PHP 8.1 Alpine Linux classic PHP 8 template csharp C# N/A Debian GNU/Linux 9 classic C# template 此外，还有社区提供的 community template store 模板仓库。 Classic watchdogClassic watchdog 最初用于所有官方 OpenFaaS 模板，但 of-watchdog 现在更受青睐。更多参考：https://github.com/openfaas/classic-watchdog/blob/master/README.md watchdog 调用流程 of-watchdog Reverse proxy for HTTP microservices and STDIO of-watchdog 项目是对上述 Classic Watchdog 的补充（of-watchdog 适用于生产，是 openfaas GitHub 组织的一部分）。它于 2017 年 10 月启动，为 watchdog 和函数之间的通信提供了 STDIO 的替代方案。 of-watchdog 组件的各种模式 of-watchdog 实现了一个监听 8080 端口的 HTTP 服务器，作为运行函数和微服务的反向代理。它可以独立使用，也可以作为 OpenFaaS 容器的入口点。 这个版本的 OpenFaaS 看门狗增加了对 HTTP 代理和 STDIO 的支持，具有内存重用和高速请求服务响应的特性，主要区别在于在调用之间保持函数进程处于待命状态（warm）的能力。Classic watchdog 为每个请求 fork 一个进程，提供最高级别的可移植性，在较新的版本启用了一种 HTTP 模式，在该模式下，可以复用进程以抵消 fork 带来的延迟。 它的目的不是要取代 Classic watchdog，而是为那些需要这些功能的人提供另一种选择。 Auto Scaler仅 OpenFaas Pro 支持。 OpenFaas Pro 的自动缩放的策略是根据以下函数标签进行配置。通过网关的所有调用，无论是同步函数 /function/ 还是异步函数 /async-function ，都采用这种自动缩放配置： Label Description Default com.openfaas.scale.max The maximum number of replicas to scale to. 20 com.openfaas.scale.min The minimum number of replicas to scale to. 1 com.openfaas.scale.zero Whether to scale to zero. false com.openfaas.scale.zero-duration Idle duration before scaling to zero 15m com.openfaas.scale.target Target load per replica for scaling 50 com.openfaas.scale.target-proportion Proportion as a float of the target i.e. 1.0 = 100% of target 0.90 com.openfaas.scale.type Scaling mode of rps, capacity, cpu rps OpenFaaS Pro 自动缩放示例： 缩放依据 OpenFaaS Pro 提供三种自动缩放模式： capacity 基于请求或连接总量。适用于长时间运行的函数或一次只能处理有限数量请求的函数 rps 基于函数每秒完成的请求。非常适合执行速度快且吞吐量高的函数 cpu 基于函数的 CPU 使用率，此策略适用于受 CPU 限制的工作负载，或者在 capacity 和 RPS 模式下未提供最佳扩展配置文件的情况。这里配置的值是以 milli-CPU 为单位的，所以1000 占 1 个 CPU 核 无论哪种缩放模式，都需要在配置函数自动缩放时设置一个目标值，即函数每个副本的平均负载。OpenFaaS 会定期查询并计算当前负载，用于计算预期副本数，规则为： desired = ready pods * ( mean load per pod / target load per pod ) 此外，target-proportion 可用于调整提前或延迟缩放发生的时间： desired = ready pods * ( mean load per pod / ( target load per pod * target-proportion ) ) 流程示例 前置条件为： sleep 函数应用在 capacity 模式下运行，目标负载为 5 个请求量 当前 sleep 函数应用的实际负载为 15 个请求量 sleep 函数应用副本当前为 1 target-proportion 设置为 1.0，即为 100% 缩放流程为： 参考上述规则，平均每个副本的请求量为 15 / 1 = 15，超出 5 个 请求量的预期值，评估副本数为 ceil ( 1 * ( 15 / 5 * 1 ) ) = 3 副本数调整为 3 后，请求量增加到 25，此时平均每个副本的请求量为 25 / 3 = 8.33，评估副本数为 ceil ( 3 * ( 8.33 / 5 * 1 ) ) = 5 当不再有请求时，评估副本数为 ceil ( 3 * ( 0 / 5 * 1) ) = 0 是否支持缩容为 0 取决于 OpenFaaS 版本 设计初衷 在闲置时将函数缩容到零副本可以通过减少集群中所需的节点数量来节省成本，还可以减少静态大小或本地集群上的节点消耗。 在 OpenFaaS 中，缩放到零在默认情况下是关闭的，并且是 OpenFaaS Pro 一部分功能。安装后，空闲函数可以配置为在一段时间内未收到任何请求时缩减。社区建议将此数字设置为最大超时的 2 倍。 可以通过 OpenFaaS 网关的 scale_from_zero 环境变量切换从零副本向上扩展。该特性在 Kubernetes 和 faasd 上默认开启。 对不可用函数的请求，从发送处理到服务处理该请求之间的延迟成为冷启动。 如果不想冷启动怎么办？ OpenFaaS 中的冷启动是严格可选的。对于时间敏感的操作，可以通过至少有 1 个或多个副本来避免冷启动。通过关键函数禁止缩放到 0，或者通过异步路由调用来实现，从而将请求时间与调用者分离 冷启动到底发生了什么？ 冷启动包括以下流程：创建请求在节点上调度容器、找到合适的节点、拉取 Docker 镜像、在容器启动并运行后进行初始检查。可以通过在每个节点上预热镜像以及将 Kubernetes Liveness 和 Readiness Probes 设置为更快的节奏运行，可以降低总开销。更多参考：冷启动进行优化的说明。 当启用 scale_from_zero 时，缓存会保留在内存中，根据每个函数的就绪情况，如果收到请求时函数未就绪，则 HTTP 连接将被阻止，函数将缩放到最小副本，一旦副本可用，请求就会按正常方式处理。具体流程在网关组件的日志中可以看到。更多参考：冷启动概述。 如果函数在按比例缩小时仍在运行怎么办？ 不应该发生，前提是已经为函数的空闲检测设置了足够的值。但如果是这样，OpenFaaS watchdog 和官方函数模板将允许函数正常终止。更多参考：为 OpenFaaS 用户改进长时间运行的作业。 Prometheus 将监控指标发给 AlertManager 之后，AlertManager 会调用 /system/alert 接口，这个接口的 handler 是由 handlers.MakeAlertHandler 方法生成。MakeAlertHandler 方法接收的参数是 ServiceQuery。ServiceQuery 是一个接口，它有两个函数，用来获取或者设置最大的副本数。 12345// ServiceQuery provides interface for replica querying/settingtype ServiceQuery interface { GetReplicas(service, namespace string) (response ServiceQueryResponse, err error) SetReplicas(service, namespace string, count uint64) error} MakeAlertHandler 的函数主要是从 http.Request 中读取 body，然后反序列化成 PrometheusAlert 对象，该对象是一个数组类型，支持对多个函数进行缩放。反序列化之后，调用 handleAlerts 方法，而 handleAlerts 对 alerts 进行遍历，针对每个 alert 调用了 scaleService 方法。scaleService 才是真正处理伸缩服务的函数。 对于 OpenFaaS CE 而言，Auto Scaler 能力相对而言较低，仅支持最大和最小的副本数： Label Description Default com.openfaas.scale.max The maximum number of replicas to scale to. 5 com.openfaas.scale.min The minimum number of replicas to scale to. 1 com.openfaas.scale.factor Define the overall scaling behavior of the function. 20% Faas Providerfaas-provider 提供函数的 CRUD API 以及调用功能。 faas-provider 是一个用 Go 编写的 SDK，它符合 OpenFaaS Provider 的 HTTP REST API。实现接口声明的 provider 应该与 OpenFaaS 工具链和生态系统兼容，包括 UI、CLI、Function Store 和 Template Store。 每个 Provider 都实现以下行为： 函数（或微服务）的 CRUD 通过代理调用函数 函数缩放 Secret 的 CRUD（可选） 日志流（可选） Provider Overview Kubernetes Provider (faas-netes) 针对 Kubernetes 的官方 OpenFaaS Provider，默认内置在 Helm chart 中 faasd Provider (faasd) OpenFaaS 的另一种思路实现，抛去了 Kubernetes 的成本和复杂性。可以在要求非常低的单个主机上运行，且具备快速、易于管理的特点。其底层是由 Containerd 、容器网络接口 （CNI） 以及来自 OpenFaaS 项目的核心组件构成 Docker Swarm Provider faas-swarm) 针对 Docker Swarm 的官方 OpenFaaS Provider，现已弃用且不再维护 faas-memory Provider (faas-memory) 使用本地代码内存空间存储状态，仅用于测试目的和简单示例 社区 Provider 参考实现：https://github.com/openfaas/faas/blob/master/community.md#openfaas-providers faas-netes faasd Log ProviderOpenFaaS 支持集成自定义的 Log Provider。 Log Provider 是一个 HTTP 服务器，对外暴露 /system/logs endpoint，该 endpoint 支持具有以下查询参数的 GET 请求： name - 函数名称（必需） instance - 容器名称（可选），允许从特定函数实例中请求日志 since - 日志起始时间（可选） tail - 日志消息返回的最大数量，&lt;=0 表示无限制 follow - 允许用户请求日志流直至超时（启用时，服务器必须使用 HTTP 分块编码来发送日志的实时流） 默认情况下，OpenFaaS Gateway 会将日志请求代理到函数 Provider。可以在 OpenFaaS Gateway 服务中设置 logs_provider_url 环境变量，OpenFaaS Gateway 会将日志请求代理到此 URL，实现 Log Provider 替换。 Log Provider Overview Kubernetes Provider (faas-netes) Kubernetes Provider 并从 Kubernetes API 查询日志 faasd Provider (faasd) 从 journal 服务中查询日志，按函数和核心服务存储 Grafana Provider (openfaas-loki) 社区提供的 Log Provider，使用 Grafana Loki 来收集和查询功能日志 自定义 Provider 借助 github.com/openfaas/faas-provider/logs 包提供的封装，可以构建自定义的 Log Provider HTTP 服务，参考示例：https://github.com/openfaas/faas-provider/tree/master/logs/example","link":"/2023/05/08/2023-05-08%20OpenFaaS%20%E6%9E%B6%E6%9E%84%E4%B8%8E%E7%BB%84%E4%BB%B6%E6%A6%82%E8%BF%B0/"},{"title":"「 Kata Containers 」资源限制","text":"based on 3.0.0 cgroup 管理Kata Containers 目前支持 cgroups v1 和 v2。 Kata Containers 中，工作负载是在 VM 中运行，VM 由运行在 host 上的 VMM（virtual machine monitor）管理。因此，Kata Containers 运行在两层 cgroup 之上：一层为工作负载所在的 guest，另一层为运行 VMM 和相关线程的 host。 容器 cgroup 路径的配置是在 OCI runtime spec 中声明的 cgroupsPath 字段，可用于控制容器的 cgroup 层次结构以及在容器中运行的进程。在 Kubernetes 场景中，Pod 的 cgroup 是由 Kubelet 管理，而容器的 cgroup 是由运行时管理。 Kubelet 将根据容器资源需求调整 Pod 的 cgroup 大小，其中包含 Pod spec.Overhead 中声明的资源。 Kata Containers 的设计为 sandbox 引入了不可忽略的资源开销。通常，与基于进程级别的容器运行时相比，Kata shim（即 containerd-shim-kata-v2）会调用底层 VMM 创建额外的线程，例如半虚拟化 I/O 后端、VMM 实例以及 Kata shim 进程。这些 host 进程消耗的内存和 CPU 资源是不与容器中的工作负载直接相关，而是属于引入 sandbox 带来的额外开销。为了使 Kata 工作负载在不显着降低性能的情况下运行，必须相应地配置其 sandbox 的开销。因此，可能有两种情况： 上层编排器在调整 Pod cgroup 大小时考虑运行 sandbox 的额外开销。例如，Kubernetes 的 Pod Overhead 特性允许编排器将 sandbox 的额外开销计入其所有容器资源的总和中。在这种情况下，Kata 创建的所有进程都将在 Pod 的 cgroup 约束和限制下运行 上层编排器不考虑 sandbox 的额外开销，因此 Pod 的 cgroup 大小可能无法满足运行 Kata 创建的所有进程。在这种情况下，将所有 Kata 相关进程附加到 Pod 的 cgroup 中可能会导致不可忽略的工作负载性能下降。因此，Kata Containers 会将除 vCPU 线程之外的所有进程移动到名为 /kata_overhead 下的子 cgroup 中。 Kata 运行时不会对该 cgroup 作出任何约束或限制，而由集群管理员选择性设置 Kata Containers 并不会动态检测这两种情况，而是通过配置文件中的 [runtime].sandbox_cgroup_only 选项决定的。 cgroup 种类 Pod cgroup 位于 /kubepods 层级下的子 cgroup，命名为 /kubepods/&lt;PodUID&gt;，由 Kubelet 管理 sandbox cgroup 位于 /kubepods/&lt;PodUID&gt; 层级下的子 cgroup，命名为 /kata_&lt;sandboxID&gt;，由运行时管理 overhead cgroup 位于 /kata_overhead 层级下的子 cgroup，命名为 /kata_overhead/&lt;sandboxID&gt;，由运行时管理 测试负载 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: kataspec: runtimeClassName: kata containers: - name: kata image: ubuntu:18.04 command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;] resources: requests: memory: &quot;1Gi&quot; cpu: &quot;1&quot; limits: memory: &quot;1Gi&quot; cpu: &quot;1&quot; sandbox_cgroup_only = truesandbox_cgroup_only 设置为 true 意味着 Kubelet 在设置 Pod cgroup 的大小时会将 Pod 的额外开销考虑在内（Kubernetes 1.16 起，借助 Pod Overhead 特性）。相对而言，这种方式较为推荐，Kata Containers 所有相关进程都可以简单地放置在给定的 cgroup 路径中。 1234567891011121314151617┌─────────────────────────────────────────┐│ ┌──────────────────────────────────┐ ││ │ ┌─────────────────────────────┐ │ ││ │ │ ┌─────────────────────┐ │ │ ││ │ │ │ vCPU threads │ │ │ ││ │ │ │ I/O threads │ │ │ ││ │ │ │ VMM │ │ │ ││ │ │ │ Kata Shim │ │ │ ││ │ │ │ │ │ │ ││ │ │ │ /kata_&lt;sandboxID&gt; │ │ │ ││ │ │ └─────────────────────┘ │ │ ││ │ │Pod │ │ ││ │ └─────────────────────────────┘ │ ││ │/kubepods │ ││ └──────────────────────────────────┘ ││ Node │└─────────────────────────────────────────┘ 实现细节当启用 sandbox_cgroup_only 时，Kata shim 将在 Pod cgroup 下创建一个名为 /kata_&lt;sandboxID&gt; 的子 cgroup，即 sandbox cgroup。大多数情况下，sandbox cgroup 不作单独约束和限制，而是自继承父 cgroup。cpuset 和 devices cgroup 子系统除外，它们是由 Kata shim 管理。 12345678910111213141516171819202122232425262728293031323334353637# runC 的 cgroup 层级与限制└── /kubepods/pod505eb17b-78d4-4dce-bfb2-60085f629344 ├── tasks (空) ├── cpu.cfs_period_us (-&gt; 100000) ├── cpu.cfs_quota_us (-&gt; 100000) ├── 499316b3661bc989f0999dd51901d2afaad0dda0aa614a2ebcd39f2517e7c56b (业务容器) | ├── tasks (业务进程) | ├── cpu.cfs_period_us (-&gt; 100000) | └── cpu.cfs_quota_us (-&gt; 100000) └── fa6545c433f02a1c712db11cb58bb100a013f9622d725c6a41c60500c20031c5 (infra 容器) ├── tasks (pause 进程) ├── cpu.cfs_period_us (-&gt; 100000) └── cpu.cfs_quota_us (-&gt; -1)# Kata 的 cgroup 层级与限制└── /kubepods/pod08ae4074-5398-439b-93ae-a63035cbd3ae ├── tasks (空) ├── cpu.cfs_period_us (-&gt; 100000) ├── cpu.cfs_quota_us (-&gt; 100000) └── kata_dc5e4c1588ba3cdeb4fe1dffcb2420997408f42ad2545ddc792724b3bbfb7654 (infra 容器) ├── tasks (containerd-shim-kata-v2、virtiofsd、vhost 和 qemu-system 虚拟化进程) ├── cpu.cfs_period_us (-&gt; 100000) └── cpu.cfs_quota_us (-&gt; -1)# Kata VM 中的 cgroup 层级与限制└── /kubepods/pod08ae4074-5398-439b-93ae-a63035cbd3ae ├── tasks (空) ├── cpu.cfs_period_us (-&gt; 100000) ├── cpu.cfs_quota_us (-&gt; -1) ├── 8d0a3396afc32d47276b4b25e76e23cdf80dfc51ca980846fb3c847effbe84f9 (业务容器) | ├── tasks (业务进程) | ├── cpu.cfs_period_us (-&gt; 100000) | └── cpu.cfs_quota_us (-&gt; 100000) └── dc5e4c1588ba3cdeb4fe1dffcb2420997408f42ad2545ddc792724b3bbfb7654 (infra 容器) ├── tasks (pause 进程) ├── cpu.cfs_period_us (-&gt; 100000) └── cpu.cfs_quota_us (-&gt; -1) 创建 sandbox cgroup 之后，Kata shim 会在 VM 启动之前将其自身加入到该 cgroup 中。因此，随后由 Kata shim 创建的所有进程（VMM 本身，以及所有 vCPU 和 I/O 相关线程）都将受 sandbox cgroup 约束。 sandbox cgroup 的价值为什么不直接将 sandbox、shim 等 Kata 相关进程添加到 Pod cgroup？ Kata shim 实现了 per-sandbox cgroup （即每一个 sandbox 都有一个对应的 sandbox cgroup）来支持 Docker 场景。尽管 Docker 没有 Pod 的概念，但 Kata Containers 仍然创建了一个 sandbox 来支持 Docker 实现的无 Pod、单一容器用例（即 single_container）。为了简化使用，Kata Containers 选择一个独立的 sandbox cgroup，而不是构建容器和 sandbox 之间的 cgroup 映射关系。 优点将 Kata Containers 所有进程放置在适当大小的 Pod cgroup 中可以简化控制流程，有助于收集准确的指标统计数据并防止 Kata 工作负载产生近邻干扰（noisy neighbor），具体为： Pod 资源统计 如果想获取 Kata 容器在 host 上的资源使用情况，可以从 Pod cgroup 中获取指标统计信息。其中，cgroup 的统计数据包括 Kata 的额外开销，提供了在 Pod 级别和容器级别收集使用静态信息的能力。 更好的 host 资源隔离 Kata 运行时会将所有 Kata 进程放在 Pod cgroup 中，所以对 Pod cgroup 设置的资源限制将作用于 host 中属于 Kata sandbox 的所有进程（例如 qemu-system、virtiofsd 等），从而可以改善 host 中的隔离，防止 Kata 产生近邻干扰（noisy neighbor）。 sandbox_cgroup_only = false如果提供给 Kata 容器的 Pod cgroup 大小不合适，Kata 组件将消耗实际容器工作负载期望使用的资源，导致不稳定和性能下降。 为避免这种情况，Kata Containers 创建了一个名为 /kata_overhead 的 cgroup，即 overhead cgroup，并将所有与工作负载无关的进程（除 vCPU 线程外的任何进程）移至其中。 Kata Containers 不对 overhead cgroup 作任何约束或限制，因此可以 预先创建并规划 overhead cgroup 的限制条件，Kata Containers 不会再额外创建，而是将所有与工作负载无关的进程移动到其中 让 Kata Containers 创建 overhead cgroup，让其不受约束或事后调整大小 12345678910111213141516┌────────────────────────────────────────────────────────────────────┐│ ┌─────────────────────────────┐ ┌───────────────────────────┐ ││ │ ┌─────────────────────────┼────┼─────────────────────────┐ │ ││ │ │ ┌─────────────────────┐ │ │ ┌─────────────────────┐ │ │ ││ │ │ │ vCPU threads │ │ │ │ VMM │ │ │ ││ │ │ │ │ │ │ │ I/O threads │ │ │ ││ │ │ │ │ │ │ │ Kata Shim │ │ │ ││ │ │ │ │ │ │ │ │ │ │ ││ │ │ │ /kata_&lt;sandboxID&gt; │ │ │ │ /&lt;sandboxID&gt; │ │ │ ││ │ │ └─────────────────────┘ │ │ └─────────────────────┘ │ │ ││ │ │ Pod │ │ │ │ ││ │ └─────────────────────────┼────┼─────────────────────────┘ │ ││ │ /kubepods │ │ /kata_overhead │ ││ └─────────────────────────────┘ └───────────────────────────┘ ││ Node │└────────────────────────────────────────────────────────────────────┘ 实现细节当 sandbox_cgroup_only 被禁用时，Kata shim 将在 Pod cgroup 下创建 sandbox cgroup 子 cgroup，并在 overhead cgroup 下创建一个名为 /&lt;sandboxID&gt; 的子 cgroup。 1234567891011121314151617181920212223242526272829303132333435363738394041# runC 的 cgroup 层级与限制└── /kubepods/pod505eb17b-78d4-4dce-bfb2-60085f629344 ├── tasks (空) ├── cpu.cfs_period_us (-&gt; 100000) ├── cpu.cfs_quota_us (-&gt; 100000) ├── 499316b3661bc989f0999dd51901d2afaad0dda0aa614a2ebcd39f2517e7c56b (业务容器) | ├── tasks (业务进程) | ├── cpu.cfs_period_us (-&gt; 100000) | └── cpu.cfs_quota_us (-&gt; 100000) └── fa6545c433f02a1c712db11cb58bb100a013f9622d725c6a41c60500c20031c5 (infra 容器) ├── tasks (pause 进程) ├── cpu.cfs_period_us (-&gt; 100000) └── cpu.cfs_quota_us (-&gt; -1)# Kata 的 cgroup 层级与限制├── /kubepods/podf2f4d981-27ab-4deb-87c0-07764f72f63c| ├── tasks (空)| ├── cpu.cfs_period_us (-&gt; 100000)| ├── cpu.cfs_quota_us (-&gt; 100000)| └── kata_db541270577881d786b38b188d86959301c2e3e22bb7f08dcab009ed089d80d8 (infra 容器)| ├── tasks (有 PID，但是进程信息已销毁，应该就是社区说的 vCPU 线程)| ├── cpu.cfs_period_us (-&gt; 100000)| └── cpu.cfs_quota_us (-&gt; -1)└── /kata_overhead/db541270577881d786b38b188d86959301c2e3e22bb7f08dcab009ed089d80d8 ├── tasks (containerd-shim-kata-v2、virtiofsd、vhost 和 qemu-system 虚拟化进程) ├── cpu.cfs_period_us (-&gt; 100000) └── cpu.cfs_quota_us (-&gt; -1)# Kata VM 中的 cgroup 层级与限制└── /kubepods/podf2f4d981-27ab-4deb-87c0-07764f72f63c ├── tasks (空) ├── cpu.cfs_period_us (-&gt; 100000) ├── cpu.cfs_quota_us (-&gt; -1) ├── 87bd38d05b248b095e2feb4d3e1196a8ab604baf1ede6f81b55a3fca42545a83 (业务容器) | ├── tasks (业务进程) | ├── cpu.cfs_period_us (-&gt; 100000) | └── cpu.cfs_quota_us (-&gt; 100000) └── db541270577881d786b38b188d86959301c2e3e22bb7f08dcab009ed089d80d8 (infra 容器) ├── tasks (pause 进程) ├── cpu.cfs_period_us (-&gt; 100000) └── cpu.cfs_quota_us (-&gt; -1) 与启用 sandbox_cgroup_only 时不同，Kata shim 将其自身加入到 overhead cgroup 中，然后将 vCPU 线程移动到 sandbox cgroup 中。除 vCPU 线程外的其他 Kata 进程和线程都将在 overhead cgroup 下运行。 在禁用 sandbox_cgroup_only 的情况下，Kata Containers 假定 Pod cgroup 的大小仅能满足容器工作负载进程。VMM 创建的 vCPU 线程是唯一在 Pod cgroup 下运行的 Kata 相关线程，降低了 VMM、Kata shim 和 I/O 线程 OOM 的风险。 优缺点在不受约束的 overhead cgroup 下运行所有非 vCPU 线程可能会导致工作负载潜在地消耗大量 host 资源。 另一方面，由于 overhead cgroup 的专用性，在 overhead cgroup 下运行所有非 vCPU 线程可以获取 Kata Container Pod 额外开销的准确指标，以此更合理的调整 overhead cgroup 大小和约束。 总结 VM 自身的规格用于限制 VM 中所有系统服务（如 Kata agent）与用户服务（如容器工作负载）的资源开销 VM 中的 cgroup 用于限制 Kata 容器的工作负载的资源开销 host 的 cgroup 用于限制 Kata 容器在 host 侧虚拟化层面的资源开销（视不同的 cgroup 管理方式而定） runtimeClass.overhead123456789apiVersion: node.k8s.io/v1kind: RuntimeClassmetadata: name: katahandler: kataoverhead: podFixed: memory: &quot;1024Mi&quot; cpu: &quot;500m&quot; requests：resources.requests + runtimeClass.overhead 节点调度时，无论是否声明 resources.requests，runtimeClass.overhead 均会追加到 resources.requests 中，两者之和作为调度的资源请求量 limits：resources.limits + runtimeClass.overhead 资源限制时，如果声明了 resources.limit，则 runtimeClass.overhead 会追加到其中，两者之和作为资源的限制使用量 runtimeClass.overhead 部分会作用在 Pod cgroup 层面 123Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age--------- ---- ------------ ---------- --------------- ------------- ---default kata 1500m (4%) 1500m (4%) 2Gi (13%) 2Gi (13%) 7s 此外，overhead 的资源声明规范并不会影响到 Pod 的 QoS，也不会影响到 VM 最终的规格。 VM 规格VM 最终规格为 CPU：[hypervisor].default_vcpus + resources.limits，最大不超过 [hypervisor].default_maxvcpus MEM：[hypervisor].default_memory + resources.limits 123456root@localhost:/# nproc2root@localhost:/# free -m total used free shared buff/cache availableMem: 3017 46 2832 0 138 2929Swap: 0 0 0","link":"/2023/05/15/2023-05-15%20Kata%20Containers%20%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6/"},{"title":"「 OpenFaaS 」快速开始","text":"OpenFaaS 支持部署至以下环境中： Kubernetes、K3s、OpenShift 等容器编排环境 运行 faasd 服务的单点服务器环境 相比之下，与 Kubernetes 等容器编排环境集成能够提供更好的可扩展能力。 OpenFaaS CE based on 0.26.3 OpenFaaS Community Edition 版本面向内部使用、开发和概念验证。 OpenFaaS CE 安装方法： arkadearkade 本质上为一站式部署工具，支持许多服务部署，其中对于 OpenFaaS 服务支持完善。内部集成了二进制的下载与 Helm chart 的安装，arkade 封装了 helm 的参数赋值与部署，也就是在 helm 中通过 –set 设置的变量，arkade 通过 –flags 的方式处理了 Helm chart、Flux 或者 ArgoCD 静态 YAML 配置文件 安装faas-cli使用 arkade 安装 123456# 安装 arkade$ curl -sSL https://get.arkade.dev | sudo -E sh# 安装 faas-cli$ arkade get faas-cli$ sudo mv /root/.arkade/bin/faas-cli /usr/local/bin/ 使用 bash 安装 123$ curl -sSL https://cli.openfaas.com | shNew version of faas-cli installed to /usr/local/binCreating alias 'faas' for 'faas-cli'. OpenFaaS使用 arkade 安装 arkade 封装了 helm 的参数赋值与部署，也就是在 helm 中通过 –set 设置的变量，arkade 通过 –flags 的方式处理了 12345678# 安装 arkade$ curl -sSL https://get.arkade.dev | sudo -E sh# 默认安装$ arkade install openfaas# 安装可选参数$ arkade install openfaas --help 使用 helm 安装 123456789# 安装 helm$ curl -sSLf https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash# 推荐创建两个 namespace：openfaas 和 openfaas-fn，前者用于部署 OpenFaaS 服务组件，后者用于部署函数# arkade 部署中，默认创建了这两个 namespace$ kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml$ helm repo add openfaas https://openfaas.github.io/faas-netes/$ helm repo update &amp;&amp; helm upgrade openfaas --install openfaas/openfaas --namespace openfaas 安装结果 123456789101112131415$ kubectl get deploy -n openfaasNAME READY UP-TO-DATE AVAILABLE AGEalertmanager 1/1 1 1 71sgateway 1/1 1 1 71snats 1/1 1 1 71sprometheus 1/1 1 1 71squeue-worker 1/1 1 1 71s$ kubectl get svc -n openfaasNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEalertmanager ClusterIP 10.96.76.14 &lt;none&gt; 9093/TCP 74sgateway ClusterIP 10.96.4.162 &lt;none&gt; 8080/TCP 74sgateway-external NodePort 10.96.10.134 &lt;none&gt; 8080:31112/TCP 74snats ClusterIP 10.96.196.157 &lt;none&gt; 4222/TCP 74sprometheus ClusterIP 10.96.166.131 &lt;none&gt; 9090/TCP 74s 网关认证这里采用 NodePort 的形式部署 OpenFaaS CE 服务。其中，gateway-external 为对外暴露的网关服务，gateway 为对内暴露的网关服务，其后端均为 gateway Pod。无论哪种方式均采用 HTTP 认证登录方式，认证用户名和密码在 Secret 中保存： 1234$ USER=$(kubectl get secret -n openfaas basic-auth -o jsonpath=&quot;{.data.basic-auth-user}&quot; | base64 --decode; echo)$ PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath=&quot;{.data.basic-auth-password}&quot; | base64 --decode; echo)$ echo &quot;OpenFaaS user: $USER&quot;$ echo &quot;OpenFaaS password: $PASSWORD&quot; gateway 123456$ kubectl rollout status -n openfaas deploy/gateway$ kubectl port-forward -n openfaas svc/gateway 8080:8080 &amp;$ echo -n $PASSWORD | faas-cli login --username admin --password-stdinCalling the OpenFaaS server to validate the credentials...Handling connection for 8080credentials saved for admin http://127.0.0.1:8080 默认 faas-cli 操作的 OpenFaaS 实例为 http://127.0.0.1:8080，也可以通过 –gateway 进一步指定。 gateway-external HTTP 认证登录 gateway-external 暴露的服务，即 http://178.104.162.69:31112/ui/。认证后，即可进入 OpenFaaS UI： 模板商店社区提供的模板商店为 https://github.com/openfaas/store/blob/master/templates.json，其中来源自 OpenFaaS 官方社区与周边社区。 1234567891011121314151617181920{ { &quot;template&quot;: &quot;go&quot;, &quot;platform&quot;: &quot;x86_64&quot;, &quot;language&quot;: &quot;Go&quot;, &quot;source&quot;: &quot;openfaas&quot;, &quot;description&quot;: &quot;Legacy Golang template&quot;, &quot;repo&quot;: &quot;https://github.com/openfaas/templates&quot;, &quot;official&quot;: &quot;true&quot; }, { &quot;template&quot;: &quot;rust&quot;, &quot;platform&quot;: &quot;x86_64&quot;, &quot;language&quot;: &quot;Rust&quot;, &quot;source&quot;: &quot;openfaas-incubator&quot;, &quot;description&quot;: &quot;Community Rust template&quot;, &quot;repo&quot;: &quot;https://github.com/openfaas-incubator/openfaas-rust-template&quot;, &quot;official&quot;: &quot;false&quot; },} 查看默认支持的模板商店。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ faas-cli template store listNAME RECOMMENDED DESCRIPTION SOURCEbash-streaming [x] openfaas-incubator Bash Streaming templatedockerfile [x] openfaas Classic Dockerfile templategolang-middleware [x] openfaas HTTP middleware interface in Gojava11-vert-x [x] openfaas Java 11 Vert.x templatenode18 [x] openfaas HTTP-based Node 18 templatephp8 [x] openfaas Classic PHP 8 templatepython3-http [x] openfaas Python 3 with Flask and HTTPpython3-http-debian [x] openfaas Python 3 with Flask and HTTP based on Debianruby-http [x] openfaas Ruby 2.4 HTTP templatecobol [ ] devries COBOL Templatecrystal [ ] tpei Crystal templatecrystal-http [ ] koffeinfrei Crystal HTTP templatecsharp-httprequest [ ] distantcam C# HTTP templatecsharp-kestrel [ ] burtonr C# Kestrel HTTP templatelua53 [ ] affix Lua 5.3 Templateperl-alpine [ ] tmiklas Perl language template based on Alpine imagepython3-dlrs [ ] intel Deep Learning Reference Stack v0.4 for ML workloadsquarkus-native [ ] pmlopes Quarkus.io native image templaterust [ ] openfaas-incubator Community Rust templaterust-http [ ] openfaas-incubator Community Rust template with HTTP bindingsswift [ ] affix Swift 4.2 Templatevala [ ] affix Vala Templatevala-http [ ] affix Non-Forking Vala Templatevertx-native [ ] pmlopes Eclipse Vert.x native image templatecsharp [ ] openfaas Classic C# templatego [ ] openfaas Legacy Golang templategolang-http [ ] openfaas Request/response style HTTP templatejava11 [ ] openfaas Java 11 templatenode [ ] openfaas Legacy Node 12 templatenode12 [ ] openfaas HTTP-based Node 12 templatenode14 [ ] openfaas HTTP-based Node 14 templatenode16 [ ] openfaas HTTP-based Node 16 templatenode17 [ ] openfaas HTTP-based Node 17 templatephp7 [ ] openfaas Classic PHP 7 templatepowershell-http-template [ ] openfaas-incubator Powershell Core HTTP Ubuntu:16.04 templatepowershell-template [ ] openfaas-incubator Powershell Core Ubuntu:16.04 templatepuppeteer-nodelts [ ] alexellis A puppeteer template for headless Chromepython [ ] openfaas Classic Python 2.7 templatepython27-flask [ ] openfaas Python 2.7 Flask templatepython3 [ ] openfaas Classic Python 3 templatepython3-debian [ ] openfaas Python 3 Debian templatepython3-flask [ ] openfaas Python 3 Flask templatepython3-flask-debian [ ] openfaas Python 3 Flask template based on Debianruby [ ] openfaas Classic Ruby 2.5 template 获取托管在模板商店中的 OpenFaaS 官方经典模板。 1234$ faas-cli template pullFetch templates from repository: https://github.com/openfaas/templates.git at 2023/06/05 17:15:07 Attempting to expand templates from https://github.com/openfaas/templates.git2023/06/05 17:15:09 Fetched 18 template(s) : [csharp dockerfile go java11 java11-vert-x node node12 node12-debian node14 node16 node17 node18 php7 php8 python python3 python3-debian ruby] from https://github.com/openfaas/templates.git 获取托管在模板商店中的指定模板。 1234$ faas-cli template store pull rustFetch templates from repository: https://github.com/openfaas-incubator/openfaas-rust-template at 2023/06/05 17:41:58 Attempting to expand templates from https://github.com/openfaas-incubator/openfaas-rust-template2023/06/05 17:42:02 Fetched 1 template(s) : [rust] from https://github.com/openfaas-incubator/openfaas-rust-template 也可以通过 –url 参数，获取指定来源的模板。 1$ faas-cli template store pull --url=https://raw.githubusercontent.com/openfaas/store/master/templates.json 获取到的模板文件保存在当前的 template 目录中。 123456789101112131415161718192021222324252627282930313233343536$ ls template/csharp dockerfile go java11 java11-vert-x node node12 node12-debian node14 node16 node17 node18 php7 php8 python python3 python3-debian ruby$ tree template/golang-middleware/template/golang-middleware/├── Dockerfile # 函数最终会构建成镜像├── function # 业务代码，比如实现 HTTP endpoint 处理请求│ ├── go.mod│ └── handler.go├── go.mod├── go.work├── main.go # 函数入口，用于启动 HTTP 服务器，注册 endpoint└── template.yml # 模板说明1 directory, 7 files$ faas-cli new --listLanguages available as templates:- csharp- dockerfile- go- java11- java11-vert-x- node- node12- node12-debian- node14- node16- node17- node18- php7- php8- python- python3- python3-debian- ruby Hello World based on Kubernetes 1.24.10 创建函数这里以 Golang 的 golang-middleware 函数模板为例，该函数为简单的 HTTP 请求响应。 可选模板 托管商店 watchdog Go 版本 基础 OS 说明 go https://github.com/openfaas/templates classic 1.18 Alpine Linux Legacy Golang template golang-middleware https://github.com/openfaas/golang-http-template of-watchdog 1.19 Alpine Linux HTTP middleware interface in Go golang-http https://github.com/openfaas/golang-http-template of-watchdog 1.19 Alpine Linux Request/response style HTTP template 使用 golang-middleware 模板创建名为 go-fn 的函数。 12345678910111213141516171819202122232425$ faas-cli new go-fn --lang golang-middlewareFolder: go-fn created. ___ _____ ____ / _ \\ _ __ ___ _ __ | ___|_ _ __ _/ ___|| | | | '_ \\ / _ \\ '_ \\| |_ / _` |/ _` \\___ \\| |_| | |_) | __/ | | | _| (_| | (_| |___) | \\___/| .__/ \\___|_| |_|_| \\__,_|\\__,_|____/ |_|Function created in folder: go-fnStack file written: go-fn.ymlNotes:You have created a new function which uses Go 1.19 and AlpineLinux as its base image.To disable the go module, for private vendor code, please use&quot;--build-arg GO111MODULE=off&quot; with faas-cli build or configure thisvia your stack.yml file.See more: https://docs.openfaas.com/cli/templates/For the template's repo and more examples:https://github.com/openfaas/golang-http-template 函数模板创建后，会在当前目录生成 go-fn 目录，其内容源自于 template/golang-middleware/function 以及 go-fn.yml 文件，用于描述函数构建的具体规格，例如： 123456789version: 1.0provider: name: openfaas gateway: http://127.0.0.1:8080functions: go-fn: lang: golang-middleware handler: ./go-fn image: harbor.archeros.cn/dev/ake/openfaas-fn:dev 构建函数镜像faas-cli build 构建函数时，默认读取当前目录下的 stack.yml 文件，也可以通过 -f 指定。 1234$ faas-cli build -f go-fn.yml$ docker imagesharbor.archeros.cn/dev/ake/openfaas-fn dev 891e42d0a44c 23 minutes ago 21MB 构建时使用的 Dockerfile 位于 build/go-fn/Dockerfile（源自 template/golang-middleware/Dockerfile）。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061FROM --platform=${TARGETPLATFORM:-linux/amd64} ghcr.io/openfaas/of-watchdog:0.9.11 as watchdogFROM --platform=${BUILDPLATFORM:-linux/amd64} golang:1.19-alpine as buildARG TARGETPLATFORMARG BUILDPLATFORMARG TARGETOSARG TARGETARCHRUN apk --no-cache add gitCOPY --from=watchdog /fwatchdog /usr/bin/fwatchdogRUN chmod +x /usr/bin/fwatchdogRUN mkdir -p /go/src/handlerWORKDIR /go/src/handlerCOPY . .ARG GO111MODULE=&quot;on&quot;ARG GOPROXY=&quot;&quot;ARG GOFLAGS=&quot;&quot;ARG CGO_ENABLED=0ENV CGO_ENABLED=${CGO_ENABLED}# Run a gofmt and exclude all vendored code.RUN test -z &quot;$(gofmt -l $(find . -type f -name '*.go' -not -path &quot;./vendor/*&quot; -not -path &quot;./function/vendor/*&quot;))&quot; || { echo &quot;Run \\&quot;gofmt -s -w\\&quot; on your Golang code&quot;; exit 1; }WORKDIR /go/src/handler/functionRUN mkdir -p /go/src/handler/function/staticRUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} go test ./... -coverWORKDIR /go/src/handlerRUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} \\ go build --ldflags &quot;-s -w&quot; -o handler .FROM --platform=${TARGETPLATFORM:-linux/amd64} alpine:3.17.2 as ship# Add non root user and certsRUN apk --no-cache add ca-certificates \\ &amp;&amp; addgroup -S app &amp;&amp; adduser -S -g app app# Split instructions so that buildkit can run &amp; cache# the previous command ahead of time.RUN mkdir -p /home/app \\ &amp;&amp; chown app /home/appWORKDIR /home/appCOPY --from=build --chown=app /go/src/handler/handler .COPY --from=build --chown=app /usr/bin/fwatchdog .COPY --from=build --chown=app /go/src/handler/function/static staticUSER appENV fprocess=&quot;./handler&quot;ENV mode=&quot;http&quot;ENV upstream_url=&quot;http://127.0.0.1:8082&quot;ENV prefix_logs=&quot;false&quot;CMD [&quot;./fwatchdog&quot;] 此外，构建参数可以通过 stack.yml 文件中的 build_args 选项指定，效果等价于 faas-cli build --build-arg key1=value1,key2=value2。最终，build_args 指定的参数会通过 docker build –build-arg 透传给 Dockerfile 中的 ARG。 例如，指定使用本地 vendor 构建 Golang 应用 1234567functions: with_go_modules: handler: ./with_go_modules lang: go build_args: GO111MODULE: off GOFLAGS: &quot;-mod=vendor&quot; 默认函数应用镜像拉取策略为 Always，需要将镜像推送至远程仓库。 1$ faas-cli push -f go-fn.yml 发布函数123456$ faas-cli deploy -f go-fn.yml Deploying: go-fn.Handling connection for 8080Deployed. 202 Accepted.URL: http://127.0.0.1:8080/function/go-fn 函数发布后，对应着一个 Deploy 创建。OpenFaaS CE 版本中，副本数最小为 1。 123$ kubectl get pod -n openfaas-fnNAME READY STATUS RESTARTS AGEgo-fn-757f844cc5-v5tvn 1/1 Running 0 8s Pod 的关键参数为： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980spec: containers: - env: - name: fprocess value: ./handler image: harbor.archeros.cn/dev/ake/openfaas-fn:dev imagePullPolicy: Always livenessProbe: failureThreshold: 3 httpGet: path: /_/health port: 8080 scheme: HTTP initialDelaySeconds: 2 periodSeconds: 2 successThreshold: 1 timeoutSeconds: 1 name: go-fn ports: - containerPort: 8080 name: http protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /_/health port: 8080 scheme: HTTP initialDelaySeconds: 2 periodSeconds: 2 successThreshold: 1 timeoutSeconds: 1 resources: {} securityContext: readOnlyRootFilesystem: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-b7kx5 readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: wnx preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: kube-api-access-b7kx5 projected: defaultMode: 420 sources: - serviceAccountToken: expirationSeconds: 3607 path: token - configMap: items: - key: ca.crt path: ca.crt name: kube-root-ca.crt - downwardAPI: items: - fieldRef: apiVersion: v1 fieldPath: metadata.namespace path: namespace 已发布的函数中，Invocations 为调用次数，Replicas 为当前函数应用的副本。 123456789101112131415161718$ faas-cli listFunction Invocations Replicasgo-fn 0 1 $ faas describe go-fnName: go-fnStatus: Not ReadyReplicas: 1Available Replicas: 0Invocations: 17542Image: harbor.archeros.cn/dev/ake/openfaas-fn:devFunction Process: ./handlerURL: http://127.0.0.1:8080/function/go-fnAsync URL: http://127.0.0.1:8080/async-function/go-fnLabels: faas_function: go-fnAnnotations: prometheus.io.scrape: false 也可以通过部署时指定 –label 限制扩容规格等信息。 123456789101112131415161718$ faas-cli deploy -f go-fn.yml --label com.openfaas.scale.max=2# 此时无论多大的请求量，最大扩容规格为 2 副本$ faas-cli describe go-fnName: go-fnStatus: ReadyReplicas: 2Available Replicas: 2Invocations: 21517Image: harbor.archeros.cn/dev/ake/openfaas-fn:devFunction Process: ./handlerURL: http://127.0.0.1:8080/function/go-fnAsync URL: http://127.0.0.1:8080/async-function/go-fnLabels: com.openfaas.scale.max: 2 faas_function: go-fnAnnotations: prometheus.io.scrape: false 除了 faas-cli，函数的创建、构建与发布也可以通过 OpenFaaS UI 操作： 调用函数123456789$ faas-cli invoke go-fnReading from STDIN - hit (Control + D) to stop.Hello WorldBody: Hello World# 等价于$ echo Hello World | faas-cli invoke go-fn$ faas-cli invoke go-fn --from-literal=&quot;Hello World&quot;$ faas-cli invoke go-fn --from-file=~/Downloads/derek.pem UI 调用方式为： 此外，根据函数调用的路由，也分为同步调用与异步调用： 123456789101112131415161718192021# 同步调用 function 路由$ curl -i -d &quot;Hello World&quot; http://127.0.0.1:8080/function/go-fnHandling connection for 8080HTTP/1.1 200 OKContent-Length: 17Content-Type: text/plain; charset=utf-8Date: Fri, 09 Jun 2023 03:43:13 GMTX-Call-Id: 98630f05-1b39-4fc5-bb89-7c2c66c1cf7fX-Duration-Seconds: 0.002147X-Start-Time: 1686282193791968394Body: Hello World# 异步调用 async-function 路由$ curl -i -d &quot;Hello World&quot; http://127.0.0.1:8080/async-function/go-fnHandling connection for 8080HTTP/1.1 202 AcceptedX-Call-Id: fea5f39d-e7ad-4454-8283-1a213710f3a7X-Start-Time: 1686282228379248896Date: Fri, 09 Jun 2023 03:43:48 GMTContent-Length: 0 自动扩缩容模拟海量请求，观察 OpenFaaS 函数应用的 自动扩容。 1$ while true ;do echo Hello World | faas-cli invoke go-fn; done 随着请求调用量的增加，副本数也逐渐增加。 1234567891011$ kubectl get deploy -n openfaas-fn go-fn -wNAME READY UP-TO-DATE AVAILABLE AGEgo-fn 1/1 1 1 7m57sgo-fn 1/2 2 1 8m35sgo-fn 2/2 2 2 8m38sgo-fn 2/3 3 2 9m15sgo-fn 3/3 3 3 9m18sgo-fn 3/4 3 3 9m55sgo-fn 4/4 4 4 9m59sgo-fn 4/5 4 4 10mgo-fn 5/5 5 5 10m 当停止模拟请求时，副本数也逐渐减少。 12345$ kubectl get deploy -n openfaas-fn go-fn -wNAME READY UP-TO-DATE AVAILABLE AGEgo-fn 5/5 5 5 11mgo-fn 5/1 5 5 11mgo-fn 1/1 1 1 11m 也可以通过部署时指定 –label 限制扩容规格等信息。 1234567891011121314151617$ faas-cli deploy -f go-fn.yml --label com.openfaas.scale.max=2$ faas-cli describe go-fnName: go-fnStatus: ReadyReplicas: 2Available Replicas: 2Invocations: 21517Image: harbor.archeros.cn/dev/ake/openfaas-fn:devFunction Process: ./handlerURL: http://127.0.0.1:8080/function/go-fnAsync URL: http://127.0.0.1:8080/async-function/go-fnLabels: com.openfaas.scale.max: 2 faas_function: go-fnAnnotations: prometheus.io.scrape: false 函数日志针对 Kubernetes 的日志 provider 为 faas-netes 组件，其获取日志的方式等价于 kubectl logs -n openfaas-fn deploy/function。 123456$ faas-cli logs go-fn2023-06-07T16:15:23+08:00 2023/06/07 08:15:23 POST / - 200 OK - ContentLength: 18B (0.0008s)2023-06-07T16:15:23+08:00 2023/06/07 08:15:23 POST / - 200 OK - ContentLength: 18B (0.0005s)2023-06-07T16:15:24+08:00 2023/06/07 08:15:24 POST / - 200 OK - ContentLength: 18B (0.0015s)2023-06-07T16:15:24+08:00 2023/06/07 08:15:24 POST / - 200 OK - ContentLength: 18B (0.0007s)2023-06-07T16:15:24+08:00 2023/06/07 08:15:24 POST / - 200 OK - ContentLength: 18B (0.0017s) 移除函数1234$ faas-cli remove go-fnDeleting: go-fn.Handling connection for 8080Removing old function. Secret 管理之所以二次封装 API，是为了便于管理函数所用到的 Secret。 默认操作的 Secret 位于 openfaas-fn 命名空间下，可以通过 –namespace 指定；默认操作的 OpenFaaS 实例为 create 1234567891011$ faas-cli secret create my-secretReading from STDIN - hit (Control + D) to stop.my-passwordCreating secret: my-secret.Handling connection for 8080Created: 202 Accepted# 等价于$ echo my-password | faas-cli secret create my-secret$ faas-cli secret create my-secret --from-literal=&quot;my-password&quot;$ faas-cli secret create my-secret --from-file=~/Downloads/derek.pem 12345678910111213apiVersion: v1data: my-secret: bXktcGFzc3dvcmQ=kind: Secretmetadata: creationTimestamp: &quot;2023-06-07T06:19:14Z&quot; labels: app.kubernetes.io/managed-by: openfaas name: my-secret namespace: openfaas-fn resourceVersion: &quot;11584461&quot; uid: dd0803f2-76d8-453a-a770-ec2633cd6b22type: Opaque update 1234567891011$ faas-cli secret update my-secretReading from STDIN - hit (Control + D) to stop.my-new-paswordUpdating secret: my-secretHandling connection for 8080Updated: 202 Accepted# 等价于$ echo my-new-secret | faas-cli secret update my-secret$ faas-cli secret update my-secret --from-literal=&quot;my-password&quot;$ faas-cli secret update my-secret --from-file=~/Downloads/derek.pem list 12345$ faas-cli secret listHandling connection for 8080NAMEmy-secret delete 1234$ faas-cli secret remove my-secretfaas-cli secret remove my-secretHandling connection for 8080Removed.. OK. OpenFaaS Pro based on 0.26.3 OpenFaaS Pro 是 OpenFaaS 的商业许可发行版，具有附加功能、配置和商业支持。 安装faas-cli profaas-cli 对于 OpenFaaS Pro 的支持是通过插件的方式： 123456$ faas-cli plugin get proFetching plugin: proDownloaded in (4s)Usage: faas-cli pro 根据 github 账号校验 OpenFaaS Pro 购买认证： 12345678$ faas-cli pro enablePlease visit: https://github.com/login/deviceand enter the code: 168C-29B2Waiting for authorization...Waiting for authorization...Waiting for authorization...Waiting for authorization...GET https://api.github.com/user/memberships/orgs: 401 Bad credentials [] 对比Support Description OpenFaaS CE OpenFaaS Pro OpenFaaS for Enterprise Suitability Open Source developers and initial exploration Production, business critical, or PoC Regulated companies which may have additional legal and compliance requirements SLA N/a N/a Response within 1 business day for P1 Buying process N/a Invoice paid by bank transfer Supplier portals, custom paperwork, negotiation with procurement. Legal review of contract N/a N/a Yes Signing of Mutual NDA N/a N/a Subject to agreement Additional compliance needs N/a N/a Subject to agreement Support via email N/a Pro features only All certified Open Source and commercial components Support via GitHub N/a Pro features only using the Customer Community N/a Support via Slack N/a N/a Up to 5 developers License MIT Commercial license EULA As per Pro Architecture review N/a N/a With our team via Zoom Onboarding call N/a N/a With our team via Zoom Customer Community N/a Private access to Customer Community - one user per licensed cluster Custom amount of users Autoscaling Description OpenFaaS CE OpenFaaS Pro OpenFaaS for Enterprise Scale to Zero Not supported Global default, or custom time per function As per Pro Maximum replicas per function 5 Pods No limit applied As per Pro Scale to From Not supported Supported, with additional checks for Istio As per Pro Autoscaling strategy RPS-only CPU utilization, Capacity (inflight requests) or RPS As per Pro Autoscaling granularity Single rule for all functions Configurable per function As per Pro Core Features Description OpenFaaS CE OpenFaaS Pro OpenFaaS for Enterprise UI Dashboard Legacy UI (in code-freeze) New UI dashboard with metrics, logs &amp; CI integration As per Pro, but with support for multiple namespaces Consume secrets in faas-cli build for npm, Go and Pypy Not available Via build-time secrets As Per Pro Kubernetes service accounts for functions N/a Supported per function As per Pro Async / queueing In-memory only, max 10 items in queue, 256KB message size JetStream with shared queue JetStream with dedicated queues Metrics Basic function metrics Function, HTTP, CPU/RAM usage, and async/queue metrics As per Pro CPU &amp; RAM utilization Not available Integrated with Prometheus metrics, OpenFaaS REST API &amp; CLI As per Pro Grafana Dashboards N/a 4x dashboards supplied in Customer Community - overview, spotlight for debugging a function, queue-worker and Function Builder API As per Pro GitOps &amp; CRD support Not available ArgoCD, FluxCD and Helm compatibility using the Function CRD As per Pro Deployment options faas-cli or REST API As per CE, plus: Function CRD with kubectl, Helm or GitOps As per Pro Custom Resource Definition Not available Function and Profile Event Connectors Description OpenFaaS CE OpenFaaS Pro OpenFaaS for Enterprise Number of topics per function One topic per function Multiple topics per function As per Pro Kafka event trigger Not supported Supports SASL or TLS auth, Aiven, Confluent and self-hosted Support with SLA Postgres trigger Not supported Supports insert, update and delete, with table-level filters using WAL or LISTEN/NOTIFY. Support with SLA AWS SQS trigger Not supported Standard support Support with SLA Cron and scheduled invocations Community support Standard support Support with SLA Durability and Reliability Description OpenFaaS CE OpenFaaS Pro OpenFaaS for Enterprise Readiness probes Not supported Readiness probes supported with custom HTTP path and intervals per function As per Pro Retries for failed function invocations Not supported Retry invocations for configured HTTP codes with an exponential back-off As per Pro Highly Available messaging Not available, in-memory only Available for NATS JetStream, with 3x servers. As per Pro Long executions of async functions Limited to 5 minutes Configurable duration As per Pro Callback to custom URL for async functions Supported As per CE As per CE Security Description OpenFaaS CE OpenFaaS Pro OpenFaaS for Enterprise Authentication for OpenFaaS API, CLI and UI Shared admin password between everyone who uses OpenFaaS N/a Single Sign-On with OIDC Compatibility with Istio for mTLS N/a Supported As per Pro PCI/GDPR compliance Sensitive information such as the request body/response body, headers may be printed into the logs for each asynchronous invocation Sensitive information is not printed to the logs for asynchronous requests As per Pro Secure isolation with Kata containers or gVisor N/a N/a Supported using an OpenFaaS Pro Profile and runtimeClass Service links injected as environment variables Yes, cannot be disabled Disabled as a default As per Pro Pod privilege escalation Default for Kubernetes Explicitly disabled As per Pro Split installation without ClusterAdmin role N/a Provided in Customer Community As per Pro Platform Features Description OpenFaaS CE OpenFaaS Pro OpenFaaS for Enterprise Deploy functions via REST API Yes As per CE As per CE Build containers and functions via REST API N/a N/a Yes via Function Builder API Multiple namespace support No support N/a Supported with Kubernetes namespaces faasd based on 0.16.9 faasd 是 OpenFaaS 的重新构想，但没有 Kubernetes 的成本和复杂性。其本质就是一个 Golang 二进制文件，它可以在要求非常低的单个主机上运行，使其快速且易于管理。在底层，它使用 Containerd 和 CNI 以及来自主项目的相同核心 OpenFaaS 组件，因此在使用层面可以完全参考 OpenFaaS CE 的操作。 安装12$ wget https://github.com/openfaas/faasd/releases/download/0.16.9/faasd$ chmod +x faasd &amp;&amp; mv faasd /usr/local/bin 123456789# basic-auth-user 和 secrets/basic-auth-password 为网关认证的用户名和密码$ faasd install2023/06/08 17:53:15 Writing to: &quot;/var/lib/faasd/secrets/basic-auth-password&quot;2023/06/08 17:53:15 Writing to: &quot;/var/lib/faasd/secrets/basic-auth-user&quot;Check status with: sudo journalctl -u faasd --lines 100 -fLogin with: sudo cat /var/lib/faasd/secrets/basic-auth-password | faas-cli login -s 1234567891011121314151617181920$ systemctl status faasd● faasd.service - faasd Loaded: loaded (/usr/lib/systemd/system/faasd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2023-06-08 17:53:17 CST; 4min 26s ago Main PID: 43031 (faasd) Tasks: 11 Memory: 30.9M (limit: 500.0M) CGroup: /system.slice/faasd.service └─43031 /usr/local/bin/faasd upJun 08 17:54:34 wnx faasd[43031]: 2023/06/08 17:54:34 Looking up IP for: &quot;prometheus&quot;Jun 08 17:54:34 wnx faasd[43031]: 2023/06/08 17:54:34 Resolver: &quot;localhost&quot;=&quot;127.0.0.1&quot;Jun 08 17:54:34 wnx faasd[43031]: 2023/06/08 17:54:34 Resolver: &quot;faasd-provider&quot;=&quot;10.62.0.1&quot;Jun 08 17:54:34 wnx faasd[43031]: 2023/06/08 17:54:34 Resolver: &quot;nats&quot;=&quot;10.62.0.2&quot;Jun 08 17:54:34 wnx faasd[43031]: 2023/06/08 17:54:34 Resolver: &quot;prometheus&quot;=&quot;10.62.0.3&quot;Jun 08 17:54:34 wnx faasd[43031]: 2023/06/08 17:54:34 Resolver: &quot;gateway&quot;=&quot;10.62.0.4&quot;Jun 08 17:54:34 wnx faasd[43031]: 2023/06/08 17:54:34 Resolver: &quot;queue-worker&quot;=&quot;10.62.0.5&quot;Jun 08 17:54:34 wnx faasd[43031]: 2023/06/08 17:54:34 Proxy from: 127.0.0.1:9090, to: prometheus:9090 (10.62.0.3)Jun 08 17:54:34 wnx faasd[43031]: 2023/06/08 17:54:34 faasd: waiting for SIGTERM or SIGINTJun 08 17:54:34 wnx faasd[43031]: 2023/06/08 17:54:34 Proxy from: 0.0.0.0:8080, to: gateway:8080 (10.62.0.4) 通过服务状态可以看到，10.62.0.1 ~ 10.62.0.5 用于监听 OpenFaaS 核心服务，位于 Containerd 的 openfaas 命名空间中： 123456$ ctr -n openfaas c lsCONTAINER IMAGE RUNTIME gateway ghcr.io/openfaas/gateway:0.26.3 io.containerd.runc.v2 nats docker.io/library/nats-streaming:0.25.3 io.containerd.runc.v2 prometheus docker.io/prom/prometheus:v2.42.0 io.containerd.runc.v2 queue-worker ghcr.io/openfaas/queue-worker:0.13.3 io.containerd.runc.v2 网关认证外部网关为 http://178.104.162.69:8080/ui，认证信息位于 /var/lib/faasd/secrets/basic-auth-user 和 /var/lib/faasd/secrets/basic-auth-password。 123$ cat /var/lib/faasd/secrets/basic-auth-password | faas-cli login -sCalling the OpenFaaS server to validate the credentials...credentials saved for admin http://127.0.0.1:8080 其余模板商店和使用方式等操作和 OpenFaaS CE 完全一致。其中，OpenFaaS 函数容器托管在 Containerd 的 openfaas-fn 命名空间中： 123$ ctr -n openfaas-fn c lsCONTAINER IMAGE RUNTIME go-fn harbor.archeros.cn/dev/ake/openfaas-fn:dev io.containerd.runc.v2","link":"/2023/06/05/2023-06-05%20OpenFaaS%20%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/"},{"title":"「 Kubernetes 」节点资源超卖","text":"based on v1.24.10 背景Kubernetes 设计原语中，Pod 声明的 spec.resources.requests 用于描述容器所需资源的最小规格，Kube-scheduler 会根据资源请求量执行调度流程，并在节点资源视图中扣除；spec.resources.limits 用于限制容器资源最大使用量，避免容器服务使用过多的资源导致节点性能下降或崩溃。Kubelet 通过参考 Pod 的 QoS 等级来管理容器的资源质量，例如 OOM 优先级控制等。Pod 的 QoS 级别分为 Guaranteed、Burstable 和 BestEffort，QoS 级别并不是显式定义，而是取决于 Pod 声明的 spec.resources.requests 和 spec.resources.limits 中 CPU 与内存。 而在实际使用过程中，为了提高稳定性，应用管理员在提交 Guaranteed 和 Burstable 这两类 QoS Pod 时会预留相当数量的资源缓冲来应对上下游链路的负载波动，在大部分时间段，服务的资源请求量会远高于实际的资源使用率。 为了提升集群资源利用率，应用管理员会提交一些 BestEffort QoS 的低优任务，来充分使用那些已分配但未使用的资源。即基于 Pod QoS 的服务混部（co-location）以实现 Kubernetes 节点资源的超卖（overcommitted）。 这种策略常用于容器服务平台的在离线业务混部，但是这种基础的混部方案存在一些弊端： 混部会带来底层共享资源（CPU、内存、网络、磁盘等）的竞争，会导致在线业务性能下降，并且这种下降是不可预测的 节点可容纳低优任务的资源量没有任何参考，即使节点实际负载已经很高，由于 BestEffort 任务在资源规格上缺少容量约束，仍然会被调度到节点上运行 BestEffort 任务间缺乏公平性保证，任务资源规格存在区别，但无法在 Pod 描述上体现 设计思考在基于 Pod QoS 混部实现的 Kubernetes 节点资源超卖方案中，所要解决的核心问题是如何充分合理的利用缓冲资源，即 request buffer 与 limit buffer。 其中，limit buffer 在 Kubernetes 设计中天然支持超卖，Pod 在声明 spec.resources.limits 时，不受集群剩余资源的影响，集群中 Pod limits 之和也存在超出节点资源容量的情况，limit buffer 部分的资源是共享抢占的；而 request buffer 部分的资源是逻辑独占的，也就是说 spec.resources.requests 的大小会决定 Pod 能否调度，进而直接影响到节点资源的使用率。 因此，节点资源超卖理念更多的是对 request buffer 如何充分利用的思考。 资源回收与超卖资源回收是指回收业务应用已申请的，目前还处于空闲的资源，将其给低优业务使用。但是这部分资源是低质量的，不具备太高的可用性保证。 如图所示，reclaimed 资源代表可动态超卖的资源量，这部分需要根据节点真实负载情况动态更新，并以标准扩展资源的形式实时更新到 Kubernetes 的 Node 元信息中。低优任务可以通过在 spec.resources.requests 和 spec.resources.limits 中定义的 reclaimed 资源配置来使用这部分资源，这部分配置同时也会体现在节点侧的资源限制参数上，保证低优作业之间的公平性。 可回收资源的推导公式大致如下： reclaimed = nodeAllocatable * thresholdPercent - podUsage - systemUsage nodeAllocatable — 节点可分配资源总量 thresholdPercent — 预留水位比例 podUsage — 高优任务 Pod 的资源使用量 systemUsage — 系统资源真实使用量 弹性资源限制原生的 BestEffort 应用缺乏资源用量的公平保证，而使用动态资源的 BestEffort 应用需要保证其 CPU 使用量被限制在其允许使用的合理范围内，避免在不同 QoS 混部的场景下对高优 Pod 的干扰，确保整机的资源使用率控制在安全水位之下。 考虑到 Kubelet cgroup manager 不支持接口扩展，所以需要借助 agent 类型的组件维护容器的 cgroup，同时在 CPU 竞争时也能按照各自声明量公平竞争。 社区成果国内社区在节点资源超卖方面的落地思路整体相似，都是围绕弹性资源的回收、超卖与限制三个部分展开。无论是阿里 Koordinator、腾讯 Crane、华为 Volcano、字节 Katalyst 等开源项目，还是网易轻舟 NCS 和美团 LAR 等内部平台等都是类似的解决方案，它们的本质相同，只是在弹性资源结算方式等细节点上有所不同。 Koordinatorhttps://github.com/koordinator-sh/koordinator Koordinator 是一个基于 QoS 的 Kubernetes 混合工作负载调度系统，旨在提高对延迟敏感的工作负载和批处理作业的运行时效率和可靠性，简化与资源相关的配置调整的复杂性，并增加 Pod 部署密度以提高资源利用率。 SLO 在集群中运行的 Pod 资源 SLO（Service Level Objectives）由两个概念组成，即优先级和 QoS 优先级，即资源的优先级，代表了请求资源被调度的优先级。通常情况下，优先级会影响 Pod 在调度器待定队列中的相对位置 QoS，代表 Pod 运行时的服务质量。如 cgroups cpu share、cfs 配额、LLC、内存、OOM 优先级等等 Koordinator 定义了五种类型的 QoS，用于编排调度与资源隔离场景： QoS 特点 说明 SYSTEM 系统进程，资源受限 对于 DaemonSets 等系统服务，虽然需要保证系统服务的延迟，但也需要限制节点上这些系统服务容器的资源使用，以确保其不占用过多的资源 LSE(Latency Sensitive Exclusive) 保留资源并组织同 QoS 的 Pod 共享资源 很少使用，常见于中间件类应用，一般在独立的资源池中使用 LSR(Latency Sensitive Reserved) 预留资源以获得更好的确定性 类似于社区的 Guaranteed，CPU 核被绑定 LS(Latency Sensitive) 共享资源，对突发流量有更好的弹性 微服务工作负载的典型 QoS 级别，实现更好的资源弹性和更灵活的资源调整能力 BE(Best Effort) 共享不包括 LSE 的资源，资源运行质量有限，甚至在极端情况下被杀死 批量作业的典型 QoS 水平，在一定时期内稳定的计算吞吐量，低成本资源 此外，进一步定义了四类优先级，用于扩展优先级维度以对混部场景的细粒度支持： PriorityClass 优先级范围 描述 koord-prod [9000, 9999] 需要提前规划资源配额，并且保证在配额内成功 koord-mid [7000, 7999] 需要提前规划资源配额，并且保证在配额内成功 koord-batch [5000, 5999] 需要提前规划资源配额，一般允许借用配额 koord-free [3000, 3999] 不保证资源配额，可分配的资源总量取决于集群的总闲置资源 弹性资源回收与超卖 Koordinator 的混部资源模型，其基本思想是利用那些已分配但未使用的资源来运行低优先级的 Pod。如图所示，有四条线： limit：灰色，高优先级 Pod 所请求的资源量，对应于 Kubernetes 的 Pod 请求。 usage：红色，Pod 实际使用的资源量，横轴为时间线，红线为 Pod 负载随时间变化的波动曲线。 short-term reservation：深蓝色，这是基于过去（较短）时期内的资源使用量，对未来一段时间内其资源使用量的估计。预留和限制的区别在于，分配的未使用（未来不会使用的资源）可以用来运行短期执行的批处理 Pod。 long-term reservation：浅蓝色，与 short-term reservation 类似，但估计的历史使用期更长。从保留到限制的资源可以用于生命周期较长的 Pod，与短期的预测值相比，可用的资源较少，但更稳定。 Koordinator 的差异化 SLO 提供将这部分资源量化的能力。将上图中的红线定义为 usage，蓝线到红线预留部分资源定义为 buffered，绿色覆盖部分定义为 reclaimed。为体现与原生资源类型的差异性，Koordinator 使用 Batch 优先级的概念描述该部分超卖资源，也就是 batch-cpu 和 batch-memory。 节点中可超卖资源的计算公式为： nodeBatchAllocatable = nodeAllocatable * thresholdPercent - podRequest(non-BE) - systemUsage 公式中的 thresholdPercent 为可配置参数，通过修改 ConfigMap 中的配置项可以实现对资源的灵活管理。 Pod 通过声明标准扩展资源的方式使用超卖资源： 123456789101112131415metadata: labels: # 必填，标记为低优先级 Pod koordinator.sh/qosClass: &quot;BE&quot;spec: containers: - resources: requests: # 单位为千分之一核，如下表示 1 核 kubernetes.io/batch-cpu: &quot;1k&quot; # 单位为字节，如下表示 1 GB kubernetes.io/batch-memory: &quot;1Gi&quot; limits: kubernetes.io/batch-cpu: &quot;1k&quot; kubernetes.io/batch-memory: &quot;1Gi&quot; 此外，Koordinator 提供了一个 ClusterColocationProfile CRD 和对应的 webhook 修改和验证新创建的 Pod，主要为 Pod 注入 ClusterColocationProfile 中声明的 Koordinator QoSClass、Koordinator Priority 等，以及将 Pod 申请的标准资源变更至扩展资源。工作流程如下： 弹性资源限制 Koordinator 在宿主机节点提供了弹性资源限制能力，确保低优先级 BE（BestEffort）类型 Pod 的 CPU 资源使用在合理范围内，保障节点内容器稳定运行。 在 Koordinator 提供的动态资源超卖模型中，reclaimed 资源总量根据高优先级 LS（Latency Sensitive）类型 Pod 的实际资源用量而动态变化，这部分资源可以供低优先级 BE（BestEffort）类型 Pod 使用。通过动态资源超卖能力，可以将 LS 与 BE 类型容器混合部署，以此提升集群资源利用率。为了确保 BE 类型Pod 的 CPU 资源使用在合理范围内，避免 LS 类型应用的运行质量受到干扰，Koordinator 在节点侧提供了 CPU 资源弹性限制的能力。弹性资源限制功能可以在整机资源用量安全水位下，控制 BE 类型 Pod 可使用的 CPU 资源量，保障节点内容器稳定运行。 如下图所示，在整机安全水位下（CPU Threshold），随着 LS 类型 Pod 资源使用量的变化（Pod（LS）.Usage），BE 类型 Pod 可用的 CPU 资源被限制在合理的范围内（CPU Restriction for BE）。限制水位的配置与动态资源超卖模型中的预留水位基本一致，以此保证 CPU 资源使用的一致性。 Koordinator 支持通过 ConfigMap 配置弹性限制参数。 实践验证以 Koordinator 为例 based on v1.2.0 使用 helm 安装 12345678# Firstly add koordinator charts repository if you haven't do this.$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/# [Optional]$ helm repo update# Install the latest version.$ helm install koordinator koordinator-sh/koordinator --version 1.2.0 安装结果 12345678910111213141516171819202122232425$ kubectl get all -n koordinator-systemNAME READY STATUS RESTARTS AGEpod/koord-descheduler-68845fcc47-k72l5 1/1 Running 0 3d4hpod/koord-descheduler-68845fcc47-vk79v 1/1 Running 0 3d4hpod/koord-manager-7f68bbcf77-cbscj 1/1 Running 0 3d4hpod/koord-manager-7f68bbcf77-sjpw7 1/1 Running 0 3d4hpod/koord-scheduler-f4db87d4c-5p5j4 1/1 Running 0 3d4hpod/koord-scheduler-f4db87d4c-x242f 1/1 Running 0 3d4hpod/koordlet-nz58m 1/1 Running 0 3d4hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/koordinator-webhook-service ClusterIP 10.96.178.39 &lt;none&gt; 443/TCP 3d4hNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEdaemonset.apps/koordlet 1 1 1 1 1 &lt;none&gt; 3d4hNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/koord-descheduler 2/2 2 2 3d4hdeployment.apps/koord-manager 2/2 2 2 3d4hdeployment.apps/koord-scheduler 2/2 2 2 3d4hNAME DESIRED CURRENT READY AGEreplicaset.apps/koord-descheduler-68845fcc47 2 2 2 3d4hreplicaset.apps/koord-manager-7f68bbcf77 2 2 2 3d4hreplicaset.apps/koord-scheduler-f4db87d4c 2 2 2 3d4h Koordinator 由两个控制面 Koordinator Scheduler、Koordinator Manager 和一个 DaemonSet 组件 Koordlet 组成。Koordinator 在 Kubernetes 原有的能力基础上增加了混部功能，并兼容了 Kubernetes 原有的工作负载。 Koordinator Scheduler Koordinator Scheduler 以 Deployment 的形式部署，用于增强 Kubernetes 在混部场景下的资源调度能力，包括: 更多的场景支持，包括弹性配额调度、资源超卖、资源预留、Gang 调度、异构资源调度 更好的性能，包括动态索引优化、等价 class 调度、随机算法优化 更安全的 descheduling，包括工作负载感知、确定性的 Pod 迁移、细粒度的流量控制和变更审计支持 Koordinator Manager Koordinator Manager 以 Deployment 的形式部署，通常由两个实例组成，一个 leader 实例和一个 backup 实例。Koordinator Manager 由几个控制器和 webhooks 组成，用于协调混部场景下的工作负载，资源超卖和 SLO 管理。 目前，提供了三个组件: Colocation Profile，用于支持混部而不需要修改工作负载。用户只需要在集群中做少量的配置，原来的工作负载就可以在混部模式下运行 SLO 控制器，用于资源超卖管理，根据节点混部时的运行状态，动态调整集群的超发配置比例。该控制器的核心职责是管理混部时的 SLO，如智能识别出集群中的异常节点并降低其权重，动态调整混部时的水位和压力策略，从而保证集群中 Pod 的稳定性和吞吐量 Recommender，它使用 histograms 来统计和预测工作负载的资源使用细节，用来预估工作负载的峰值资源需求，从而支持更好地分散热点，提高混部的效率。此外，提供资源画像功能，预估工作负载的峰值资源需求，资源 profiling 还将用于简化用户资源规范化配置的复杂性，如支持 VPA Koordlet Koordlet 以 DaemonSet 的形式部署在 Kubernetes 集群中，用于支持混部场景下的资源超卖、干扰检测、QoS 保证等。 在 Koordlet 内部，它主要包括以下模块: 资源 profiling，估算 Pod 资源的实际使用情况，回收已分配但未使用的资源，用于低优先级 Pod 的 overcommit 资源隔离，为不同类型的 Pod 设置资源隔离参数，避免低优先级的 Pod 影响高优先级 Pod 的稳定性和性能 干扰检测，对于运行中的 Pod，动态检测资源争夺，包括 CPU 调度、内存分配延迟、网络、磁盘 IO 延迟等 QoS 管理器，根据资源剖析、干扰检测结果和 SLO 配置，动态调整混部节点的水位，抑制影响服务质量的 Pod 资源调优，针对混部场景进行容器资源调优，优化容器的 CPU Throttle、OOM 等，提高服务运行质量 弹性资源配置 123456789101112131415161718192021222324252627282930apiVersion: v1kind: ConfigMapmetadata: name: ack-slo-config namespace: kube-systemdata: colocation-config: | { # 是否开启节点 Batch 资源的动态更新，关闭时 Batch 资源量会被重置为 0。默认值为 false &quot;enable&quot;: true, # Batch 资源最小更新频率，单位为秒。通常建议保持为 1 分钟 &quot;metricAggregateDurationSeconds&quot;: 60, # 计算节点 batch-cpu 资源容量时的预留系数。默认值为 65，单位为百分比 &quot;cpuReclaimThresholdPercent&quot;: 60, # 计算节点 batch-memory 资源容量时的预留系数。默认值为 65，单位为百分比 &quot;memoryReclaimThresholdPercent&quot;: 70, # 计算节点 batch-memory 资源容量时的策略 # &quot;usage&quot;：默认值，表示 batch-memory 内存资源按照高优先级 Pod 的内存真实用量计算，包括了节点未申请的资源，以及已申请但未使用的资源量。 # &quot;request&quot;：表示 batch-memory 内存资源按照高优先级 Pod 的内存请求量计算，仅包括节点未申请的资源 &quot;memoryCalculatePolicy&quot;: &quot;usage&quot; } resource-threshold-config: | { &quot;clusterStrategy&quot;: { # 集群是否开启弹性资源限制能力 &quot;enable&quot;: true, # 单位为百分比，表示弹性资源限制对应的节点安全水位阈值，默认为 65 &quot;cpuSuppressThresholdPercent&quot;: 65 } } 开启后动态资源后，可以看到节点已经识别到扩展资源 kubernetes.io/batch-cpu 与 kubernetes.io/batch-memory。 12345678910111213141516171819$ kubectl describe node wnxCapacity: cpu: 8 memory: 12057632Ki kubernetes.io/batch-cpu: 4034 kubernetes.io/batch-memory: 4455468942Allocatable: cpu: 8 memory: 11955232Ki kubernetes.io/batch-cpu: 4034 kubernetes.io/batch-memory: 4455468942Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 4100m (51%) 6500m (81%) memory 1776Mi (15%) 6740Mi (57%) kubernetes.io/batch-cpu 0 0 kubernetes.io/batch-memory 0 0 mutating webook 注入的信息是根据 ClusterColocationProfile 决定的： 12345678910111213141516171819202122apiVersion: config.koordinator.sh/v1alpha1kind: ClusterColocationProfilemetadata: name: colocation-profile-examplespec: namespaceSelector: matchLabels: koordinator.sh/enable-colocation: &quot;true&quot; selector: matchLabels: koordinator.sh/enable-colocation: &quot;true&quot; qosClass: BE priorityClassName: koord-batch koordinatorPriority: 1000 schedulerName: koord-scheduler labels: koordinator.sh/mutated: &quot;true&quot; annotations: koordinator.sh/intercepted: &quot;true&quot; patch: spec: terminationGracePeriodSeconds: 30 模拟在离线服务混部 1234567891011121314151617181920212223242526272829303132333435apiVersion: v1kind: Podmetadata: name: onlinespec: containers: - name: app image: ubuntu:18.04 command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;] resources: limits: cpu: &quot;3&quot; memory: &quot;3000Mi&quot; requests: cpu: &quot;3&quot; memory: &quot;3000Mi&quot;---apiVersion: v1kind: Podmetadata: name: offline labels: koordinator.sh/enable-colocation: &quot;true&quot;spec: containers: - name: app image: ubuntu:18.04 command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;] resources: limits: cpu: &quot;3&quot; memory: &quot;100Mi&quot; requests: cpu: &quot;3&quot; memory: &quot;100Mi&quot; 在节点剩余 4C 左右的资源时，通过离线服务使用超卖资源、在线服务使用标准资源的混部模式，可以让服务均成功部署在节点上。 12345678910111213141516171819$ kubectl get pod NAME READY STATUS RESTARTS AGEoffline 1/1 Running 0 2m25sonline 1/1 Running 0 2m23s$ kubectl describe node wnxNon-terminated Pods: (18 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- default offline 0 (0%) 0 (0%) 0 (0%) 0 (0%) 8s default online 3 (37%) 3 (37%) 3000Mi (25%) 3000Mi (25%) 6sAllocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 7100m (88%) 9500m (118%) memory 4776Mi (40%) 9740Mi (83%) kubernetes.io/batch-cpu 3k 3k kubernetes.io/batch-memory 100Mi 100Mi 弹性资源限制 虽然离线服务的 cgroup 还是位于 kubepods 的 besteffort 组中（由于原本声明的标准资源被 webhook 变更为扩展资源，也就变成了 BestEffort QoS 的 Pod），但是 Koordlet 会根据扩展资源的声明规格手动维护。 12345$ cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod4bb9f204_6690_43cf_a871_808874ad0ed4.slice/cpu.cfs_quota_us 300000$ cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod4bb9f204_6690_43cf_a871_808874ad0ed4.slice/cpu.cfs_period_us 100000 此外，在离线服务所使用的 CPU 是有区别的 12345678910111213# BE QoS Pod 使用的 CPU 为 0-4$ kubectl logs -n koordinator-system koordlet-bw5kjnodeSuppressBE[CPU(Core)]:5 = node.Total:8 * SLOPercent:65% - systemUsage:1 - podLSUsed:1calculated BE suppress policy: cpuset [0 1 2 3 4]suppressBECPU finished, suppress be cpu successfully: current cpuset [0 1 2 3 4]# 在线服务使用的 CPU 仍然为 0-7$ cat /sys/fs/cgroup/cpuset/kubepods/pod7090d2d1-48db-4fb2-8318-dea7084334e2/c108b3e5f44969b6f9a71fd4217b909f3639dafe4cc88665db93b986abe0a031/cpuset.cpus0-7# 离线服务使用的 CPU 为 0-4$ cat /sys/fs/cgroup/cpuset/kubepods/besteffort/podcee106b7-48ff-4441-9bba-b37c0e4620f9/ef02220e4d28ff6ebd8768ac55a4f73b22b80fac5321f1acbde35acebaa1a74f/cpuset.cpus0-4 并且，随着节点负载上升（通过在线服务容器 stress 进程模拟），节点中可用的弹性资源（capacity 与 allocatable）也会逐渐变少，离线服务使用的 CPU 也会相应的缩减，但是不会停止离线服务。 123456$ cat /sys/fs/cgroup/cpuset/kubepods/besteffort/podcee106b7-48ff-4441-9bba-b37c0e4620f9/ef02220e4d28ff6ebd8768ac55a4f73b22b80fac5321f1acbde35acebaa1a74f/cpuset.cpus0-4# 当在线服务资源用量提升时，离线服务使用的 CPU 被逐渐缩减$ cat /sys/fs/cgroup/cpuset/kubepods/besteffort/podcee106b7-48ff-4441-9bba-b37c0e4620f9/ef02220e4d28ff6ebd8768ac55a4f73b22b80fac5321f1acbde35acebaa1a74f/cpuset.cpus0-1","link":"/2023/06/13/2023-06-13%20Kubernetes%20%E8%8A%82%E7%82%B9%E8%B5%84%E6%BA%90%E8%B6%85%E5%8D%96/"},{"title":"「 Kata Containers 」源码走读 — virtcontainers&#x2F;hypervisor","text":"based on 3.0.0 Hypervisorsrc/runtime/virtcontainers/hypervisor.go Kata Containers 支持的 hypervisor 有 QEMU、Cloud Hypervisor、Firecracker、ACRN 以及 DragonBall，其中 DragonBall 是 Kata Containers 3.0 为新增的 runtime-rs 组件引入的内置 hypervisor，而 runtime-rs 的整体架构区别于当前的 runtime，不在此详读 DragonBall 实现。 目前，暂时走读 QEMU 实现，后续补充其他 hypervisor 实现。 12345678910111213141516171819202122232425262728293031323334353637// qemu is an Hypervisor interface implementation for the Linux qemu hypervisor.type qemu struct { // 针对不同 CPU 架构下的 QEMU 配置项，后续会进一步构建成 qemuConfig arch qemuArch virtiofsDaemon VirtiofsDaemon ctx context.Context id string mu sync.Mutex // fds is a list of file descriptors inherited by QEMU process // they'll be closed once QEMU process is running fds []*os.File // HotplugVFIOOnRootBus: [hypervisor].hotplug_vfio_on_root_bus // PCIeRootPort: [hypervisor].pcie_root_port state QemuState // path // &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/qmp.sock qmpMonitorCh qmpChannel // QEMU 进程的配置参数 qemuConfig govmmQemu.Config // QEMU 实现下的 hypervisor 配置 config HypervisorConfig // if in memory dump progress memoryDumpFlag sync.Mutex // NVDIMM 设备数量 nvdimmCount int stopped bool} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308/* 前置说明 protection - amd64：默认为 noneProtection 如果启用 [hypervisor].confidential_guest，则进一步判断 protection - 如果 host 上 /sys/firmware/tdx_seam/ 文件夹存在或者 CPU flags 中包含 tdx，则为 tdxProtection（Intel Trust Domain Extensions） - 如果 host 上 /sys/module/kvm_amd/parameters/sev 文件存在且内容为 1 或者 Y 则为 sevProtection（AMD Secure Encrypted Virtualization） - arm64：noneProtection */// Config is the qemu configuration structure.// It allows for passing custom settings and parameters to the qemu API.// nolint: govettype Config struct { // Path is the qemu binary path. // - amd64: /usr/bin/qemu-system-x86_64 // - arm64: /usr/bin/qemu-system-aarch64 Path string // Ctx is the context used when launching qemu. Ctx context.Context // User ID. Uid uint32 // Group ID. Gid uint32 // Supplementary group IDs. Groups []uint32 // Name is the qemu guest name // -name 参数 // sandbox-&lt;sandboxID&gt; Name string // UUID is the qemu process UUID. // -uuid 参数 // 随机生成 UUID string // CPUModel is the CPU model to be used by qemu. // -cpu 参数，例如 -cpu host,pmu=off // 默认为 host，如果指定 [hypervisor].cpu_features 则继续追加 CPUModel string // SeccompSandbox is the qemu function which enables the seccomp feature // [hypervisor].seccompsandbox SeccompSandbox string // Machine // -machine 参数，例如 -machine q35,accel=kvm,kernel_irqchip=on,nvdimm=on // Type: [hypervisor].machine_type // - amd64: 默认为 q35 // - arm64: virt // Options: // - amd64: 默认为 accel=kvm,kernel_irqchip=on // 如果启用 [hypervisor].confidential_guest 或者启用 hypervisor[enable_iommu]，则覆盖 Options 为 accel=kvm,kernel_irqchip=split // 如果 sgxEPCSize 不为 0，则追加 sgx-epc.0.memdev=epc0,sgx-epc.0.node=0 // 如果启用 [hypervisor].confidential_guest: // - 如果 protection 为 tdxProtection，则追加 kvm-type=tdx,confidential-guest-support=tdx // - 如果 protection 为 sevProtection，则追加 confidential-guest-support=sev // 如果镜像类型为 [hypervisor].image 且 disableNvdimm 为 false，则追加 nvdimm=on // - arm64: usb=off,accel=kvm,gic-version=host // 如果指定 [hypervisor].machine_accelerators，则继续追加 Machine Machine // QMPSockets is a slice of QMP socket description. // -qmp 参数，例如 -qmp unix:/run/vc/vm/&lt;sandboxID&gt;/qmp.sock,server=on,wait=off // Type: unix // Name: &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/qmp.sock // Server: true // NoWait: true QMPSockets []QMPSocket // Devices is a list of devices for qemu to create and drive. // -device 参数 // // =========== Bridge =========== // 例如 -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=off,addr=2,io-reserve=4k,mem-reserve=1m,pref64-reserve=1m // BridgeDevice（数量等于 [hypervisor].default_bridges） // Type: 默认为 0，即 PCI，如果 bridge 类型为 PCIe，则为 PCIe // Bus: 默认为 pci.0，如果 Machine.Type 为 q35 或者 virt，则为 pcie.0 // ID: &lt;bt&gt;-bridge-&lt;idx&gt;，其中 idx 为 0 ~ [hypervisor].default_bridges 的递增索引 // - 如果 Machine.Type 为 q35、virt 和 pseries，则 bt 为 pci，容量为 30 // - 如果 Machine.Type 为 s390-ccw-virtio，则 bt 为 ccw，容量为 65535 // Chassis: idx + 1，其中 idx 为 bridge 列表的索引 // SHPC: false // Addr: idx + 2，其中 idx 为 bridge 列表的索引 // IOReserve: 4k // MemReserve: 1m // Pref64Reserve: 1m // // =========== Console =========== // - 禁用 [hypervisor].use_legacy_serial // 例如 -device virtio-serial-pci,disable-modern=true,id=serial0 -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/&lt;sandboxID&gt;/console.sock,server=on,wait=off // CharDevice // Driver: virtconsole // Backend: socket // DeviceID: console0 // ID: charconsole0 // Path: &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/console.sock // SerialDevice // Driver: virtio-serial // ID: serial0 // DisableModern: // - amd64: 当未禁用 [hypervisor].disable_nesting_checks，且 CPU flags 中有 hypervisor，视为 true；否则，为 false // - arm64: false // MaxPorts: 2 // - 启用 [hypervisor].use_legacy_serial // 例如 -serial chardev:charconsole0 -chardev socket,id=charconsole0,path=/run/vc/vm/&lt;sandboxID&gt;/console.sock,server=on,wait=off // CharDevice // Driver: serial // Backend: socket // DeviceID: console0 // ID: charconsole0 // Path: &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/console.sock // LegacySerialDevice // Chardev: charconsole0 // // =========== Image（当镜像类型为 [hypervisor].image） =========== // - 禁用 [hypervisor].disable_image_nvdimm // 例如 -drive id=image-199896efe4d8ad3b,file=/opt/kata/share/kata-containers/kata-clearlinux-latest.image,aio=threads,format=raw,if=none,readonly=on // BlockDrive // File: [hypervisor].image // Format: raw // ID: image-&lt;随机字符串&gt; // ShareRW: true // ReadOnly: true // - 启用 [hypervisor].disable_image_nvdimm // 例如 -device nvdimm,id=nv0,memdev=mem0,unarmed=on -object memory-backend-file,id=mem0,mem-path=/opt/kata/share/kata-containers/kata-clearlinux-latest.image,size=134217728,readonly=on // Object // Driver: nvdimm // Type: memory-backend-file // DeviceID: nv0 // ID: mem0 // MemPath: [hypervisor].image // Size: [hypervisor].image 大小 // ReadOnly: true // // =========== IOMMU（当启用 [hypervisor].enable_iommu） =========== // IommuDev // Intremap: true // DeviceIotlb: true // CachingMode: true // // =========== PVPanic（当指定 [hypervisor].guest_memory_dump_path） =========== // PVPanicDevice // NoShutdown: true // // =========== BlockDeviceDriver（当 [hypervisor].block_device_driver 为 virtio-scsi） =========== // 例如 -device virtio-scsi-pci,id=scsi0,disable-modern=true // SCSIController // ID: scsi0 // DisableModern: // - amd64: 当未禁用 [hypervisor].disable_nesting_checks，且 CPU flags 中有 hypervisor，视为 true；否则，为 false // - arm64: false // IOThread:（当启用 [hypervisor].enable_iothreads） // ID: iothread-&lt;随机字符串&gt; // // =========== Protection =========== // Object（当 sgxEPCSize 不为 0 时） // Type: memory-backend-epc // ID: epc0 // Prealloc: true // Size: sgxEPCSize // Object（当 protection 为 tdxProtection 时） // Driver: loader // Type: tdx-guest // ID: tdx // DeviceID: fd&lt;idx&gt;，其中 idx 为 loader 类型 Driver 的统计数量 // Debug: false // File: [hypervisor].firmware // FirmwareVolume: [hypervisor].firmware_volume // Object（当 protection 为 sevProtection 时） // Type: sev-guest // ID: sev // Debug: false // File: [hypervisor].firmware // CBitPos: ebx &amp; 0x3F // ReducedPhysBits: (ebx &gt;&gt; 6) &amp; 0x3F // // =========== rngDev（当 Machine.Type 不为 s390-ccw-virtio）=========== // RNGDev // 例如 -object rng-random,id=rng0,filename=/dev/urandom // ID: rng0 // FileName: [hypervisor].entropy_source // // =========== PCIe（当 [hypervisor].pcie_root_port 大于 0 且 Machine.Type 为 q35 或 virt）=========== // PCIeRootPortDevice（数量等于 [hypervisor].pcie_root_port） // 例如 -device pcie-root-port,id=rp1,bus=pcie.0,chassis=0,slot=1,multifunction=off,pref64-reserve=2097152B,mem-reserve=4194304B // ID: rp&lt;idx&gt;，其中 idx 为 0 ~ [hypervisor].pcie_root_port 的递增索引 // Bus: pcie.0 // Chassis: 0 // Slot: idx // Multifunction: false // Addr: 0 // MemReserve: 默认 4MB，如果累加每个 BAR 的 32 位内存窗口值更大，则以此值为准，并乘以 2 // Pref64Reserve: 默认 2MB，如果累加每个 BAR 的 64 位内存窗口值更大，则以此值为准 Devices []Device // RTC is the qemu Real Time Clock configuration // -rtc 参数，例如 -rtc base=utc,driftfix=slew,clock=host // Base: utc // Clock: host // DriftFix: slew RTC RTC // VGA is the qemu VGA mode. // -vga 参数 // none VGA string // Kernel is the guest kernel configuration. // -kernel 参数，例如 -kernel /opt/kata/share/kata-containers/vmlinux-5.19.2-96 // -initrd 参数，例如 -initrd /opt/kata/share/kata-containers/kata-alpine-3.15.initrd // -append 参数，例如 -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k cryptomgr.notests net.ifnames=0 pci=lastbus=0 console=hvc0 console=hvc1 debug panic=1 nr_cpus=8 scsi_mod.scan=none agent.log=debug agent.debug_console agent.debug_console_vport=1026 // Path: [hypervisor].kernel // InitrdPath: [hypervisor].initrd，当镜像类型为 [hypervisor].image 时，没有 -initrd 参数 // Params: // - kernelParams: // - amd64: 默认为 tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k cryptomgr.notests net.ifnames=0 pci=lastbus=0 panic=1 nr_cpus=[hypervisor].default_maxvcpus // 如果启用 [hypervisor].enable_iommu，则追加 intel_iommu=on iommu=pt // 如果镜像类型为 [hypervisor].image: // - 如果 disableNvdimm 为 true，则追加 root=/dev/vda1 rootflags=data=ordered errors=remount-ro ro rootfstype=ext4 // - 如果 disableNvdimm 为 false: // - 如果 dax 为 false，则追加 root=/dev/pmem0p1 rootflags=data=ordered errors=remount-ro ro rootfstype=ext4 // - 如果 dax 为 true，则追加 root=/dev/pmem0p1 rootflags=dax data=ordered errors=remount-ro ro rootfstype=ext4 // 如果启用 [hypervisor].use_legacy_serial，则追加 console=ttyS0，否则，则追加 console=hvc0 console=hvc1 // - arm64: iommu.passthrough=0 panic=1 nr_cpus=[hypervisor].default_maxvcpus // - kernelParamsDebug: 默认为 debug，如果镜像类型为 [hypervisor].image，则追加 systemd.show_status=true systemd.log_level=debug // - kernelParamsNonDebug: 默认为 quiet，如果镜像类型为 [hypervisor].image，则追加 systemd.show_status=false // 由以上三个参数组成，具体为 kernelParams + kernelParamsDebug/kernelParamsNonDebug（取决于 [hypervisor].enable_debug），如果指定 [hypervisor].kernel_params，则继续追加 Kernel Kernel // Memory is the guest memory configuration. // -m 参数，例如 -m 2048M,slots=10,maxmem=12799M // Size: [hypervisor].default_memory // Slots: [hypervisor].memory_slots // MaxMem: // - amd64: [hypervisor].memory_offset + [hypervisor].default_maxmemory // - arm64: [hypervisor].default_maxmemory // Path: // - 如果为 VM factory 场景，则为 [factory].template_path/memory // - 如果 [hypervisor].shared_fs 为 virtio-fs 或者 virtio-fs-nydus, 再或者 annotations[&quot;io.katacontainers.config.hypervisor.file_mem_backend&quot;] 不为空，则为 /dev/shm（如果 annotations 传递，则以 annotations 为准） Memory Memory // SMP is the quest multi processors configuration. // -smp 参数，例如 -smp 1,cores=1,threads=1,sockets=8,maxcpus=8 // CPUs: [hypervisor].default_vcpus // Cores: 1 // Threads: 1 // Sockets: [hypervisor].default_maxvcpus // MaxCPUs: [hypervisor].default_maxvcpus SMP SMP // GlobalParam is the -global parameter. // -global 参数 // kvm-pit.lost_tick_policy=discard GlobalParam string // Knobs is a set of qemu boolean settings. // -no-user-config -nodefaults -nographic --no-reboot -daemonize 参数 // NoUserConfig、NoDefaults、NoGraphic、NoReboot、Daemonize: true // MemPrealloc: 默认为 [hypervisor].enable_mem_prealloc，如果 [hypervisor].shared_fs 为 virtio-fs 或者 virtio-fs-nydus, 再或者 annotations[&quot;io.katacontainers.config.hypervisor.file_mem_backend&quot;] 不为空，并且启用 [hypervisor].enable_hugepages，则为 true // HugePages: [hypervisor].enable_hugepages // IOMMUPlatform: [hypervisor].enable_iommu_platform // FileBackedMem: // - 如果为 VM factory 场景，则为 true // - 如果 [hypervisor].shared_fs 为 virtio-fs 或者 virtio-fs-nydus, 再或者 annotations[&quot;io.katacontainers.config.hypervisor.file_mem_backend&quot;] 不为空，则为 true // MemShared: // - 如果为 VM factory 中的启动为模板场景，则为 true // - 如果 [hypervisor].shared_fs 为 virtio-fs 或者 virtio-fs-nydus, 再或者 annotations[&quot;io.katacontainers.config.hypervisor.file_mem_backend&quot;] 不为空，则为 true // - 如果启用 [hypervisor].enable_vhost_user_store，则为 true Knobs Knobs // Bios is the -bios parameter // -bios 参数 // [hypervisor].firmware Bios string // PFlash specifies the parallel flash images (-pflash parameter) // -pflash 参数 // [hypervisor].pflashes PFlash []string // Incoming controls migration source preparation // MigrationType: 如果为 VM factory 中的从模板启动场景，则为 3 Incoming Incoming // fds is a list of open file descriptors to be passed to the spawned qemu process fds []*os.File // FwCfg is the -fw_cfg parameter FwCfg []FwCfg // Devices 中 SCSIController.IOThread IOThreads []IOThread // PidFile is the -pidfile parameter // -pidfile 参数 // &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/pid PidFile string // LogFile is the -D parameter // -D 参数 // &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/qemu.log LogFile string // 基于上述的 QEMU 配置项，构建 -name、-uuid、-machine、-cpu、-qmp、-m、-device、-rtc、-global、-pflash 等参数信息 qemuParams []string} Hypervisor 中声明的 HypervisorConfig、setConfig、GetVirtioFsPid，fromGrpc、toGrpc，Save 和 Load 均为参数获取与赋值，无复杂逻辑，不作详述。 qmpSetup初始化 QMP 服务 source code 如果当前 QMP 服务已经就绪，则直接返回 启动 goroutine，处理 QMP 事件，如果为 GUEST_PANICKED 事件，并且指定了 [hypervisor].guest_memory_dump_path，则转储 VM 的内存信息 保存 sandbox 元数据信息 创建 [hypervisor].guest_memory_dump_path/&lt;sandboxID&gt;/state 目录（如果不存在） 将 &lt;storage.PersistDriver.RunStoragePath&gt;/&lt;sandboxID&gt; 目录下的内容拷贝至 [hypervisor].guest_memory_dump_path/&lt;sandboxID&gt;/state 目录中 将 hypervisor 的配置信息写入 [hypervisor].guest_memory_dump_path/&lt;sandboxID&gt;/hypervisor.conf 文件中 执行 qemu-system –version，获取 QEMU 的版本信息，写入 [hypervisor].guest_memory_dump_path/&lt;sandboxID&gt;/hypervisor.version 文件中 校验 [hypervisor].guest_memory_dump_path/&lt;sandboxID&gt; 目录空间是否为 VM 内存（静态 + 热添加）的两倍以上 向 QMP 服务发送 dump-guest-memory 命令，将 VM 中的内存内容转储到 [hypervisor].guest_memory_dump_path/&lt;sandboxID&gt;/vmcore-&lt;currentTime&gt;.elf 文件中，是否内存分页取决于 [hypervisor].guest_memory_dump_paging 启动 QMP 服务，监听 qmp.sock，校验 QEMU 版本是否大于 5.x 向 QMP 服务发送 qmp_capabilities 命令，从 capabilities negotiation 模式切换至 command 模式，命令无报错则视为 VM 处于正常运行状态 hotplugDeviceVM 设备热插拔 source code block 先调用 blockdev-add 命令是为了创建一个块设备，并将其配置为所需的类型、格式等。这个过程中，QEMU 会加载相应的块设备驱动程序，并为块设备分配所需的资源。然后调用 device_add 命令是为了将该块设备添加到 VM 中，使其成为 VM 的一部分。 调用 qmpSetup，初始化 QMP 服务 如果为热添加 如果 [hypervisor].block_device_driver 为 nvdimm，或者为 PMEM 设备，则向 QMP 服务发送 object-add 和 device_add 命令，为 VM 添加块设备；否则，向 QMP 服务发送 blockdev-add 命令，准备块设备 如果 [hypervisor].block_device_driver 为 virtio-blk 或 virtio-blk-ccw 时，会在 bridge 中新增设备信息维护；如果为 virtio-scsi 时，设备会添加到 scsi0.0 总线中 向 QMP 服务发送 device_add 命令，为 VM 添加块设备 如果为热移除 如果 [hypervisor].block_device_driver 为 virtio-blk 时，移除 bridge 中维护的设备信息 向 QMP 服务发送 device_del 和 blockdev-del 命令，移除 VM 中的指定块设备 CPU 调用 qmpSetup，初始化 QMP 服务 如果为热添加 如果当前 VM 的 CPU 数量与待热添加的 CPU 数量之和超出 [hypervisor].default_maxvcpus 限制，并不会不报错中断，而是热插至最大数量限制 向 QMP 服务发送 query-hotpluggable-cpus 命令，获得 host 上可插拔的 CPU 列表 遍历所有可插拔的 CPU，向 QMP 服务发送 device_add 命令，为 VM 添加未被使用的 CPU（如果 CPU 的 qom-path 不为空，则代表其正在使用中）添加失败并不会报错，而是尝试其他 CPU，直至满足数量要求或者再无可用的 CPU 如果为热移除 只有热添加的 CPU 才可以热移除，因此需要校验期望热移除的 CPU 数量是否小于当前热添加的 CPU 数量 向 QMP 服务发送 device_del 命令，移除 VM 中最近添加的 CPU（即倒序移除） VFIO [hypervisor].hotplug_vfio_on_root_bus 决定是否允许 VFIO 设备在 root 总线上热插拔，默认为 true。VFIO 是一种用于虚拟化环境中的设备直通技术，它允许将物理设备直接分配给 VM，从而提高 VM 的性能和可靠性。然而，在桥接设备上进行 VFIO 设备的热插拔存在一些限制，特别是对于具有大型 PCI 条的设备。因此，通过将该选项设置为 true，可以在 root 总线上启用 VFIO 设备的热插拔，从而解决这些限制问题 调用 qmpSetup，初始化 QMP 服务 如果为热添加 如果启用 [hypervisor].hotplug_vfio_on_root_bus，则后续的设备添加操作会作用在 root 总线上，否则会作用在 bridge 上 向 QMP 服务发送 device_add 命令，为 VM 添加 VFIO-CCW、VFIO-PCI 或 VFIO-AP 设备 如果为热移除 如果未启用 [hypervisor].hotplug_vfio_on_root_bus，则移除 bridge 中维护的设备信息 向 QMP 服务发送 device_del 命令，移除 VM 中的指定 VFIO 设备 memory 先调用 object-add 命令是为了创建一个内存设备对象，并为其分配内存，以便后续使用。而后调用 device_add 命令是为了将该内存设备对象添加到 VM 中，使其成为 VM 的一部分，从而实现内存的热添加。 检验 VM protection 模式是否为 noneProtection，其他 VM protection 模式下均不支持内存热插拔特性 调用 qmpSetup，初始化 QMP 服务 仅支持热添加内存，不支持热移除 向 QMP 服务发送 query-memory-devices 命令，查询 VM 中所有的内存设备，用于生成下一个内存设备的 slot 序号 向 QMP 服务发送 object-add 和 device_add 命令，为 VM 添加内存设备 如果 VM 内核只支持通过探测接口热添加内存（通过内存设备的 probe 属性判断），则需要额外向 QMP 服务发送 query-memory-devices 命令，查询 VM 中最近的一个内存设备，回写其地址信息 endpoint netdev_add 添加的是网络前端设备，而 device_add 添加的是一个完整的设备，其中包括前端设备和后端设备。在添加网络设备时，通常需要先添加一个网络前端设备，然后再将它连接到一个网络后端设备上。 调用 qmpSetup，初始化 QMP 服务 如果为热添加 向 QMP 服务发送 getfd 命令，分别获取 tap 设备的 VMFds 和 VhostFds 的信息 bridge 中新增设备信息维护 向 QMP 服务发送 netdev_add 和 device_add 命令，为 VM 添加 PCI 或 CCW 类型（[hypervisor].machine 为 s390-ccw-virtio 时）的网络设备 如果为热移除 移除 bridge 中维护的设备信息 向 QMP 服务发送 device_del 和 netdev_del 命令，移除 VM 中指定的网络设备 vhost-user vhost-user 设备需要与 host 的网络堆栈进行通信，而 host 网络堆栈使用字符设备来管理网络连接。因此，要创建一个 vhost-user 设备，需要先创建一个字符设备，然后将其与 vhost-user 设备连接。 调用 qmpSetup，初始化 QMP 服务 如果为热添加，仅支持 vhost-user-blk-pci 类型的设备 向 QMP 服务发送 chardev-add 和 device_add 命令，为 VM 添加指定的 vhost-user 设备 如果为热移除 向 QMP 服务发送 device_del 和 chardev-remove 命令，移除 VM 中指定的 vhost-user 设备 CreateVM准备创建 VM 所需的配置信息 source code 根据 QEMU 实现的 hypervisor 配置项初始化对应架构下的 qemu，其中包含了 qemu-system（govmmQemu.Config）和 virtiofsd/nydusd（VirtiofsDaemon）进程的配置参数 StartVM启动 VM source code 以当前用户组信息创建 &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt; 目录（如果不存在） 如果启用 [hypervisor].enable_debug，则设置 qemuConfig.LogFile 为 &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/qemu.log 如果未启用 [hypervisor].disable_selinux，则向 /proc/thread-self/attr/exec （如果其不存在，则为 /proc/self/task/&lt;PID&gt;/attr/exec）中写入 OCI spec.Process.SelinuxLabel 中声明的内容，VM 启动之后会重新置空 如果 [hypervisor].shared_fs 为 virtiofs-fs 或者 virtio-fs-nydus，则调用 VirtiofsDaemon 的 Start，启动 virtiofsd 进程，回写 virtiofsd PID 至 qemustate 中 构建 QEMU 进程的启动参数、执行命令的文件句柄、属性、标准输出等信息，执行 qemu-system 可执行文件，启动 qemu-system 进程。如果启用 [hypervisor].enable_debug 并且配置中指定了日志文件路径，则读取日志内容，追加错误信息 关停当前的 QMP 服务，执行类似于 qmpSetup 的流程，初始化 QMP 服务，无报错即视为 VM 处于正常运行状态 如果 VM 从模板启动 调用 qmpSetup，初始化 QMP 服务 向 QMP 服务发送 migrate-set-capabilities 命令，设置在迁移过程中忽略共享内存，避免数据的错误修改和不一致性 向 QMP 服务发送 migrate-incoming 命令，用于将迁移过来的 VM 恢复到 [factory].template_path/state 中 向 QMP 服务发送 query-migrate 命令，查询迁移进度，直至完成 如果启用 [hypervisor].enable_virtio_mem virtio-mem 设备后续会添加至 VM 的 root 总线中，获取地址和 bridge 等信息，后续执行 QMP 命令时传递 则向 QMP 服务发送 object-add 和 device_add 命令，为 VM 添加指定的 virio-mem 设备如果 QMP 添加设备失败，且报错中包含 Cannot allocate memory，则需要执行 echo 1 &gt; /proc/sys/vm/overcommit_memory 解决 StopVM关闭 VM source code 调用 qmpSetup，初始化 QMP 服务 如果 disableVMShutdown 为 true（调用 agent 的 init 时会返回 disableVMShutdown，用作 StopVM 的入参），则调用 GetPids，获得所有相关的 PIDs，kill 掉其中的 QEMU 进程（即列表中索引为 0 的 PID）；否则，则向 QMP 服务发送 quit 命令，关闭 QEMU 实例，关闭 VM 如果 [hypervisor].shared_fs 为 virtiofs-fs 或者 virtio-fs-nydus，调用 VirtiofsDaemon 的 Stop，关停 virtiofsd 服务 PauseVM暂停 VM source code 调用 qmpSetup，初始化 QMP 服务 向 QMP 服务发送 stop 命令，暂停 VM SaveVMsource code 调用 qmpSetup，初始化 QMP 服务 如果 VM 启动后作为模板，则向 QMP 服务发送 migrate-set-capabilities 命令，设置 VM 在迁移过程中忽略共享内存，避免数据的错误修改和不一致性 向 QMP 服务发送 migrate 命令，将 VM 迁移到指定 [factory].template_path/state 中 向 QMP 服务发送 query-migrate 命令，查询迁移进度，直至完成 ResumeVM恢复 VM source code 调用 qmpSetup，初始化 QMP 服务 向 QMP 服务发送 cont 命令，恢复 VM AddDevice向 VM 中添加设备 source code 根据不同设备类型，初始化对应的设备对象 — govmmQemu.Device，追加到 qemuConfig.Devices 中 HotplugAddDevice热添加指定设备至 VM 中 source code 调用 hotplugDevice，热添加指定设备至 VM 中 HotplugRemoveDevice热移除 VM 中的指定设备 source code 调用 hotplugDevice，热移除 VM 中的指定设备 ResizeMemory调整 VM 内存规格 source code 调用 GetTotalMemoryMB，获取 VM 当前的内存 调用 qmpSetup，初始化 QMP 服务 如果启用 [hypervisor].enable_virtio_mem，向 QMP 服务发送 qom-set 命令，设置 virtiomem0 设备的 requested-size 属性值为待热添加的内存量（即期望的 VM 内存与 [hypervisor].default_memory 的差值），直接返回virtio-mem 只需要将用于 host 和 guest 内存共享的 virtiomem0 设备内存扩大至预期大小即可，不需要返回内存设备对象，也不会调用 agent 通知内存上线 调用 HotplugAddDevice 或者 HotplugRemoveDevice，为 VM 调整内存规格，取决于 VM 当前内存是否大于预期 VM 内存大小如果期望的 VM 内存超出了 [hypervisor].default_maxmemory 限制，也不会报错中断，而是热插至最大数量限制 ResizeVCPUs调整 VM CPU 规格 source code 调用 HotplugAddDevice 或者 HotplugRemoveDevice，为 VM 调整 CPU 规格，取决于 VM 当前 CPU 是否大于预期 VM CPU 大小 GetTotalMemoryMB获取 VM 总内存 source code 返回 [hypervisor].default_memory 和已热添加内存之和 GetVMConsole获取 VM console 地址 source code 返回 &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/console.sock Disconnect断开 QMP 连接 source code channel 关闭，重置 QMP 对象 Capabilities获取 hypervisor 支持的特性 source code 设置并返回 hypervisor 默认支持特性，包括块设备、设备多队列和文件系统共享特性支持 GetThreadIDs获取 VM 中 CPU 的 threadID 信息 source code 调用 qmpSetup，初始化 QMP 服务 向 QMP 服务发送 query-cpus-fast 命令，获取 VM 中所有 CPU 详细信息 遍历所有 CPU，返回其 CPU ID 和 threadID 的映射关系 Cleanuphypervisor 相关资源清理 source code 关闭 QEMU 所有相关的文件句柄 GetPids获取 hypervisor 相关的 PID source code 读取 &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/pid 文件内容，如果 virtiofsd 服务的 PID 不为空，则一并返回 CheckVM 状态检查 source code 调用 qmpSetup，初始化 QMP 服务 向 QMP 服务发送 query-status 命令，查询并校验 VM 状态是否为 internal-error 或 guest-panicked GenerateSocket生成 host 和 guest 通信的 socket 地址 source code 获取 /dev/vhost-vsock 设备的文件句柄 获取一个从 0x3（contextID 中 1 和 2 是内部预留的） 到 0xFFFFFFFF（2^32 - 1）范围内可用的 contextID 返回包含 vhost-vsock 设备的文件句柄、可用的 contextID 以及端口为 1024 的 VSock 对象 IsRateLimiterBuiltinhypervisor 是否原生支持限速特性 source code 返回 false，QEMU 未内置支持限速功能 VirtiofsDaemonsrc/runtime/virtcontainers/virtiofsd.go VirtiofsDaemon 是用于 host 与 guest 的文件共享的进程服务，实现包括 virtiofsd 以及蚂蚁社区提出的 nydusd。 目前，暂时走读 virtiofsd 实现，后续补充其他 virtiofsd 实现。 12345678910111213141516171819202122232425262728293031323334// virtiofsd 进程启动参数中还有// --syslog：用于将日志发送至系统日志中// -o no_posix_lock：禁用 POSIX 锁定机制，从而提高文件系统的性能。但是，这也可能会导致在多个进程同时对同一个文件进行写操作时出现数据损坏的风险type virtiofsd struct { // Neded by tracing ctx context.Context // PID process ID of virtiosd process // --fd 参数，例如 --fd=3 // --fd 参数从 3 开始，0 为 stdin、1 为 stdout、2 为 stderr，具体取决于从 socketPath 中读取的 socket 文件句柄个数 PID int // path to virtiofsd daemon // [hypervisor].shared_fs path string // socketPath where daemon will serve // &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/vhost-fs.sock socketPath string // cache size for virtiofsd // -o 参数，例如 -o cache=auto // [hypervisor].virtio_fs_cache cache string // sourcePath path that daemon will help to share // -o 参数，例如 -o source=/run/kata-containers/shared/sandboxes/&lt;sandboxID&gt;/shared // &lt;XDG_RUNTIME_DIR&gt;/run/kata-containers/shared/sandboxes/&lt;containerID&gt;/shared sourcePath string // extraArgs list of extra args to append to virtiofsd command // [hypervisor].virtio_fs_extra_args extraArgs []string} Start启动 virtiofsd 服务 检验 virtiofsd 服务相关参数是否为空以及 /run/kata-containers/shared/sandboxes/&lt;containerID&gt;/shared 路径是否存在 获取 &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/vhost-fs.sock 的文件句柄，并将其权限设置为 root这里区别于 QEMU 进程，QEMU 可以以非 root 运行，而 virtiofsd 暂不支持，参考 https://github.com/kata-containers/kata-containers/issues/2542 执行 virtiofsd 可执行文件，启动 virtiofsd 进程 启动 goroutine，如果 virtiofsd 程序退出，则调用 Hypervisor 的 StopVM，执行清理操作 返回 virtiofsd 进程 PID Stop关停 virtiofsd 服务 kill 掉 virtiofsd 服务进程 移除 &lt;storage.PersistDriver.RunVMStoragePath&gt;/&lt;sandboxID&gt;/vhost-fs.sock 文件 Mount将 rafs 格式文件挂载至 virtiofs 挂载点 virtiofsd 场景下暂未实现。 Umount移除 virtiofs 挂载点下的 rafs 挂载文件 virtiofsd 场景下暂未实现。","link":"/2023/05/19/2023-05-19%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20virtcontainers%20hypervisor/"},{"title":"「 Containerd 」NRI 接口设计","text":"based on v0.3.0 目标NRI（Node Resource Interface）是 Containerd 的一个子项目，允许将自定义逻辑插入到 OCI 兼容的运行时中，从而实现在容器生命周期的某些特定时间点对容器进行更改操作或执行 OCI 规范之外的额外操作。例如，用于改进设备和其他容器资源的分配和管理。NRI 本身对任何容器运行时的内部实现细节是不感知的。它为 CRI 运行时提供了一个适配库，用于集成 NRI 和扩展插件进行交互。 NRI 提供了接口定义和基础组件，可以实现可插拔的 CRI 运行时插件，这些插件就是 NRI 插件。这些 NRI 插件是与运行时类型无关的，插件既可以应用于 Containerd，也可以应用于 CRI-O。原则上，任何 NRI 插件都应该能够和启用 NRI 的运行时正常协作。 NRI 插件是一个类似守护进程的实例。插件的单个实例会处理 NRI 所有的事件和请求，使用 socket 来进行数据传输和通信，NRI 定义了一套基于 protobuf 的协议：NRI plugin protocal，并通过 ttRPC 进行实现。这样可以通过降低每条信息的开销提高通信效率，并且可以实现有状态的 NRI 插件。 组件NRI 实现包含了两个核心组件：NRI 协议和 NRI 运行时适配器。 这些组件一起建立了运行时如何与 NRI 交互以及插件如何通过 NRI 与运行时中的容器交互的模型。它们还定义了插件可以在哪些条件下对容器进行更改以及这些更改的程度。 其余的组件是 NRI 插件 stub 库和一些 NRI 示例插件。其中，一些插件在实际应用场景中实现了有用的功能，另外一些插件则用于调试。所有示例插件都可以作为如何使用 stub 库实现 NRI 插件的示例。 API 协议NRI API 协议中定义了两个服务：runtime 和 plugin runtime 服务是运行时向 NRI 插件暴漏的接口。在此接口上的所有请求都由插件发起。该接口提供以下功能： 启动插件注册 请求对容器的更新 plugin 服务是 NRI 用于与插件交互的公共接口。在此接口上的所有请求都由 NRI 或运行时发起。该接口提供以下功能： 配置插件 获取已存在的 Pod 和容器的初始列表 将插件 hook 到 Pod/container 的生命周期事件中 关闭插件 插件需要向 NRI 注册，用于接收和处理容器事件。在注册过程中，插件和 NRI 执行以下步骤的顺序： 插件注册至运行时 NRI 向插件下发特定的配置数据 插件订阅 Pod 和容器生命周期事件 NRI 向插件发送已存在的 Pod 和容器列表 插件请求对现有容器的更新 通过插件名称和插件索引向 NRI 注册插件。NRI 通过插件索引来确定所有插件在 hook Pod 和容器的生命周期事件的处理顺序。 NRI 插件名称用于 NRI 服务从默认插件配置路径 /etc/nri/conf.d 中选择对应插件的配置文件发送给 NRI 插件。只有当对应的 NRI 插件被 NRI 服务内部调用时，才会读取对应的配置文件。如果 NRI 插件是从外部启动的，那么它也可以通过其他方式获取配置。NRI 插件可以根据需要订阅 Pod 和容器的生命周期，并且返回修改后的配置。NRI 插件如果采用预注册的方式运行时，需要将可执行文件的命名规则需要符合 xx-plugin_name，例如 01-logger。其中 xx 必须为两位数字，作为 NRI 插件的索引，决定了插件的的执行顺序。 在注册和握手的最后一步，NRI 发送 CRI 运行时已知的所有的 Pod和容器的信息。此时插件可以对任何已经存在的 Pod 和容器进行更新。一旦握手结束，并且 NRI 插件成功向 NRI 服务注册之后，它将开始根据自己的订阅接收 Pod 和容器的生命周期事件。 运行时适配器NRI 运行时适配包是用于集成到 NRI 并与 NRI 插件交互的接口运行时。它实现了插件发现，启动和配置。它还提供了将 NRI 插件插入到 CRI 运行时的 Pod 和容器的生命周期事件中的必要功能。 运行时适配器实现了多个 NRI 插件可能在处理同一个 Pod 或者容器的生命周期事件。它负责按照索引顺序依次调用插件，并把插件的修改内容合并后返回。在合并插件修改的 OCI spec 时，当检测到到多个 NRI 插件对同一个容器产生了冲突的修改，就会返回一个错误。 其他组件NRI 还包含一个 NRI 插件 stub 库，为 NRI 插件的实现提供了一个简洁易用的框架。stub 库屏蔽了 NRI 插件的底层实现细节，它负责连接建立、插件注册、配置和事件订阅。 同时 NRI 也提供了一些 NRI 插件的示例，这些示例都是结合实际使用场景创建的，其中一些示例非常适合调试场景。目前，NRI 提供的所有示例插件都基于 stub 库实现的。这些示例插件的实现都可以用作学习使用 stub 库的教程。 另外，NRI 还包含一个 OCI 规范生成器主要用于 NRI 插件用来调整和更新 OCI spec，然后更新到容器。 可订阅事件123456789101112131415161718192021// Handlers for NRI plugin event and request.type handlers struct { Configure func(context.Context, string, string, string) (api.EventMask, error) Synchronize func(context.Context, []*api.PodSandbox, []*api.Container) ([]*api.ContainerUpdate, error) Shutdown func(context.Context) // Pod 事件 RunPodSandbox func(context.Context, *api.PodSandbox) error StopPodSandbox func(context.Context, *api.PodSandbox) error RemovePodSandbox func(context.Context, *api.PodSandbox) error // 容器事件 CreateContainer func(context.Context, *api.PodSandbox, *api.Container) (*api.ContainerAdjustment, []*api.ContainerUpdate, error) StartContainer func(context.Context, *api.PodSandbox, *api.Container) error UpdateContainer func(context.Context, *api.PodSandbox, *api.Container, *api.LinuxResources) ([]*api.ContainerUpdate, error) StopContainer func(context.Context, *api.PodSandbox, *api.Container) ([]*api.ContainerUpdate, error) RemoveContainer func(context.Context, *api.PodSandbox, *api.Container) error PostCreateContainer func(context.Context, *api.PodSandbox, *api.Container) error PostStartContainer func(context.Context, *api.PodSandbox, *api.Container) error PostUpdateContainer func(context.Context, *api.PodSandbox, *api.Container) error} 在事件中可以获得的 Pod 元信息： ID name UID namespace labels annotations cgroup parent directory runtime handler name 在事件中可以获得的容器元数据： ID pod ID name state labels annotations command line arguments environment variables mounts OCI hooks linux namespace IDs devices resources memory limit reservation swap limit kernel limit kernel TCP limit swappiness OOM disabled flag hierarchical accounting flag hugepage limits CPU shares quota period realtime runtime realtime period cpuset CPUs cpuset memory Block I/O class RDT class 容器调整 在容器创建过程中可以调整容器的参数，在容器创建后，任何生命周期事件都可以更新容器的参数，但是调整参数和更新参数的范围是不同的，容器创建时支持更多的参数设置，容器创建完成后，只有部分参数可以修改。其中 ID、pod ID、name、state、labels、command line arguments、OCI hooks 和 linux.namespace IDs 信息不可修改。 容器更新 容器创建完成后，NRI 插件可以对容器进行更新。这个更新操作也可以由其他任何容器创建、更新或者停止的事件触发，或者可以主动更新容器参数。更新过程中，可以改的容器的参数要少于创建时可修改的参数，其中仅 linux.resources 信息可修改。 安全性从安全角度来看，应该将 NRI 插件视为容器运行时的一部分。NRI 没有实现对其提供的功能的细粒度访问控制。访问 NRI 是通过限制对系统范围的 NRI socket 的访问来控制的。如果进程可以连接到 NRI socket 并发送数据，则可以访问通过 NRI 可用的完整功能范围。 特别是包括： 注入OCI hook，允许以与容器运行时相同的特权级别执行任意进程 对挂载点进行任意更改，包括新的绑定挂载点、更改 proc、sys、mqueue、shm 和 tmpfs 挂载点 添加或删除任意设备 对可用内存、CPU、block I/O 和 RDT 资源的限制进行任意更改，包括通过设置非常低的限制来拒绝服务 保护 NRI socket 的注意事项和原则与保护运行时本身的 socket 相同。除非它已经存在，否则 NRI 本身会创建目录来保存其 socket，该目录具有仅允许运行时进程的用户 ID 访问的权限。默认情况下，这限制 NRI 访问以 root UID 0 身份运行的进程。强烈建议不要更改默认 socket 权限。如果没有对容器安全的全部影响和潜在后果的充分理解，就永远不应该对 NRI 启用更宽松的访问控制。 当运行时管理 Kubernetes 集群中的 Pod 和容器时，使用 Kubernetes DaemonSets 可以方便地部署和管理 NRI 插件。除此之外，这需要将 NRI socket 挂载到运行插件的特权容器的文件系统中。对于保护 NRI socket 和 NRI 插件，应采取与 Kubelet Device Manager socket 和 Kubernetes device-plugin 类似的手段。 集群配置应确保未经授权的用户无法挂载 host 目录并创建特权容器来访问这些 socket 并充当 NRI 或 device-plugin。 与运行时集成Containerd Containerd 在 v1.7.0 版本中新增对 NRI 特性的支持，通过在 Containerd 配置文件的 [plugins.&quot;io.containerd.nri.v1.nri&quot;] 部分中配置： 123456789101112131415[plugins.&quot;io.containerd.nri.v1.nri&quot;] # Enable NRI support in containerd. disable = false # Allow connections from externally launched NRI plugins. disable_connections = false # plugin_config_path is the directory to search for plugin-specific configuration. plugin_config_path = &quot;/etc/nri/conf.d&quot; # plugin_path is the directory to search for plugins to launch on startup. plugin_path = &quot;/opt/nri/plugins&quot; # plugin_registration_timeout is the timeout for a plugin to register after connection. plugin_registration_timeout = &quot;5s&quot; # plugin_requst_timeout is the timeout for a plugin to handle an event/request. plugin_request_timeout = &quot;2s&quot; # socket_path is the path of the NRI socket to create for plugins to connect to. socket_path = &quot;/var/run/nri/nri.sock&quot; 有两种方法可以启动 NRI 插件： 预注册（pre-connected）：当 NRI 适配器实例化时，NRI 插件就会自动启动。预注册就是将 NRI 的可执行文件放置到 NRI 插件的指定路径中，默认路径通过 plugin_path 指定，当 Containerd 启动时，就会自动加载并运行在该路径下注册的 NRI 插件 外部运行（external）：NRI 插件进程可以由 systemd 创建，或者运行在 Pod 中。只要保证 NRI 插件可以通过 NRI socket 和 Containerd 进行通信即可，默认的 NRI socket 存储路径通过 socket_path 指定 预注册的插件是通过一个预先连接到 NRI 的 socket 启动，外部运行的插件通过 NRI socket 向 NRI 适配器注册自己。预注册插件和外部启动插件，这两种运行方式唯一的不同点就是如何启动以及如何连接到 NRI。一旦建立了连接，所有的 NRI 插件都是相同的。 NRI 可以通过 disable_connections 选项禁用外部运行插件的连接，在这种情况下 NRI socket 将不会被创建。 简单示例以 NRI logger 插件为例。 插件编译 1234567$ git clone https://github.com/containerd/nri.git$ cd plugins/logger/# 命名格式必须为“索引-名称”，其中索引必须为 2 位数字，否则无法通过校验# FATAL [0000] failed to create plugin stub: invalid plugin index &quot;nri&quot;, must be 2 digits$ go build -o 01-logger nri-logger$ cp 01-logger /opt/nri/plugins/00-logger logger 插件逻辑就是在各个 Pod 和容器生命周期节点格式化输出元数据信息。此外， CreateContainer 阶段中会设置环境变量和注解等信息： 12345678910111213141516171819202122func (p *plugin) CreateContainer(_ context.Context, pod *api.PodSandbox, container *api.Container) (*api.ContainerAdjustment, []*api.ContainerUpdate, error) { dump(&quot;CreateContainer&quot;, &quot;pod&quot;, pod, &quot;container&quot;, container) adjust := &amp;api.ContainerAdjustment{} if cfg.AddAnnotation != &quot;&quot; { adjust.AddAnnotation(cfg.AddAnnotation, fmt.Sprintf(&quot;logger-pid-%d&quot;, os.Getpid())) } if cfg.SetAnnotation != &quot;&quot; { adjust.RemoveAnnotation(cfg.SetAnnotation) adjust.AddAnnotation(cfg.SetAnnotation, fmt.Sprintf(&quot;logger-pid-%d&quot;, os.Getpid())) } if cfg.AddEnv != &quot;&quot; { adjust.AddEnv(cfg.AddEnv, fmt.Sprintf(&quot;logger-pid-%d&quot;, os.Getpid())) } if cfg.SetEnv != &quot;&quot; { adjust.RemoveEnv(cfg.SetEnv) adjust.AddEnv(cfg.SetEnv, fmt.Sprintf(&quot;logger-pid-%d&quot;, os.Getpid())) } return adjust, nil, nil} 配置启动 这里采用外部启动（00-logger）和预先配置（01-logger）两种启动方式 00-logger：设置环境变量 logger-env 与注解 logger-annotation，值为 logger-pid-&lt;PID&gt; 01-logger：设置环境变量 logger-env，值为 logger-pid-&lt;PID&gt; 重启 Containerd 发现预先配置的 01-logger 插件： 12345678910$ journalctl -xeu containerdJul 05 17:35:43 wnx containerd[84985]: time=&quot;2023-07-05T17:35:43.730685674+08:00&quot; level=info msg=&quot;using experimental NRI integration - disable nri plugin to prevent this&quot;...Jul 05 17:35:44 wnx containerd[84985]: time=&quot;2023-07-05T17:35:44.019786972+08:00&quot; level=info msg=&quot;starting plugins...&quot;Jul 05 17:35:44 wnx containerd[84985]: time=&quot;2023-07-05T17:35:44.019915270+08:00&quot; level=info msg=&quot;discovered plugin 01-logger&quot;Jul 05 17:35:44 wnx containerd[84985]: time=&quot;2023-07-05T17:35:44.019929420+08:00&quot; level=info msg=&quot;starting plugin \\&quot;logger\\&quot;...&quot;Jul 05 17:35:44 wnx containerd[84985]: time=&quot;2023-07-05T17:35:44.049078817+08:00&quot; level=info msg=&quot;plugin \\&quot;pre-connected:01-logger[84985]\\&quot; registered as \\&quot;01-logger\\&quot;&quot;Jul 05 17:35:44 wnx containerd[84985]: time=&quot;2023-07-05T17:35:44.050249456+08:00&quot; level=info msg=&quot;plugin invocation order&quot;Jul 05 17:35:44 wnx containerd[84985]: time=&quot;2023-07-05T17:35:44.050284541+08:00&quot; level=info msg=&quot; #1: \\&quot;01-logger\\&quot; (pre-connected:01-logger[84985])&quot;Jul 05 17:35:44 wnx containerd[84985]: time=&quot;2023-07-05T17:35:44.050528623+08:00&quot; level=info msg=&quot;containerd successfully booted in 1.616308s&quot; 00-logger 插件注册后，订阅了所有的 Pod 和容器事件；插件启动后，即收到了运行时所有的 Pod 和容器信息： 1234567891011121314$ ./00-logger --set-annotation logger-annotationINFO [0000] Created plugin 00-logger (00-logger, handles RunPodSandbox,StopPodSandbox,RemovePodSandbox,CreateContainer,PostCreateContainer,StartContainer,PostStartContainer,UpdateContainer,PostUpdateContainer,StopContainer,RemoveContainer) INFO [0000] Registering plugin 00-logger... INFO [0000] Configuring plugin 00-logger for runtime containerd/v1.7.2... INFO [0000] got configuration data: &quot;&quot; from runtime containerd v1.7.2 INFO [0000] Subscribing plugin 00-logger (00-logger) for events RunPodSandbox,StopPodSandbox,RemovePodSandbox,CreateContainer,PostCreateContainer,StartContainer,PostStartContainer,UpdateContainer,PostUpdateContainer,StopContainer,RemoveContainer INFO [0000] Started plugin 00-logger... INFO [0000] Synchronize: pods: INFO [0000] Synchronize: - annotations: INFO [0000] Synchronize: io.kubernetes.cri.container-type: sandbox INFO [0000] Synchronize: io.kubernetes.cri.sandbox-cpu-period: &quot;100000&quot; INFO [0000] Synchronize: io.kubernetes.cri.sandbox-cpu-quota: &quot;100000&quot; INFO [0000] Synchronize: io.kubernetes.cri.sandbox-cpu-shares: &quot;1024&quot; ... Containerd 服务也收到了来自外部启动的 00-logger 插件信息： 12345678$ journalctl -xeu containerdJul 05 17:38:49 wnx containerd[84985]: time=&quot;2023-07-05T17:38:49.236103390+08:00&quot; level=info msg=&quot;plugin \\&quot;external:00-logger[87525]\\&quot; registered as \\&quot;00-logger\\&quot;&quot;Jul 05 17:38:49 wnx containerd[84985]: time=&quot;2023-07-05T17:38:49.237601881+08:00&quot; level=info msg=&quot;Synchronizing NRI (plugin) with current runtime state&quot;Jul 05 17:38:49 wnx containerd[84985]: time=&quot;2023-07-05T17:38:49.328071227+08:00&quot; level=info msg=&quot;synchronizing plugin 00-logger&quot;Jul 05 17:38:49 wnx containerd[84985]: time=&quot;2023-07-05T17:38:49.797028952+08:00&quot; level=info msg=&quot;plugin invocation order&quot;Jul 05 17:38:49 wnx containerd[84985]: time=&quot;2023-07-05T17:38:49.797164886+08:00&quot; level=info msg=&quot; #1: \\&quot;00-logger\\&quot; (external:00-logger[87525])&quot;Jul 05 17:38:49 wnx containerd[84985]: time=&quot;2023-07-05T17:38:49.797197017+08:00&quot; level=info msg=&quot; #2: \\&quot;01-logger\\&quot; (pre-connected:01-logger[84985])&quot;Jul 05 17:38:49 wnx containerd[84985]: time=&quot;2023-07-05T17:38:49.797226434+08:00&quot; level=info msg=&quot;plugin \\&quot;00-logger\\&quot; connected&quot; 多 NRI 插件是按照索引顺序执行，因此 01-logger 会重新设置 00-logger 设置的 logger-env 环境变量： 123$ crictl inspect 6577fa85ac7e6 | grep logger &quot;logger-env=logger-pid-85262&quot; &quot;logger-annotation&quot;: &quot;logger-pid-87525&quot; 更多价值节点细粒度资源管理为了满足不同业务应用场景的需求，特别是在在线任务与离线任务混布的场景下，在提高资源利用率的同时，也要保证延迟敏感服务可以得到充分的资源保证，这就需要 Kubernetes 提供更加细粒度的资源管理功能，增强容器的隔离性，减少容器之间的互相干扰。例如，CPU 编排，内存分层，缓存管理，IO 管理等。目前有很多方案，但是都有其一定的局限性。 截至目前，Kubernetes 并没有提供一个非常完善的资源管理方案，很多 Kubernetes 周边的开源项目通过一些自己的方式修改 Pod 的部署和管理流程，实现资源分配的细粒度管理。例如 cri-resource-manager、Koordinator、Crane 等项目。 这些项目对 Kubernetes 创建和更新 Pod 的流程的优化可以大致分为两种模式，一种是 proxy 模式，一种是 standalone 模式： proxy proxy 模式是在客户端 Kubelet 和 CRI 运行时之间增加一个 CRI proxy 中继请求和响应，在 proxy 中劫持 Pod 以及容器的创建/更新/删除事件，对 Pod spec 进行修改或者完善，将硬件感知的资源分配策略应用于容器中。 standalone standalone 模式是在每一个工作节点上创建一个 agent，当这个 agent 监听到在本节点的 Pod 创建或者修改事件的时候，再根据 Pod spec 中的注解等扩展信息，转换成细粒度资源配置的 spec，然后调用 CRI 运行时实现对 Pod 的更新。 这两种方式在满足特定业务需求的同时也存在一定的缺点, 两种方式都需要依赖额外的组件，来捕获 Pod 的生命周期事件。proxy 模式增加了 Pod 创建管理流程的链路以及部署和维护成本，standalone 模式是在侦听到 Pod 创建以及修改的事件后，才会对 Pod 进行更新，会有一定的延迟。 使用 NRI 可以将 Kubelet 的 Resource Manager 下沉到 CRI 运行时层进行管理。Kubelet 当前不适合处理多种需求的扩展，在 Kubelet 层增加细粒度的资源分配会导致 Kubelet 和 CRI 的界限越来越模糊。而 NRI，则是在 CRI 生命周期间做调用，更适合做资源绑定和节点的拓扑感知。并且在 CRI 内部做插件定义和迭代，可以做到上层 Kubenetes 以最小的代价来适配变化。 到现在为止，已经有越来越多的节点资源细粒度管理方案开始探索使用 NRI 实现的可能性。当 NRI 成为节点细粒度资源分配管理方案后，可以进一步提高资源管理方案的标准化，提高相关组件的可复用性。参考：https://github.com/containers/nri-plugins。","link":"/2023/07/03/2023-07-03%20Containerd%20NRI%20%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1/"},{"title":"「 Kubernetes 」CPU 精细化管理","text":"based on v1.24.10 背景现代多核服务器大多采用非统一内存访问架构（Non-uniform memory access，简称 NUMA）来提高硬件的可伸缩性。NUMA 是一种为多处理器的电脑设计的内存架构，内存访问时间取决于内存相对于处理器的位置。在 NUMA 架构下，处理器访问它自己的本地内存的速度比非本地内存（内存位于另一个处理器，或者是处理器之间共享的内存）快一些。 在 Kubernetes 中，调度器的调度粒度为节点级别，并不感知和考虑节点硬件拓扑的存在。在某些延迟敏感的场景下，可能希望 Kubernetes 为 Pod 分配拓扑最优的节点和硬件，以提升硬件利用率和程序性能。CPU 敏感型应用有如下特点： 对 CPU throttling 敏感 对上下文切换敏感 对处理器缓存未命中敏感 对跨 socket 内存访问敏感 同时，在某些复杂场景下，部分的 Pod 属于 CPU 密集型工作负载，Pod 之间会争抢节点的 CPU 资源。当争抢剧烈的时候，Pod 会在不同的 CPU core 之间进行频繁的切换，更糟糕的是在 NUMA node 之间的切换。这种大量的上下文切换，会影响程序运行的性能。Kubernetes 的 CPU manager 一定程度可以解决以上问题，但是因为 CPU manager 特性是节点级别的 CPU 调度选择，所以无法在集群维度中选择最优的 CPU core 组合。同时 CPU manager 特性要求 Pod QoS 为 Guaranteed 时才能生效，且无法适用于所有 QoS 类型的 Pod。 Kubernetes 中虽然有 Topology Manager 来管理节点资源的拓扑对齐，但是没有与调度器联动，导致调度结果和设备资源分配结果可能不一致。此外，Topology Manager 在进行资源对齐时，仅仅停留在 NUMA 维度，并未考量到 CPU socket 和 core 拓扑等细粒度概念。 设计思考NUMA 拓扑感知调度KEP 议题 引入 Topology Manager 后，支持 Pod 在存在不同的 NUMA 拓扑和不同数量的拓扑资源集群节点中启动。但是存在 Pod 可能被调度到总资源量足够的节点上，但资源分配却无法满足预期的拓扑策略，从而导致 Pod 启动失败（TopologyAffinityError）。对于 Kube-scheduler 来说，更好的行为方式应该是选择适当的节点，与 Kubelet Topology Manager 策略对齐，以便 Kubelet 可以允许 Pod 运行。 需要做出的改动有 当节点上有 NUMA 拓扑时，通过使用 scheduler-plugin 使调度过程更加精确 考虑 NUMA 拓扑，做出更优化的调度决策 需要一个在 Kubelet 外部运行的 agent（社区参考实现），用于收集有关正在运行 Pod 的所有必要信息，根据节点的可分配资源和 Pod 消耗的资源，它将在 CRD 中提供可用资源，其中一个 CRD 实例代表一个节点。 CRD 实例的名称就是节点的名称。 Filter 插件实现了一个与原 Topology Manager 算法不同的简化版的 Topology Manager。该插件以 single-numa-node 策略的标准检查各节点是否具备运行 Pod 的能力。由于这是最严格的 Topology Manager 策略，如果该策略条件通过，则意味着也必然满足其他策略条件。Filter 插件将使用 CRD 来识别节点上启用的拓扑策略以及节点上可用资源的拓扑信息。另外，Score 插件将进一步考虑最适合运行 Pod 的节点。 CRD 设计 具有节点拓扑的可用资源应存储在 CRD 中，其格式应遵循 Kubernetes Node Resource Topology Custom Resource Definition Standard。社区参考设计。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// NodeResourceTopologyList is a list of NodeResourceTopology resourcestype NodeResourceTopologyList struct { metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ListMeta `json:&quot;metadata&quot;` Items []NodeResourceTopology `json:&quot;items&quot;`}// NodeResourceTopology is a specification for a NodeResourceTopology resourcetype NodeResourceTopology struct { metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;` TopologyPolicies []string `json:&quot;topologyPolicies&quot;` Zones ZoneList `json:&quot;zones&quot;`}// Zone is the spec for a NodeResourceTopology resourcetype Zone struct { Name string `json:&quot;name&quot;` Type string `json:&quot;type&quot;` Parent string `json:&quot;parent,omitempty&quot;` Costs CostList `json:&quot;costs,omitempty&quot;` Attributes AttributeList `json:&quot;attributes,omitempty&quot;` Resources ResourceInfoList `json:&quot;resources,omitempty&quot;`}type ZoneList []Zonetype ResourceInfo struct { Name string `json:&quot;name&quot;` Allocatable intstr.IntOrString `json:&quot;allocatable&quot;` Capacity intstr.IntOrString `json:&quot;capacity&quot;`}type ResourceInfoList []ResourceInfotype CostInfo struct { Name string `json:&quot;name&quot;` Value int `json:&quot;value&quot;`}type CostList []CostInfotype AttributeInfo struct { Name string `json:&quot;name&quot;` Value string `json:&quot;value&quot;`}type AttributeList []AttributeInfo 例如： 123456789101112131415161718192021222324252627282930313233343536373839apiVersion: topology.node.k8s.io/v1alpha1kind: NodeResourceTopologymetadata: name: node1topologyPolicies:- SingleNUMANodeContainerLevelzones:- costs: - name: node-0 value: 10 - name: node-1 value: 21 name: node-0 resources: - allocatable: &quot;12&quot; available: &quot;12&quot; capacity: &quot;24&quot; name: cpu - allocatable: &quot;68590714880&quot; available: &quot;68590714880&quot; capacity: &quot;68590714880&quot; name: memory type: Node- costs: - name: node-0 value: 21 - name: node-1 value: 10 name: node-1 resources: - allocatable: &quot;24&quot; available: &quot;12&quot; capacity: &quot;24&quot; name: cpu - allocatable: &quot;68719476736&quot; available: &quot;68719476736&quot; capacity: &quot;68719476736&quot; name: memory type: Node 已知限制 Kube-scheduler 在 NUMA 感知调度 Pod 流程之后，并不知道节点上 Topology Manager 实际为 Pod 分配的 NUMA 情况，节点上的 Topology Manager 也未必按照 scheduler-plugin 中的预选算法进行分配。 因此，KEP 中建议 Kube-scheduler 可以将分配的 NUMA ID 作为 Pod 提示透传，节点的 Topology Manager 也可以根据 Pod 中的相关提示信息考虑实际的分配策略（这部分涉及到 Topology Manager 的改动，暂未实现）。 节点 CPU 编排 分配优先级 为了多核共享 L1 和 L2 cache，优先分配位于同一物理核心的两个逻辑核心。即图中的 0 和 16 号 CPU 分配优先级高于 0 和 1 号 CPU 为了多核共享 L3 cache ，优先分配位于同一 NUMA 的两个逻辑核心。即图中的 0 和 1 号 CPU 分配优先级高于 0 和 4 号 CPU 扩展思考点 考虑到超线程性能的发挥瓶颈，对于 CPU 满载服务而言，同一物理核心的两个逻辑核心未必比来自不同物理核心的性能强，因此可以针对应用本身的业务模型，是否分配自同一个物理核心有待考量 CPU 的分配优先级可以不仅仅从静态拓扑结构角度思考设计，也可以结合 CPU 频率、flag 等属性信息以及 CPU 真实使用率等动态实时信息，多维度的考量 考虑到节点资源利用率，对于非 Guaranteed QoS 的 Pod 而言，往往也需要不同程度的 CPU 精细化管理 由于集群资源动态变化，最初未满足最佳分配策略的服务，可以借助适时重分配或重调度调整至最优分配效果 拓扑资源对齐不仅仅限制于 CPU 资源，往往一套完整的拓扑资源对齐方案会将 CPU、内存、GPU、网卡等硬件设备均考虑在内 现阶段，在不修改 CPU Manager、Topology Manager 等原有模块逻辑的前提下，往往需要一个旁路 agent（standalone 模式）或者 hook CRI（proxy 模式）调用的模式来接管资源管理的能力，并且往往需要禁用原生的管理策略 随着 NRI（Node Resource Interface）规范的完善，可以基于 NRI hook 扩展，实现资源编排 社区成果Cranehttps://github.com/gocrane/crane Crane 是一个基于 FinOps 的云资源分析与成本优化平台。它的愿景是在保证客户应用运行质量的前提下实现极致的降本。 设计概述 Crane-scheduler 和 Crane-agent 配合工作，完成拓扑感知调度与资源分配的工作： Crane-agent 从节点采集资源拓扑，包括 NUMA、socket、设备等信息，汇总到 NodeResourceTopology CRD 中 Crane-scheduler 在调度时会参考节点的 NodeResourceTopology 对象获取到节点详细的资源拓扑结构，在调度到节点的同时还会为 Pod 分配拓扑资源，并将结果写到 Pod 的 annotations 中 Crane-agent 在节点上 watch 到 Pod 被调度后，从 Pod 的 annotations 中获取到拓扑分配结果，并按照用户给定的 CPU 绑定策略进行 CPUset 的细粒度分配 CPU 分配策略 Crane 中提供了四种 CPU 分配策略，分别如下： none：该策略不进行特别的 CPUset 分配，Pod 会使用节点 CPU 共享池 exclusive：该策略对应 Kubelet 的 static 策略，Pod 会独占 CPU 核心，其他任何 Pod 都无法使用 numa：该策略会指定 NUMA Node，Pod 会使用该 NUMA Node 上的 CPU 共享池 immovable：该策略会将 Pod 固定在某些 CPU 核心上，但这些核心属于共享池，其他 Pod 仍可使用 为系统组件预留 CPU 在某些场景下，希望能对 Kubelet 预留的 CPU 做一些保护，使用场景包括但不限于： 在混部场景下，不希望离线任务绑定系统预留的 CPU 核心，防止对 K8s 系统组件产生影响 0 号核心在 Linux 有独特用途，比如处理网络包、内核调用、处理中断等，因此不希望任务绑定 0 号核心 在 Crane 中，可以通过以下方式为系统组件预留 CPU： Kubelet 设置预留 CPU：按照官方指引设置预留的 CPU 列表 查看 NodeResourceTopology 对象，spec.attributes 中的 go.crane.io/reserved-system-cpus 存储了预留的 CPU 列表 在 Pod 的 annotations 中添加 topology.crane.io/exclude-reserved-cpus，表明 Pod 不绑定预留的 CPU 核心 Koordinatorhttps://github.com/koordinator-sh/koordinator Koordinator 是一个基于 QoS 的 Kubernetes 混合工作负载调度系统，旨在提高对延迟敏感的工作负载和批处理作业的运行时效率和可靠性，简化与资源相关的配置调整的复杂性，并增加 Pod 部署密度以提高资源利用率。 设计概述 当 Koordlet 启动时，Koordlet 从 Kubelet 收集 NUMA 拓扑信息，包括 NUMA 拓扑、CPU 拓扑、Kubelet CPU 管理策略、Kubelet 为 Guaranteed Pod 分配的 CPU 等，并更新到节点资源拓扑 CRD。当延迟敏感的应用程序扩容时，可以为新 Pod 设置 Koordinator QoS LSE/LSR、CPU 绑定策略和 CPU 独占策略，要求 Koord-scheduler 分配最适合的 CPU 以获得最佳性能。当 Koord-scheduler 调度 Pod 时，Koord-scheduler 会过滤满足 NUMA 拓扑对齐策略的节点，并通过评分选择最佳节点，在 Reserve 阶段分配 CPU，并在 PreBinding 时将结果记录到 Pod annotations。Koordlet 通过 hook Kubelet CRI 请求，替换通过 Koord-scheduler 调度的 CPU 配置参数到运行时，例如配置 cgroup。 QoS Koordinator 调度系统支持的 QoS 有五种类型: QoS 特点 说明 SYSTEM 系统进程，资源受限 对于 DaemonSets 等系统服务，虽然需要保证系统服务的延迟，但也需要限制节点上这些系统服务容器的资源使用，以确保其不占用过多的资源 LSE(Latency Sensitive Exclusive) 保留资源并组织同 QoS 的 Pod 共享资源 很少使用，常见于中间件类应用，一般在独立的资源池中使用 LSR(Latency Sensitive Reserved) 预留资源以获得更好的确定性 类似于社区的 Guaranteed，CPU 核被绑定 LS(Latency Sensitive) 共享资源，对突发流量有更好的弹性 微服务工作负载的典型QoS级别，实现更好的资源弹性和更灵活的资源调整能力 BE(Best Effort) 共享不包括 LSE 的资源，资源运行质量有限，甚至在极端情况下被杀死 批量作业的典型 QoS 水平，在一定时期内稳定的计算吞吐量，低成本资源 Koordinator 和 Kubernetes QoS 之间是有对应关系的: Koordinator QoS Kubernetes QoS SYSTEM — LSE Guaranteed LSR Guaranteed LS Guaranteed/Burstable BE BestEffort CPU 编排基本原则 仅支持 Pod 维度的 CPU 分配机制 Koordinator 将机器上的 CPU 分为 CPU Shared Pool，statically exclusive CPUs 和 BE CPU Shared Pool： CPU Shared Pool 是一组共享 CPU 池，Burstable 和 LS Pod 中的任何容器都可以在其上运行。Guaranteed fractional CPU requests 的 Pod 也可以运行在 CPU Shared Pool 中。CPU Shared Pool 包含节点中所有未分配的 CPU，但不包括由 Guaranteed、LSE 和 LSR Pod 分配的 CPU。如果 Kubelet 保留 CPU，则 CPU Shared Pool 包括保留的 CPU statically exclusive CPUs 是指分配给 Guaranteed、LSE/LSR Pods 使用的一组独占 CPU。当 Guaranteed、LSE 和 LSR Pod 申请 CPU 时，Koord-scheduler 将从 CPU Shared Pool 中分配 BE CPU Shared Pool 是一组 BestEffort 和 BE 的 Pod 都可运行的 CPU 池。BE CPU Shared Pool 包含节点中除 Guaranteed 和 LSE Pod 分配的之外的所有 CPU Koordinator QoS CPU 编排原则 LSE/LSR Pod 的 requests 和 limits 必须相等，CPU 值必须是 1000 的整数倍 LSE Pod 分配的 CPU 是完全独占的，不得共享。如果节点是超线程架构，只保证逻辑核心维度是隔离的，但是可以通过 CPUBindPolicyFullPCPUs 策略获得更好的隔离 LSR Pod 分配的 CPU 只能与 BE Pod 共享 LS Pod 绑定了与 LSE/LSR Pod 独占之外的共享 CPU 池 BE Pod 绑定使用节点中除 LSE Pod 独占之外的所有 CPU 如果 Kubelet 的 CPU 管理器策略为 static 策略，则已经运行的 Guaranteed Pods 等价于 LSR 如果 Kubelet 的 CPU 管理器策略为 none 策略，则已经运行的 Guaranteed Pods 等价于 LS 新创建但未指定 Koordinator QoS 的 Guaranteed Pod 等价于 LS Kubelet CPU Manager 策略兼容原则 如果 Kubelet 设置 CPU Manager 策略选项 full-pcpus-only=true 或者 distribute-cpus-across-numa=true，并且节点中没有 Koordinator 定义的新 CPU 绑定策略，则遵循 Kubelet 定义的这些参数的定义 如果 Kubelet 设置了 Topology Manager 策略，并且节点中没有 Koordinator 定义的新的 NUMA Topology Alignment 策略，则遵循 Kubelet 定义的这些参数的定义 接管 Kubelet CPU 管理策略 Kubelet 预留的 CPU 主要服务于 BestEffort 和 Burstable Pods。但 Koordinator 不会遵守该策略。Burstable Pod 应该使用 CPU Shared Pool，而 BestEffort Pods 应该使用 BE CPU Shared Pool。LSE 和 LSR Pod 不会从被 Kubelet 预留的 CPU 中分配。 对于 Burstable 和 LS Pod 当 Koordlet 启动时，计算 CPU Shared Pool 并将共享池应用到节点中的所有 Burstable 和 LS Pod，即更新它们的 CPU cgroups, 设置 CPUset。在创建或销毁 LSE/LSR Pod 时执行相同的逻辑 Koordlet 会忽略 Kubelet 预留的 CPU，将其替换为 Koordinator 定义的 CPU Shared Pool 对于 BestEffort 和 BE Pod 如果 Kubelet 预留了 CPU，BestEffort Pod 会首先使用预留的 CPU Koordlet 可以使用节点中的所有 CPU，但不包括由具有整数 CPU 的 Guaranteed 和 LSE Pod 分配的 CPU。这意味着如果 Koordlet 启用 CPU Suppress 功能，则应遵循约束以保证不会影响 LSE Pod。同样，如果 Kubelet 启用了 CPU Manager static 策略，则也应排除 Guaranteed Pod 对于 Guaranteed Pod 如果 Pod 的 annotations 中有 Koord-scheduler 更新的 scheduling.koordinator.sh/resource-status，在 sandbox/container 创建阶段，则会替换 Kubelet CRI 请求中的 CPUset Kubelet 有时会调用 CRI 中定义的 Update 方法来更新容器 cgroup 以设置新的 CPU，因此 Koordlet 和 koord-runtime-proxy 需要 hook 该方法 自动调整 CPU Shared Pool 大小 Koordlet 会根据 Pod 创建/销毁等变化自动调整 CPU Shared Pool 的大小。如果 CPU Shared Pool 发生变化，Koordlet 应该更新所有使用共享池的 LS 或 Burstable Pod 的 cgroups 如果 Pod 的 annotations scheduling.koordinator.sh/resource-status 中指定了对应的 CPU Shared Pool，Koordlet 在配置 cgroup 时只需要绑定对应共享池的 CPU 即可 接管逻辑要求 koord-runtime-proxy 添加新的扩展点并且 Koordlet 实现新的运行时插件的 hook 。当没有安装 koord-runtime-proxy 时，这些接管逻辑也将能够实现。 CPU 绑定策略 标签 node.koordinator.sh/cpu-bind-policy 限制了调度时如何绑定 CPU： None 或空值 — 不执行任何策略 FullPCPUsOnly — 要求调度器必须分配完整的物理核。等效于 Kubelet CPU Manager 策略选项 full-pcpus-only=true SpreadByPCPUs — 要求调度器必须按照物理核维度均匀的分配 CPU NUMA 分配策略 标签 node.koordinator.sh/numa-allocate-strategy 表示在调度时如何选择满意的 NUMA 节点： MostAllocated — 表示从可用资源最少的 NUMA 节点分配 LeastAllocated — 表示从可用资源最多的 NUMA 节点分配 DistributeEvenly — 表示在 NUMA 节点上平均分配 CPU NUMA 拓扑对齐策略 标签 node.koordinator.sh/numa-topology-alignment-policy 表示如何根据 NUMA 拓扑对齐资源分配。策略语义遵循 K8s 社区。相当于 NodeResourceTopology 中的 TopologyPolicies 字段，拓扑策略 SingleNUMANodePodLevel 和 SingleNUMANodeContainerLevel 映射到 SingleNUMANode 策略： None — 是默认策略，不执行任何拓扑对齐 BestEffort — 表示优先选择拓扑对齐的 NUMA node，如果没有，则继续为 Pod 分配资源 Restricted — 表示每个 Pod 在 NUMA 节点上请求的资源是拓扑对齐的，如果不是，Koord-scheduler 会在调度时跳过该节点 SingleNUMANode — 表示一个 Pod 请求的所有资源都必须在同一个 NUMA 节点上，如果不是，Koord-scheduler 调度时会跳过该节点 NodeResourceTopology 维护 Koordinator 在社区提供的 NodeResourceTopology CRD 基础之上通过 annotations 和 label 扩展了更多的 CPU 管理策略与限制。 Koordlet 负责创建/更新 NodeResourceTopology 建议 Koordlet 通过解析 /var/lib/kubelet/cpu_manager_state 文件来获取现有 Guaranteed Pod 的 CPU 分配信息。或者通过 Kubelet 提供的 CRI 接口和 gRPC 获取这些信息 当 Koord-scheduler 分配 Pod 的 CPU 时，替换 Kubelet 状态检查点文件中的 CPU 建议 Koordlet 从 kubeletConfiguration 获取 CPU Manager 策略和选项 NRI 重构设计 Koordinator 社区有计划将 CRI proxy 的增强方案以 NRI 理念重构：https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/20230608-nri-mode-resource-management.md。 与 standalone 和 proxy 不同，Koodlet 将启动一个 NRI 插件从 CRI 运行时订阅 Pod/容器生命周期事件，然后 Koordlet NRI 插件将调用运行时 hook 来调整 Pod 资源或 OCI 规范。流程大致为： 从 CRI 运行时获取 Pod/容器生命周期事件和 OCI 格式信息 将 OCI 格式信息转换为内部协议，以重用现有的运行时 hook 插件 将运行时 hook 插件的响应转换为 OCI 规范格式 将 OCI 规范格式响应返回到 CRI 运行时 CRI Resource Managerhttps://github.com/intel/cri-resource-manager CRI Resource Manager 是 CRI 代理，位于客户端和实际容器运行时实现（Containerd、CRI-O）之间，用于转发请求和响应。代理的主要目的是通过在转发请求之前修改请求或在处理和代理期间执行与请求相关的额外操作来应用策略以将硬件感知的资源分配策略应用于系统中运行的容器。 架构概述 CRI Resource Manager 可以通过加载节点静态配置文件，也可以通过 gRPC 请求 CRI Resource Manager Node Agent 组件动态配置。 Node Agent 组件的主要功能是维护节点级别或者全局级别的 ConfigMap，以响应 CRI Resource Manager 的 gRPC 请求，返回策略配置。 默认情况下，CRI Resource Manager 无法获取 Pod spec 中指定的原始容器资源需求。它尝试使用 CRI 容器创建请求中的相关参数来预估 CPU 和内存资源。但是，无法使用这些参数来预估其他扩展资源。如果想确保 CRI Resource Manager 使用原始 Pod spec 资源需求，CRI Resource Manager Webhook 组件负责将这部分声明复制到 Pod annotations 中，用于 CRI Resource Manager 感知扩展资源。 CRI Resource Manager 提供了极为丰富的硬件拓扑感知的能力，包括但不限于 CPU、内存、blockIO、RDT、SST 等；提供了 topology-aware、static-pools、balloons、podpools 等多种策略。 CRI Resource Manager 聚焦在节点级别的拓扑资源管理，并未提供 NUMA 拓扑感知调度器。 topology-aware 策略 topology-aware 策略根据检测到的硬件拓扑自动构建池树。每个池都有一组分配为其资源的 CPU 和内存区域。工作负载的资源分配首先选择最适合工作负载资源需求的池，然后从该池中分配 CPU 和内存： CPU 和内存拓扑对齐分配，以最严格的可用对齐方式将 CPU 和内存分配给工作负载 设备的对齐分配，根据已分配设备的位置选择工作负载池 CPU 核心共享分配，将工作负载分配给池 CPU 的共享子集 CPU 核心独占分配，从共享子集中动态分割 CPU 核心并分配给工作负载 CPU 核心混合分配，将独占和共享 CPU 核心分配给工作负载 发现和使用内核隔离的 CPU 核心 ( isolcpus )，将内核隔离的 CPU 核心用于专门分配的 CPU 核心 将分配的资源暴露给工作负载 通知工作负载有关资源分配的更改 动态放缓内存对齐以防止 OOM，动态加宽工作负载内存集以避免池/工作负载 OOM 多层内存分配：将工作负载分配到其首选类型的内存区域，该策略感知三种内存：DRAM 是常规系统主存储器；PMEM 是大容量内存，例如 Intel® Optane™内存；HBM 是高速存储器，通常出现在一些专用计算系统上 冷启动，在初始预热期间将工作负载专门固定到 PMEM 动态页面降级，强制将只读和空闲容器内存页迁移到 PMEM static-pools 策略 static-pools 策略是 Intel CMK 项目的功能移植。 balloons 策略 balloons 策略是一种用于管理系统中容器 CPU 资源分配的方法。它涉及将可用的 CPU 划分为相互独立的池，称为 balloon，每个 balloon 可以根据容器的资源请求进行扩大或缩小，即可以增加或减少其中的 CPU 数量。 balloon 可以是静态的或动态的。静态 balloon 需要手动创建并保持固定的大小，而动态 balloon 则可以根据容器的资源需求自动创建和销毁。这可以实现更高效的资源利用，因为 balloon 可以实时调整以满足不断变化的需求。 除了控制每个 balloon 中 CPU 数量外，balloon 还可以配置特定的设置，例如 CPU 核心和非核心的最小和最大频率。这可以对 CPU 资源的分配进行精细控制，确保每个容器都分配了其运行所需的资源。 大致流程为： 用户可以配置不同类型的 balloon，策略可以根据这些配置实例化 balloon balloon 有一组 CPU 和一组在 CPU 上运行的容器 每个容器都被分配给一个 balloon。容器可以使用其 balloon 的所有 CPU，而不能使用其他 CPU 每个逻辑 CPU 最多属于一个 balloon，也可能存在不属于任何 balloon 的 CPU balloon 中的 CPU 数量在 balloon 的生命周期内可能会发生变化。如果 balloon 膨胀，也就是增加了 CPU，那么 balloon 中的所有容器都可以使用更多的 CPU，反之亦然 当在 Kubernetes 节点上创建新容器时，策略首先决定将运行该容器的 balloon 的类型。该决定基于 Pod annotations，或者如果未给出 annotations 则基于命名空间 接下来，策略决定哪个 balloon 将运行容器。选项有： 现有的 balloon 已经有足够的 CPU 来运行当前和新的容器 现有的 balloon 可以扩大以适应其当前和新的容器 新 balloon 当向 balloon 添加或从其中移除 CPU 时，会根据 balloon 的 CPU 类属性或空闲 CPU 类属性重新配置 CPU podpools 策略 podpools 策略实现 Pod 级别的工作负载放置。它将 Pod 的所有容器分配到同一个 CPU/内存池。池中的 CPU 数量可由用户配置。 容器亲和与反亲和 亲和与反亲和的提示是通过 Pod annotations 声明： 同一 NUMA 节点内的 CPU 视为彼此亲和 同一 socket 中不同 NUMA 节点内的 CPU，以及不同 socket 内的 CPU 视为彼此反亲和 blockIO blockIO 提供以下控制： 块设备 IO 调度优先级（权重） 限制 IO 带宽 限制 IO 操作的数量 CRI Resource Manager 通过 cgroups blockIO 控制器将 blockIO 的相关参数应用于 Pod。 Volcanohttps://github.com/volcano-sh/volcano Volcano 是 CNCF 下首个也是唯一的基于 Kubernetes 的容器批量计算平台，主要用于高性能计算场景。它提供了 Kubernetes 目前缺少的一套机制，这些机制通常是机器学习大数据应用、科学计算、特效渲染等多种高性能工作负载所需的。作为一个通用批处理平台，Volcano 与几乎所有的主流计算框架无缝对接，如Spark、TensorFlow 、PyTorch、 Flink 、Argo 、MindSpore 、 PaddlePaddle 等。它还提供了包括基于各种主流架构的 CPU、GPU 在内的异构设备混合调度能力。Volcano 的设计理念建立在 15 年来多种系统和平台大规模运行各种高性能工作负载的使用经验之上，并结合来自开源社区的最佳思想和实践。 感知调度流程 policy action none 无 best-effort 过滤出拓扑策略为 best-effort 的节点 restricted 过滤出拓扑策略为 restricted 且满足 CPU 拓扑要求的节点 single-numa-node 过滤出拓扑策略为 single-numa-node 且满足 CPU 拓扑要求的节点 Volcano 在的感知调度和其他项目类似，将 Kubernetes Topology Manager 的原生策略扩展至调度器层面，只不过 CRD 采用的是 Volcano 设计的 Numatopology，而非社区提出的 NodeResourceTopology CRD，其他流程方面大同小异。 节点 CPU 编排 Volcano 并未提供节点 CPU 编排的能力，但是参考华为 CCE 产品文档中，CCE 基于社区原生的 CPU Manager 策略的基础上，提出了 enhanced-static 策略，是在兼容 static 策略的基础上，新增一种符合某些资源特征的 Burstable Pod（CPU 的 requests 和 limits 值都是正整数）优先使用某些 CPU 的能力，以减少应用在多个 CPU 间频繁切换带来的影响。 该特性是基于 Huawei Cloud EulerOS 2.0 内核中优化了 CPU 调度能力实现的。在 Pod 容器优先使用的 CPU 利用率超过 85% 时，会自动分配到其他利用率较低的 CPU 上，进而保障了应用的响应能力。 开启 enhanced-static 策略时，应用性能优于 none 策略，但弱于 static 策略 应用分配的优先使用的 CPU 并不会被独占，仍处于共享的 CPU 池中。因此在该 Pod 处于业务波谷时，节点上其他 Pod 可使用该部分 CPU 资源 实践验证以 cri-resource-manager为例 based on v0.8.3 服务安装 12345# 安装 cri-resource-manager 服务$ yum -y install https://github.com/intel/cri-resource-manager/releases/download/v0.8.3/cri-resource-manager-0.8.3-0.centos-7.x86_64.rpm# 安装 cri-resmgr-agent 服务（需要手动编译并替换 IMAGE_PLACEHOLDER 占位符，这里不做详述）$ kubectl apply -f https://raw.githubusercontent.com/intel/cri-resource-manager/master/cmd/cri-resmgr-agent/agent-deployment.yaml 安装结果 12345678910111213141516171819202122232425262728293031323334$ systemctl start cri-resource-manager$ systemctl status cri-resource-manager● cri-resource-manager.service - A CRI proxy with (hardware) resource aware container placement policies. Loaded: loaded (/usr/lib/systemd/system/cri-resource-manager.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2023-06-28 16:26:04 CST; 29min ago Docs: https://github.com/intel/cri-resource-manager Main PID: 32130 (cri-resmgr) Tasks: 49 Memory: 41.6M CGroup: /system.slice/cri-resource-manager.service └─32130 /usr/bin/cri-resmgr --fallback-config /etc/cri-resmgr/fallback.cfg $ kubectl get ds -ANAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEkube-system cri-resmgr-agent 1 1 1 1 1 &lt;none&gt; 11m# 采用 cri-resmgr-agent 维护的动态配置，采用 topology-aware 策略# 节点全量的 CPU 为 0-47# - 0-4 用于非 Kubernetes 平台使用，如节点系统服务等# - AvailableResources 中的 5-47 号 CPU 用于 Kubernetes 平台使用# - ReservedResources 5-10 号 CPU 用于 Kubernetes 的预留命名空间下的服务使用# - 剩余的 10-47 号 CPU 用于 Kubernetes 的其他命名空间下的服务使用$ kubectl get cm -n kube-system cri-resmgr-config.node.node1 -o yamlapiVersion: v1data: policy: | Active: topology-aware topology-aware: ReservedPoolNamespaces: [kube-system,arsdn,secboat] ReservedResources: cpu: cpuset:5-10 AvailableResources: cpu: cpuset:5-47kind: ConfigMap 服务配置 12345678910111213141516171819202122# 配置 Kubelet 的 CRI endpoint 为 cri-resmgr.sock$ cat /var/lib/kubelet/kubeadm-flags.envKUBELET_KUBEADM_ARGS=&quot;--container-runtime=remote --container-runtime-endpoint=unix:///var/run/cri-resmgr/cri-resmgr.sock&quot;$ cat /etc/kubernetes/kubelet.env...KUBELET_ARGS=&quot;--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \\--config=/etc/kubernetes/kubelet-config.yaml \\--kubeconfig=/etc/kubernetes/kubelet.conf \\--log-dir=/var/log/kubelet \\--log-file=/var/log/kubelet/kubelet.log \\--logtostderr=false \\--alsologtostderr=false \\--feature-gates=CSIInlineVolume=true,CSIVolumeHealth=true,CPUManagerPolicyOptions=true \\--pod-infra-container-image=harbor.archeros.cn:443/library/ake/pause:3.5-amd64 \\--container-runtime=remote \\--runtime-request-timeout=15m \\--container-runtime-endpoint=unix:///var/run/cri-resmgr/cri-resmgr.sock \\--runtime-cgroups=/systemd/system.slice \\...$ systemctl daemon-reload &amp;&amp; systemctl restart kubelet 节点 CPU 编排 123456789101112$ numactl -Havailable: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 24 25 26 27 28 29 30 31 32 33 34 35node 0 size: 65413 MBnode 0 free: 15969 MBnode 1 cpus: 12 13 14 15 16 17 18 19 20 21 22 23 36 37 38 39 40 41 42 43 44 45 46 47node 1 size: 65536 MBnode 1 free: 21933 MBnode distances:node 0 1 0: 10 21 1: 21 10 12345678910111213141516171819202122232425262728# 部署共享 CPU 的 Pod$ kubectl apply -f besteffort.yaml &amp;&amp; kubectl apply -f busterable.yaml &amp;&amp; kubectl apply -f guaranteed.yaml# 查看 CPU 分配情况：共享一个合适的 NUMA node$ crictl ps | grep besteffort | awk '{print $1}' | xargs crictl inspect | grep &quot;\\&quot;cpus\\&quot;:&quot; &quot;cpus&quot;: &quot;12-23,36-47&quot;,$ crictl ps | grep busterable | awk '{print $1}' | xargs crictl inspect | grep &quot;\\&quot;cpus\\&quot;:&quot; &quot;cpus&quot;: &quot;12-23,36-47&quot;,$ crictl ps | grep guaranteed | awk '{print $1}' | xargs crictl inspect | grep &quot;\\&quot;cpus\\&quot;:&quot; &quot;cpus&quot;: &quot;12-23,36-47&quot;, # 部署独占 CPU 的 Pod$ kubectl apply -f guaranteed-exclusive.yaml# 查看 CPU 分配情况：独占同一物理核心的两个逻辑核心$ crictl ps | grep guaranteed-exclusive | awk '{print $1}' | xargs crictl inspect | grep &quot;\\&quot;cpus\\&quot;:&quot; &quot;cpus&quot;: &quot;23,47&quot;,# 查看热更新，共享 CPU 中将独占的 CPU 扣除$ crictl ps | grep besteffort | awk '{print $1}' | xargs crictl inspect | grep &quot;\\&quot;cpus\\&quot;:&quot; &quot;cpus&quot;: &quot;12-22,36-46&quot;,$ crictl ps | grep busterable | awk '{print $1}' | xargs crictl inspect | grep &quot;\\&quot;cpus\\&quot;:&quot; &quot;cpus&quot;: &quot;12-22,36-46&quot;,$ crictl ps | grep guaranteed | awk '{print $1}' | xargs crictl inspect | grep &quot;\\&quot;cpus\\&quot;:&quot; &quot;cpus&quot;: &quot;12-22,36-46&quot;,# 预留 namespace CPU 分配$ kubectl apply -f reserved.yaml$ crictl ps | grep reserved | awk '{print $1}' | xargs crictl inspect | grep &quot;\\&quot;cpus\\&quot;:&quot; &quot;cpus&quot;: &quot;5-10&quot;,","link":"/2023/06/25/2023-06-25%20Kubernetes%20CPU%20%E7%B2%BE%E7%BB%86%E5%8C%96%E7%AE%A1%E7%90%86/"},{"title":"「 Kubernetes 」InPlacePodVerticalScaling 特性","text":"based on v1.27.3 Kubernetes v1.27 版本中，添加了一个 alpha 版本的 feature gates — InPlacePodVerticalScaling，该特性允许用户在不重启容器的情况下调整分配给 Pod 的 CPU 或内存资源的大小。为了实现这一点，现在允许通过 patch 修改正在运行的 Pod resources 中 CPU 和内存资源。这也意味着 Pod spec 中 resources 字段不能再作为 Pod 实际资源的指标，监控工具类服务现在必须查看 Pod status 中的新字段。 KEP-1287 提案旨在改进 CRI，用于管理运行时容器的 CPU 和内存资源配置。扩展 UpdateContainerResources CRI，使其适用于 Windows 以及除 Linux 之外的其他未来运行时。此外，扩展 CRI 的 ContainerStatus API，允许 Kubelet 发现容器上配置的当前资源。 初衷由于多种原因，分配给 Pod 容器的资源可能需要动态更改： Pod 负载大幅增加，当前资源不足 Pod 负载显着下降，分配的资源未使用 Pod 资源设置不正确 目前，由于 Pod 容器资源是不可变的，更改资源分配需要重新创建 Pod。虽然许多无状态工作负载可以接受此类中断，但有些工作负载更为敏感，尤其是在使用少量 Pod 副本时。此外，对于有状态或批量工作负载，Pod 重启会造成严重中断，导致可用性降低或运行成本升高。 因此，需要一种不重新创建 Pod 或重新启动容器的情况下更改资源的方案，即 InPlacePodVerticalScaling 特性，该特性依赖于 CRI 接口来更新 Pod 容器的 CPU 和内存的 requests 与 limits。 当前的 CRI 接口有一些需要解决的缺点： UpdateContainerResources API 接受的更改 Linux 容器资源的参数无法适用于 Windows 容器或未来可能出现的其他非 Linux 运行时 CRI 中缺少 Kubelet 查询并发现容器运行时中配置的 CPU 和内存限制的机制 处理 UpdateContainerResources API 的预期行为并没有非常明确定义或记录 目标 允许更改容器资源 requests 和 limits，而无需重新启动容器 允许用户、VPA、StatefulSet、JobController 等角色决定在 Pod 无法就地调整资源大小时如何继续 允许用户指定哪些容器可以在不重新启动的情况下调整大小 此外，该提案为 CRI 设定了两个目标： 修改 UpdateContainerResources API，使其适用于 Windows 容器以及除 Linux 之外的其他运行时管理的容器 CRI 提供查询容器运行时机制，用于获取当前应用于容器的 CPU 和内存资源配置 该提案的另一个目标是更好地定义和记录处理资源更新时容器运行时的预期行为。 非目标提案明确非目标是避免介入未能就地资源调整大小的 Pod 的整个生命周期。这应该由发起调整大小的参与者来处理。其他确定的非目标是： 允许在不重新启动的情况下更改 Pod QoS 等级 无需重新启动即可更改 Init 容器的资源 驱逐优先级较低的 Pod 以方便调整 Pod 大小 更新扩展资源或除 CPU、内存之外的任何其他资源类型 支持除 None 策略之外的 CPU/内存管理器策略 该提案的目标并非是定义实现这些功能的详细或具体方式，实现细节留给运行时来确定，在预期行为的限制范围内即可。 API 变更API 的核心思想是让 Pod spec 中的容器资源 requests 和 limits 可变，对 Pod status 进行了扩展，以显示为 Pod 及其容器分配和应用的资源。 Pod spec 中 resources 变成纯粹的声明，表示 Pod 资源的所需状态 Pod status 中 containerStatuses 的 allocatedResources 字段反映了分配给 Pod 容器的资源 Pod status 中 containerStatuses 的 resources 字段反映了如同容器运行时所报告的、针对正运行的容器配置的实际资源 requests 和 limits Pod status 中 resize 字段解释容器上给定资源发生的情况 新增的 allocatedResources 字段代表正在进行的调整大小操作。在考虑节点上可用资源空间时，Kube-scheduler 应使用 Pod spec 中的容器资源 requests 和 allocatedResources 中较大的值作为标准。 容器调整策略resizePolicy 调整策略允许更精细地控制 Pod 中的容器如何针对 CPU 和内存资源进行调整。针对调整 CPU 和内存可以设置以下重启策略： NotRequired：默认值，如果可能的话，在不重新启动的情况下调整容器的大小 RestartContainer：重启容器并在重启后应用新资源 NotRequired 调整大小的重新启动策略并不能保证容器不会重新启动。如果容器无法在不重新启动的情况下应用新资源，则运行时可能会选择停止容器；此外，容器的应用程序可以处理 CPU 资源的调整而不必重启， 但是调整内存可能需要应用程序重启，因此容器也必须重启。 如果同时更新具有不同策略的多种资源类型，则 RestartContainer 策略优先于 NotRequired 策略。 如果 Pod 的 restartPolicy 为 Never，则 Pod 中所有容器的调整重启策略必须被设置为 NotRequired，也就是说，如果无法就地调整大小，则任何就地调整大小的动作都可能导致容器停止，且无法重新启动。 调整状态大小Pod status 中新增一个 resize 字段 ，用于表明 Kubelet 是否已接受或拒绝针对给定资源的建议调整大小操作。当 Pod spec 和 status 中 resources 不同时给出具体的原因： Proposed：表示请求调整已被确认，并且请求已被验证和记录 InProgress：表示节点已接受调整请求，并正在将其应用于 Pod 的容器 Deferred：意味着在此时无法批准请求的调整，节点将继续重试。当其他 Pod 退出并释放节点资源时，调整可能会被真正实施 Infeasible：表示节点无法承接所请求的调整值。 如果所请求的调整超过节点可分配给 Pod 的最大资源，则可能会发生这种情况 每当 Kube-apiserver 收到调整资源的请求时，它都会自动将该字段设为 Proposed。 CRI 变化Kubelet 会调用 UpdateContainerResources API，该 API 目前采用 LinuxContainerResources 参数，但不适用于 Windows。因此，此参数更改为 ContainerResources，该参数与平台无关，并将包含特定于平台的信息，通过使 API 中传递的资源参数特定于目标运行时，使 UpdateContainerResources API 适用于 Windows 以及除 Linux 之外的任何其他未来运行时。 此外，ContainerStatus API 新增 ContainerResources 信息，以便允许 Kubelet 从运行时查询容器的 CPU 和内存限制配置，需要运行时返回当前应用于容器的 CPU 和内存资源值。 为了实现上述理念，涉及到如下的改动： 新的 protobuf 消息对象 ContainerResources 封装了 LinuxContainerResources 和 WindowsContainerResources。后续只需追加新运行时的资源结构，即可轻松扩展并适应未来的运行时 1234567// ContainerResources holds resource configuration for a container.message ContainerResources { // Resource configuration specific to Linux container. LinuxContainerResources linux = 1; // Resource configuration specific to Windows container. WindowsContainerResources windows = 2;} ContainerStatus 消息对象新增 ContainerResources 字段，用于 Kubelet 使用 ContainerStatus API 查询运行时并发现当前应用于容器的资源 1234567@@ -914,6 +912,8 @@ message ContainerStatus { repeated Mount mounts = 14; // Log path of container. string log_path = 15;+ // Resource configuration of the container.+ ContainerResources resources = 16; } UpdateContainerResources API 采用 ContainerResources 参数而不是 LinuxContainerResources 1234567891011121314--- a/staging/src/k8s.io/cri-api/pkg/apis/services.go+++ b/staging/src/k8s.io/cri-api/pkg/apis/services.go@@ -43,8 +43,10 @@ type ContainerManager interface { ListContainers(filter *runtimeapi.ContainerFilter) ([]*runtimeapi.Container, error) // ContainerStatus returns the status of the container. ContainerStatus(containerID string) (*runtimeapi.ContainerStatus, error)- // UpdateContainerResources updates the cgroup resources for the container.- UpdateContainerResources(containerID string, resources *runtimeapi.LinuxContainerResources) error+ // UpdateContainerResources updates ContainerConfig of the container synchronously.+ // If runtime fails to transactionally update the requested resources, an error is returned.+ UpdateContainerResources(containerID string, resources *runtimeapi.ContainerResources) error // ExecSync executes a command in the container, and returns the stdout output. // If command exits with a non-zero exit code, an error is returned. ExecSync(containerID string, cmd []string, timeout time.Duration) (stdout []byte, stderr []byte, err error) Kubelet 代码对此也做了相应更改 设计细节Kubelet 与 Kube-apiserver 交互对于新创建的 Pod，Kube-apiserver 将设置 allocatedResources 字段以匹配每个容器的资源请求量。当 Kubelet 接纳 Pod 时，allocatedResources 中的值用于确定是否有足够的空间接纳该 Pod，Kubelet 在接纳 Pod 时不会设置 allocatedResources。 当请求调整 Pod 大小时，Kubelet 会尝试更新分配给 Pod 及其容器的资源。Kubelet 首先通过计算节点中所有 Pod（正在调整大小的 Pod 除外）分配的资源总和（即 allocatedResources）来检查新的所需资源是否适合节点可分配资源。对于调整大小的 Pod，它将新的所需资源（即 Spec.Containers[i].Resources.Requests）添加到总和中。 如果新的所需资源适合，Kubelet 通过更新 allocatedResources 字段并将 Status.Resize 设置为 InProgress 来接受调整大小。然后，Kubelet 调用 UpdateContainerResources API 来更新容器资源限制。成功更新所有容器后，它会更新 Pod status 中的 resources 字段，以反映新的资源值并取消设置 resize 字段 如果新的所需资源不适合，Kubelet 会将 resize 字段更新为 Infeasible，并且不会对调整大小进行操作 如果新的所需资源适合但目前正在使用，Kubelet 会将 resize 字段更新为 Deferred 除了上述内容之外，每当接受或拒绝调整大小时，以及如果可能的话，在调整大小过程中的关键步骤上，Kubelet 都会在 Pod 上生成事件。 如果多个 Pod 需要调整大小，则会按照 Kubelet 定义的顺序（例如，按到达顺序）处理它们；Kube-scheduler 可以并行地将新的 Pod 分配给节点，如果发生竞争情况，也就是 Pod 调整大小后节点没有空间，Kubelet 将通过拒绝新 Pod 来解决该问题。 Kubelet 重启容忍度 如果 Kubelet 在处理 Pod 大小调整过程中发生重启，则在重新启动时，所有 Pod 都会以其当前的 allocatedResources 值被接纳，并在添加所有现有 Pod 后处理调整大小。这可确保调整大小不会影响之前的 Pod。 Kube-scheduler 和 Kube-apiserver 交互Kube-scheduler 使用 Pod spec 中 resources 的资源 request 来调度新的 Pod，并继续 watch Pod 更新并更新其缓存。为了计算分配给 Pod 的节点资源，它必须考虑待处理的调整大小，如 resize 所述： 对于 resize 为 InProgress 或 Infeasible 的容器，可以简单地使用 allocatedResources 对于 resize 为 Proposed 的容器，假设调整大小被接受。因此，必须使用 Pod spec 中 resources 的资源 request 和 allocatedResources 值中较大的那个 工作流123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111T=0: A new pod is created - `spec.containers[0].resources.requests[cpu]` = 1 - all status is unsetT=1: apiserver defaults are applied - `spec.containers[0].resources.requests[cpu]` = 1 - `status.containerStatuses[0].allocatedResources[cpu]` = 1 - `status.resize[cpu]` = unsetT=2: kubelet runs the pod and updates the API - `spec.containers[0].resources.requests[cpu]` = 1 - `status.containerStatuses[0].allocatedResources[cpu]` = 1 - `status.resize[cpu]` = unset - `status.containerStatuses[0].resources.requests[cpu]` = 1T=3: Resize #1: cpu = 1.5 (via PUT or PATCH or /resize) - apiserver validates the request (e.g. `limits` are not below `requests`, ResourceQuota not exceeded, etc) and accepts the operation - apiserver sets `resize[cpu]` to &quot;Proposed&quot; - `spec.containers[0].resources.requests[cpu]` = 1.5 - `status.containerStatuses[0].allocatedResources[cpu]` = 1 - `status.resize[cpu]` = &quot;Proposed&quot; - `status.containerStatuses[0].resources.requests[cpu]` = 1T=4: Kubelet watching the pod sees resize #1 and accepts it - kubelet sends patch { `resourceVersion` = `&lt;previous value&gt;` # enable conflict detection `status.containerStatuses[0].allocatedResources[cpu]` = 1.5 `status.resize[cpu]` = &quot;InProgress&quot;' } - `spec.containers[0].resources.requests[cpu]` = 1.5 - `status.containerStatuses[0].allocatedResources[cpu]` = 1.5 - `status.resize[cpu]` = &quot;InProgress&quot; - `status.containerStatuses[0].resources.requests[cpu]` = 1T=5: Resize #2: cpu = 2 - apiserver validates the request and accepts the operation - apiserver sets `resize[cpu]` to &quot;Proposed&quot; - `spec.containers[0].resources.requests[cpu]` = 2 - `status.containerStatuses[0].allocatedResources[cpu]` = 1.5 - `status.resize[cpu]` = &quot;Proposed&quot; - `status.containerStatuses[0].resources.requests[cpu]` = 1T=6: Container runtime applied cpu=1.5 - kubelet sends patch { `resourceVersion` = `&lt;previous value&gt;` # enable conflict detection `status.containerStatuses[0].resources.requests[cpu]` = 1.5 `status.resize[cpu]` = unset } - apiserver fails the operation with a &quot;conflict&quot; errorT=7: kubelet refreshes and sees resize #2 (cpu = 2) - kubelet decides this is possible, but not right now - kubelet sends patch { `resourceVersion` = `&lt;updated value&gt;` # enable conflict detection `status.containerStatuses[0].resources.requests[cpu]` = 1.5 `status.resize[cpu]` = &quot;Deferred&quot; } - `spec.containers[0].resources.requests[cpu]` = 2 - `status.containerStatuses[0].allocatedResources[cpu]` = 1.5 - `status.resize[cpu]` = &quot;Deferred&quot; - `status.containerStatuses[0].resources.requests[cpu]` = 1.5T=8: Resize #3: cpu = 1.6 - apiserver validates the request and accepts the operation - apiserver sets `resize[cpu]` to &quot;Proposed&quot; - `spec.containers[0].resources.requests[cpu]` = 1.6 - `status.containerStatuses[0].allocatedResources[cpu]` = 1.5 - `status.resize[cpu]` = &quot;Proposed&quot; - `status.containerStatuses[0].resources.requests[cpu]` = 1.5T=9: Kubelet watching the pod sees resize #3 and accepts it - kubelet sends patch { `resourceVersion` = `&lt;previous value&gt;` # enable conflict detection `status.containerStatuses[0].allocatedResources[cpu]` = 1.6 `status.resize[cpu]` = &quot;InProgress&quot;' } - `spec.containers[0].resources.requests[cpu]` = 1.6 - `status.containerStatuses[0].allocatedResources[cpu]` = 1.6 - `status.resize[cpu]` = &quot;InProgress&quot; - `status.containerStatuses[0].resources.requests[cpu]` = 1.5T=10: Container runtime applied cpu=1.6 - kubelet sends patch { `resourceVersion` = `&lt;previous value&gt;` # enable conflict detection `status.containerStatuses[0].resources.requests[cpu]` = 1.6 `status.resize[cpu]` = unset } - `spec.containers[0].resources.requests[cpu]` = 1.6 - `status.containerStatuses[0].allocatedResources[cpu]` = 1.6 - `status.resize[cpu]` = unset - `status.containerStatuses[0].resources.requests[cpu]` = 1.6T=11: Resize #4: cpu = 100 - apiserver validates the request and accepts the operation - apiserver sets `resize[cpu]` to &quot;Proposed&quot; - `spec.containers[0].resources.requests[cpu]` = 100 - `status.containerStatuses[0].allocatedResources[cpu]` = 1.6 - `status.resize[cpu]` = &quot;Proposed&quot; - `status.containerStatuses[0].resources.requests[cpu]` = 1.6T=12: Kubelet watching the pod sees resize #4 - this node does not have 100 CPUs, so kubelet cannot accept - kubelet sends patch { `resourceVersion` = `&lt;previous value&gt;` # enable conflict detection `status.resize[cpu]` = &quot;Infeasible&quot;' } - `spec.containers[0].resources.requests[cpu]` = 100 - `status.containerStatuses[0].allocatedResources[cpu]` = 1.6 - `status.resize[cpu]` = &quot;Infeasible&quot; - `status.containerStatuses[0].resources.requests[cpu]` = 1.6 CRI 工作流 下图概述了 Kubelet 使用 UpdateContainerResources 和 ContainerStatus CRI API 设置新的容器资源限制，并更新 Pod status 以响应用户更改 Pod spec 中所需资源的情况。 123456789101112131415161718192021222324252627+-----------+ +-----------+ +-----------+| | | | | || apiserver | | kubelet | | runtime || | | | | |+-----+-----+ +-----+-----+ +-----+-----+ | | | | watch (pod update) | | |------------------------------&gt;| | | [Containers.Resources] | | | | | | (admit) | | | | | | UpdateContainerResources() | | |-----------------------------&gt;| | | (set limits) | |&lt;- - - - - - - - - - - - - - -| | | | | | ContainerStatus() | | |-----------------------------&gt;| | | | | | [ContainerResources] | | |&lt;- - - - - - - - - - - - - - -| | | | | update (pod status) | | |&lt;------------------------------| | | [ContainerStatuses.Resources] | | | | | Kubelet 在 ContainerManager 接口中调用 UpdateContainerResources() CRI API，通过在 API 的 ContainerResources 参数中指定这些值来为容器配置新的 CPU 和内存限制。 Kubelet 在调用此 CRI API 时设置特定于目标运行时平台的 ContainerResources 参数 Kubelet 在 ContainerManager 接口中调用 ContainerStatus() CRI API 来获取应用于 Container 的 CPU 和内存限制。它使用 ContainerStatus.Resources 返回的值来更新 Pod 状态中该容器的 ContainerStatuses[i].Resources.Limits 注意事项 如果节点 CPU Manager 策略为 static，则只允许整数值的 CPU 调整大小。如果请求非整数 CPU 调整大小，则将被拒绝，并在事件流中记录错误消息 所有组件在计算 Pod 使用的资源时都将使用 allocatedResources 如果在调整 Pod 大小时收到其他调整大小请求，这些请求将在当前完成后处理，并且调整大小会朝着最新的期望状态进行 如果应用正在占用内存页，降低内存限制可能并不能很快生效。 Kubelet 将使用控制循环来设置接近使用的内存限制，以强制回收，并仅在限制达到所需值时更新 Pod status 中的 resources Pod Overhead 的影响：Kubelet 将 Pod Overhead 添加到调整大小请求中，以确定是否可以就地调整大小 目前，VPA 不应该与 CPU、内存上的 HPA 一起使用。此 KEP 不会改变该限制 受影响的组件 Pod v1 core API Admission Controllers：LimitRanger 和 ResourceQuota Kubelet Kube-scheduler 其他使用相关语义的 Kubernetes 组件 实践验证InPlacePodVerticalScaling 特性需要开启相应的 feature gates：不开启时，仍然视为 Pod spec 中容器资源 requests 和 limits 不可变更。 123456789101112# Kube-apiserver 服务其中参数中新增 --feature-gates=InPlacePodVerticalScaling=true$ cat /etc/kubernetes/manifests/kube-apiserver.yaml - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key - --feature-gates=InPlacePodVerticalScaling=true # Kubelet 参数中新增 featureGates$ cat /var/lib/kubelet/config.yamlsyncFrequency: 0svolumeStatsAggPeriod: 0sfeatureGates: InPlacePodVerticalScaling: true 测试服务如下： 123456789101112131415161718192021apiVersion: v1kind: Podmetadata: name: demospec: containers: - name: demo image: ubuntu:18.04 command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;] resizePolicy: - resourceName: cpu restartPolicy: NotRequired - resourceName: memory restartPolicy: RestartContainer resources: limits: memory: &quot;200Mi&quot; cpu: &quot;1000m&quot; requests: memory: &quot;200Mi&quot; cpu: &quot;1000m&quot; Pod 运行后此时的状态信息为： 1234567891011121314151617181920212223242526spec: containers: - resizePolicy: - resourceName: cpu restartPolicy: NotRequired - resourceName: memory restartPolicy: RestartContainer resources: limits: cpu: 1 memory: 200Mi requests: cpu: 1 memory: 200Mistatus: containerStatuses: - allocatedResources: cpu: 1000m memory: 200Mi resources: limits: cpu: 1 memory: 200Mi requests: cpu: 1 memory: 200Mi 此时，将 Pod CPU 由 1C 调整为 2C： 1$ kubectl patch pod demo --patch '{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;demo&quot;, &quot;resources&quot;:{&quot;requests&quot;:{&quot;cpu&quot;:&quot;2000m&quot;}, &quot;limits&quot;:{&quot;cpu&quot;:&quot;2000m&quot;}}}]}}' 可以看到，Pod resize 处于 InProgress 状态，allocatedResources 已经调整为预期规格，status 的 resources 暂未变化。在调整结束后，status 的 resources 会跟进更新，resize 字段重置为空。 123456789101112131415161718192021222324252627spec: containers: - resizePolicy: - resourceName: cpu restartPolicy: NotRequired - resourceName: memory restartPolicy: RestartContainer resources: limits: cpu: 2 memory: 200Mi requests: cpu: 2 memory: 200Mistatus: containerStatuses: - allocatedResources: cpu: &quot;2&quot; memory: 200Mi resources: limits: cpu: &quot;1&quot; memory: 200Mi requests: cpu: &quot;1&quot; memory: 200Mi resize: InProgress 调整之后，Pod cgroup 信息也跟着发生变化： 12345$ cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-podab959cd5_f9e3_4b34_8051_861f7caca04c.slice/cpu.cfs_period_us100000$ cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-podab959cd5_f9e3_4b34_8051_861f7caca04c.slice/cpu.cfs_quota_us200000 InPlacePodVerticalScaling 特性不允许修改 Pod QoS： 12$ kubectl patch pod demo --patch '{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;demo&quot;, &quot;resources&quot;:{&quot;requests&quot;:{&quot;cpu&quot;:&quot;2000m&quot;}, &quot;limits&quot;:{&quot;cpu&quot;:&quot;3000m&quot;}}}]}}'The Pod &quot;demo&quot; is invalid: metadata: Invalid value: &quot;Burstable&quot;: Pod QoS is immutable 当修改请求中，资源无法满足时，除 Pod spec 的 resources 变化外，allocatedResources、status 的 resources 以及 cgroup 等信息均未变化，resize 状态为 Infeasible，服务仍在运行，当集群资源满足时会自动调整。 当修改的资源 restartPolicy 为 RestartContainer 时，会触发一次重启操作： 123$ kubectl get pod NAME READY STATUS RESTARTS AGEdemo 1/1 Running 1 (11s ago) 9m8s 当请求缩小内存资源时，通过容器内部 free 看到的内存仍未变化，但是 cgroup 中已经更新： 12345678$ kubectl exec -it demo free kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. total used free shared buff/cache availableMem: 12057632 2150040 6937804 167128 2969788 9441392Swap: 0 0 0$ cat /sys/fs/cgroup/memory/kubepods.slice/kubepods-poddf0fdfe6_59d6_4ffb_9a8d_0c014c182cd0.slice/memory.limit_in_bytes524288000 troubleshooting 当 Kubelet CPU Manager 策略为 static 时，调整 CPU 之后，发现并未生效，并且设置非整数的 CPU 时也未报错 1234567891011121314151617$ cat /sys/fs/cgroup/cpuset/kubepods-pod243ca361_5bde_4b8d_b5f6_522961c3ae11.slice:cri-containerd:9bfaa6d8f87fdcbe22851c1fdcbcb662e0fbf025e1299f2cf4507f09da95de4b/cpuset.cpus3$ kubectl patch pod demo --patch '{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;demo&quot;, &quot;resources&quot;:{&quot;requests&quot;:{&quot;cpu&quot;:&quot;3000m&quot;}, &quot;limits&quot;:{&quot;cpu&quot;:&quot;3000m&quot;}}}]}}'# 调整 CPU 后，CPUset 未发生变化，但是 CPU 限制配额却更新了$ cat /sys/fs/cgroup/cpuset/kubepods-pod243ca361_5bde_4b8d_b5f6_522961c3ae11.slice:cri-containerd:9bfaa6d8f87fdcbe22851c1fdcbcb662e0fbf025e1299f2cf4507f09da95de4b/cpuset.cpus3$ cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-pod243ca361_5bde_4b8d_b5f6_522961c3ae11.slice/cpu.cfs_quota_us300000$ cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-pod243ca361_5bde_4b8d_b5f6_522961c3ae11.slice/cpu.cfs_period_us100000# Pod 可用 CPU 也仍然为 1，通过 stress 模拟也是一样$ kubectl exec -it demo nprockubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.1","link":"/2023/07/10/2023-07-10%20Kubernetes%20InPlacePodVerticalScaling%20%E7%89%B9%E6%80%A7/"},{"title":"「 Kata Containers 」源码走读 — virtcontainers&#x2F;agent","text":"based on 3.0.0 agentsrc/runtime/virtcontainers/agent.go 从代码结构来看，agent 是一个典型的 CS 架构，客户端部分位于 virtcontainers 库中，与位于 guest VM 中运行的服务端进程进行 RPC 通信（实际为 ttrpc），管理 VM 中容器进程的生命周期。 目前 agent 的实现方式仅有一种：kataAgent。 123456789101112131415161718192021222324252627type kataAgent struct { ctx context.Context // 包含两种类型：VSock 和 HybridVSock // VSock：vsock://&lt;contextID&gt;:1024，QEMU 支持 // HybridVSock：hvsock://&lt;udsPath&gt;:1024，CloudHypervisor 和 Firecracker 支持 vmSocket interface{} client *kataclient.AgentClient // lock protects the client pointer sync.Mutex state KataAgentState reqHandlers map[string]reqFunc // [agent].kernel_modules kmodules []string // [agent].dial_timeout dialTimout uint32 // 固定为 true，如果为 false，代表不是长连接，需要调用 disconnect 手动断开连接 keepConn bool dead bool} agent 中声明的 longLiveConn、getAgentURL、setAgentURL、reuseAgent 均为参数获取与赋值，无复杂逻辑，不作详述。 init初始化 agent 服务 source code disableVMShutdown 是否为 true 取决于是否启用了 [agent].enable_tracing当 disableVMShutdown 为 true，也就意味着在关停 VM 时，会向 QMP 服务发送 quit 命令，请求关闭 VM 实例；否则，直接 syscall kill 掉 QEMU 进程，不会等待 VM 关闭，因此在启用 [agent].enable_tracing 时，VM 的关停时间会有所增加 初始化 agent 实现，返回 disableVMShutdown，后续决定在调用 Hypervisor 的 Stop 操作时的 VM 关闭方式 capabilities获取 agent 支持的特性 source code 设置并返回 agent 默认支持特性，包括块设备特性支持 check检查 agent 是否存活 source code 请求 agent server 的 grpc.Health 接口的 Check 方法，检测 agent server 的存活性 disconnect断开与 agent 的连接 source code 关闭 agent 客户端，重置 gRPC 路由映射表 createSandboxsandbox 运行前的准备工作 source code 调用 Hypervisor 的 GenerateSocket，生成用于 host 和 guest 通信的 socket 地址 调用 Hypervisor 的 AddDevice，根据生成的 socket 的类型，为 VM 添加对应类型的设备（例如 vhost-vsock 或 virtio-vsock） 调用 Hypervisor 的 Capabilities，检验 hypervisor 是否支持文件系统共享特性 创建 /run/kata-containers/shared/sandboxes/&lt;containerID&gt;/shared 目录 调用 Hypervisor 的 AddDevice，为 VM 添加 filesystem 类型的设备，其中，挂载标签为 kataShared，挂载源为 /run/kata-containers/shared/sandboxes/&lt;containerID&gt;/shared exec在运行的容器中执行命令 source code 请求 agent server 的 grpc.AgentService 接口的 ExecProcess 方法，进入指定容器执行命令期间涉及到 types.Cmd 和 grpc.Process 的转换，两者都包含要在容器中运行的命令的主要信息，包括工作目录、用户、主组、参数和环境变量等。 startSandbox启动 sandbox 中的所有容器 source code 调用 agent 的 setAgentURL，设置通信 URL 读取 sandbox OCI spec 中挂载点为 /etc/resolv.conf 的挂载源文件内容（即位于 host 上用于挂载到容器中的 DNS 配置文件） 调用 agent 的 check，检测 agent server 的存活性 调用 Network 的 Endpoints，获取 sandbox 中所有网卡，进而调用 Endpoint 的 Properties，获取网卡属性等信息，构建 RPC 通信所需的网卡接口、路由和 ARP neighbor 信息 请求 agent server 的 grpc.AgentService 接口的 UpdateInterface 方法，更新 VM 中网卡信息 请求 agent server 的 grpc.AgentService 接口的 UpdateRoutes 方法，更新 VM 中路由信息 请求 agent server 的 grpc.AgentService 接口的 AddARPNeighbors 方法，更新 VM 中 ARP neighbor 信息 调用 Hypervisor 的 Capabilities，检验 hypervisor 是否支持文件系统共享特性 如果 [hypervisor].shared_fs 为 virtio-fs 或者 virtio-fs-nydus 当 [hypervisor].virtio_fs_cache 不为 none 且 [hypervisor].virtio_fs_cache_size 不为 0 时，挂载参数中会追加 dax如果 virtio-fs 使用 auto 或者 always，则可以使用选项 dax 挂载 guest 目录，从而允许它直接映射来自 host 的内容。 当设置为 none 时，挂载选项不应包含 dax，以免 virtio-fs 守护进程因无效地址引用而崩溃。 生成一个类型为 virtiofs、挂载源为 kataShared、挂载点为 /run/kata-containers/shared/containers/（当 [hypervisor].shared_fs 为 virtio-fs-nydus 时， 为 /run/kata-containers/shared/）以及含上述挂载参数的 virtio-fs 挂载信息 如果 [hypervisor].shared_fs 为 virtio-9p，则生成一个类型为 9p、挂载源为 kataShared，挂载点为 /run/kata-containers/shared/containers/、挂载参数为 msize=&lt;[hypervisor].msize_9p&gt; 的 9p 挂载信息 如果 shmSize 大于 0，则生成一个类型为 tmpfs、挂载源为 shm、挂载点为 /run/kata-containers/sandbox/shm、挂载参数为 size=&lt;shmSize&gt;,noexec,nosuid,nodev,mode=1777 的 ephemeral 挂载信息shmSize 为 sandbox OCI spec 中 destination 为 /dev/shm，type 为 bind 的挂载点的 source 大小 请求 agent server 的 grpc.AgentService 接口的 CreateSandbox 方法，启动 sandbox 中的所有容器其中参数包含 [hypervisor].guest_hook_path，表示 VM 中 hook 脚本路径，hook 必须按照其 hook 类型存储在 guest_hook_path 的子目录中，例如 guest_hook_path/{prestart,poststart,poststop}。Kata agent 将扫描这些目录查找可执行文件，按字母顺序将其添加到容器的生命周期中，并在 VM 运行时命名空间中执行。 stopSandbox关停 sandbox 中的所有容器 source code 请求 agent server 的 grpc.AgentService 接口的 DestroySandbox 方法，关停 sandbox 中的所有容器 createContainer创建容器 source code 调用 FilesystemSharer 的 ShareRootFilesystem，创建 VM 中容器 rootfs 的共享挂载 startContainer启动容器 source code 请求 agent server 的 grpc.AgentService 接口的 StartContainer 方法，启动指定容器 stopContainer关停容器 source code 请求 agent server 的 grpc.AgentService 接口的 RemoveContainer 方法，关停指定容器 signalProcess向指定进程发送信号 source code 请求 agent server 的 grpc.AgentService 接口的 SignalProcess 方法，根据函数入参 all 是否为 true 决定是否向所有进程发送信号 winsizeProcess设置进程的 tty 大小 source code 请求 agent server 的 grpc.AgentService 接口的 TtyWinResize 方法，设置指定进程的 tty 大小 writeProcessStdin将内容写入至进程的标准输入流中 source code 请求 agent server 的 grpc.AgentService 接口的 WriteStdin 方法，将内容写入至指定进程的标准输入流中 closeProcessStdin关闭进程的标准输入流 source code 请求 agent server 的 grpc.AgentService 接口的 CloseStdin 方法，关闭指定进程的标准输入流 readProcessStdout读取进程的标准输出流内容 source code 请求 agent server 的 grpc.AgentService 接口的 ReadStdout 方法，将请求返回体数据拷贝至入参的接收数据中 readProcessStderr读取进程的标准错误流内容 source code 请求 agent server 的 grpc.AgentService 接口的 ReadStderr 方法，将请求返回体数据拷贝至入参的接收数据中 updateContainer更新容器资源配置 source code 请求 agent server 的 grpc.AgentService 接口的 UpdateContainer 方法，更新指定容器的资源配置 waitProcess等待进程返回退出码 source code 请求 agent server 的 grpc.AgentService 接口的 WaitProcess 方法，等待指定进程返回退出码 onlineCPUMem通知 CPU 和内存上线 source code 请求 agent server 的 grpc.AgentService 接口的 OnlineCPUMem 方法，通知上线指定 CPU 和内存 memHotplugByProbestatsContainerpauseContainerresumeContainerconfigureconfigureFromGrpcreseedRNGupdateInterfacelistInterfacesupdateRouteslistRoutesgetGuestDetailssetGuestDateTimecopyFileaddSwapmarkDeadcleanupsaveloadgetOOMEventgetAgentMetricsgetGuestVolumeStatsresizeGuestVolumegetIPTablessetIPTables","link":"/2023/07/26/2023-07-26%20Kata%20Containers%20%E6%BA%90%E7%A0%81%E8%B5%B0%E8%AF%BB%20-%20virtcontainers%20agent/"}],"tags":[{"name":"Kata Containers","slug":"Kata-Containers","link":"/tags/Kata-Containers/"},{"name":"Velero","slug":"Velero","link":"/tags/Velero/"},{"name":"Rust","slug":"Rust","link":"/tags/Rust/"},{"name":"Golang","slug":"Golang","link":"/tags/Golang/"},{"name":"Kubebuilder","slug":"Kubebuilder","link":"/tags/Kubebuilder/"},{"name":"Istio","slug":"Istio","link":"/tags/Istio/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"Virtual Kubelet","slug":"Virtual-Kubelet","link":"/tags/Virtual-Kubelet/"},{"name":"OpenFaaS","slug":"OpenFaaS","link":"/tags/OpenFaaS/"},{"name":"Containerd","slug":"Containerd","link":"/tags/Containerd/"}],"categories":[{"name":"Container Runtime","slug":"Container-Runtime","link":"/categories/Container-Runtime/"},{"name":"Disaster Recovery","slug":"Disaster-Recovery","link":"/categories/Disaster-Recovery/"},{"name":"Programming","slug":"Programming","link":"/categories/Programming/"},{"name":"Scheduling &amp; Orchestration","slug":"Scheduling-Orchestration","link":"/categories/Scheduling-Orchestration/"},{"name":"Service Mesh","slug":"Service-Mesh","link":"/categories/Service-Mesh/"},{"name":"Kubernetes","slug":"Scheduling-Orchestration/Kubernetes","link":"/categories/Scheduling-Orchestration/Kubernetes/"},{"name":"Serverless","slug":"Serverless","link":"/categories/Serverless/"}],"pages":[]}